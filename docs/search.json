[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Traitement d‚Äôimages satellites avec Python",
    "section": "",
    "text": "Pr√©face",
    "crumbs": [
      "Pr√©face"
    ]
  },
  {
    "objectID": "index.html#sect001",
    "href": "index.html#sect001",
    "title": "Traitement d‚Äôimages satellites avec Python",
    "section": "Un manuel sous la forme d‚Äôune ressource √©ducative libre",
    "text": "Un manuel sous la forme d‚Äôune ressource √©ducative libre\nPourquoi un manuel sous licence libre?\nLes logiciels libres sont aujourd‚Äôhui tr√®s r√©pandus. Comparativement aux logiciels propri√©taires, l‚Äôacc√®s au code source permet √† quiconque de l‚Äôutiliser, de le modifier, de le dupliquer et de le partager. Le logiciel Python, dans lequel sont mises en ≈ìuvre les m√©thodes de traitement d‚Äôimages satellites d√©crites dans ce livre, est d‚Äôailleurs √† la fois un langage de programmation et un logiciel libre (sous la licence publique g√©n√©rale GNU GPL2). Par analogie aux logiciels libres, il existe aussi des ressources √©ducatives libres (REL) ¬´¬†dont la licence accorde les permissions d√©sign√©es par les 5R (Retenir ‚Äî R√©utiliser ‚Äî R√©viser ‚Äî Remixer ‚Äî Redistribuer) et donc permet n√©cessairement la modification¬†¬ª (fabriqueREL). La licence de ce livre, CC BY-SA (figure¬†1), permet donc de¬†:\n\nRetenir, c‚Äôest-√†-dire t√©l√©charger et imprimer gratuitement le livre. Notez qu‚Äôil aurait √©t√© plut√¥t surprenant d‚Äô√©crire un livre payant sur un logiciel libre et donc gratuit. Aussi, nous aurions √©t√© tr√®s embarrass√©s que des personnes √©tudiantes avec des ressources financi√®res limit√©es doivent payer pour avoir acc√®s au livre, sans pour autant savoir pr√©alablement si le contenu est r√©ellement adapt√© √† leurs besoins.\nR√©utiliser, c‚Äôest-√†-dire utiliser la totalit√© ou une section du livre sans limitation et sans compensation financi√®re. Cela permet ainsi √† d‚Äôautres personnes enseignantes de l‚Äôutiliser dans le cadre d‚Äôactivit√©s p√©dagogiques.\nR√©viser, c‚Äôest-√†-dire modifier, adapter et traduire le contenu en fonction d‚Äôun besoin p√©dagogique pr√©cis puisqu‚Äôaucun manuel n‚Äôest parfait, tant s‚Äôen faut! Le livre a d‚Äôailleurs √©t√© √©crit int√©gralement dans R avec Quatro. Quiconque peut ainsi t√©l√©charger gratuitement le code source du livre sur github et le modifier √† sa guise (voir l‚Äôencadr√© intitul√© Suggestions d‚Äôadaptation du manuel).\nRemixer, c‚Äôest-√†-dire ¬´¬†combiner la ressource avec d‚Äôautres ressources dont la licence le permet aussi pour cr√©er une nouvelle ressource int√©gr√©e¬†¬ª (fabriqueREL).\nRedistribuer, c‚Äôest-√†-dire distribuer, en totalit√© ou en partie le manuel ou une version r√©vis√©e sur d‚Äôautres canaux que le site Web du livre (par exemple, sur le site Moodle de votre universit√© ou en faire une version imprim√©e).\n\nLa licence de ce livre, CC BY-SA (figure¬†1), oblige donc √†¬†:\n\nAttribuer la paternit√© de l‚Äôauteur dans vos versions d√©riv√©es, ainsi qu‚Äôune mention concernant les grandes modifications apport√©es, en utilisant la formulation suivante¬†:\n\nSamuel Foucher, Apparicio Philippe, Micka√´l Germain, Yacine Bouroubi et √âtienne Clabaut (2024). Traitement d‚Äôimages satellites : . Universit√© de Sherbrooke, D√©partement de g√©omatique appliqu√©e. fabriqueREL. Licence CC¬†BY-SA.\n\nUtiliser la m√™me licence ou une licence similaire √† toutes versions d√©riv√©es.\n\n\n\n\n\n\n\nFigure¬†1: Licence Creative Commons du livre\n\n\n\n\n\n\n\n\nSuggestions d‚Äôadaptation du manuel\n\n\nPour chaque m√©thode de traitement d‚Äôimage abord√©e dans le livre, une description d√©taill√©e et une mise en ≈ìuvre dans Python sont disponibles. Par cons√©quent, plusieurs adaptations du manuel sont possibles¬†:\n\nConserver uniquement les chapitres sur les m√©thodes cibl√©es dans votre cours.\nEn faire une version imprim√©e et la distribuer aux personnes √©tudiantes.\nModifier la description d‚Äôune ou de plusieurs m√©thodes en effectuant les mises √† jour directement dans les chapitres.\nIns√©rer ses propres jeux de donn√©es dans les sections intitul√©es Mise en ≈ìuvre dans Python.\nModifier les tableaux et figures.\nAjouter une s√©rie d‚Äôexercices.\nModifier les quiz de r√©vision.\nR√©diger un nouveau chapitre.\nModifier des syntaxes en Python. Plusieurs librairies Python peuvent √™tre utilis√©es pour mettre en ≈ìuvre telle ou telle m√©thode. Ces derniers √©voluent aussi tr√®s vite et de nouvelles librairies sont propos√©es fr√©quemment! Par cons√©quent, il peut √™tre judicieux de modifier une syntaxe Python du livre en fonction de ses habitudes de programmation en Python (utilisation d‚Äôautres librairies que ceux utilis√©s dans le manuel par exemple) ou de bien mettre √† jour une syntaxe √† la suite de la parution d‚Äôune nouvelle librairie plus performante ou int√©ressante.\nToute autre adaptation qui permet de r√©pondre au mieux √† un besoin p√©dagogique.",
    "crumbs": [
      "Pr√©face"
    ]
  },
  {
    "objectID": "index.html#sect002",
    "href": "index.html#sect002",
    "title": "Traitement d‚Äôimages satellites avec Python",
    "section": "Comment lire ce manuel?",
    "text": "Comment lire ce manuel?\nLe livre comprend plusieurs types de blocs de texte qui en facilitent la lecture.\n\n\n\n\n\nBloc packages\n\n\nHabituellement localis√© au d√©but d‚Äôun chapitre, il comprend la liste des packages Python utilis√©s pour un chapitre.\n\n\n\n\n\n\n\nBloc objectif\n\n\nIl comprend une description des objectifs d‚Äôun chapitre ou d‚Äôune section.\n\n\n\n\n\n\n\nBloc notes\n\n\nIl comprend une information secondaire sur une notion, une id√©e abord√©e dans une section.\n\n\n\n\n\n\n\nBloc pour aller plus loin\n\n\nIl comprend des r√©f√©rences ou des extensions d‚Äôune m√©thode abord√©e dans une section.\n\n\n\n\n\n\n\nBloc astuce\n\n\nIl d√©crit un √©l√©ment qui vous facilitera la vie¬†: une propri√©t√© statistique, un package, une fonction, une syntaxe Python.\n\n\n\n\n\n\n\nBloc attention\n\n\nIl comprend une notion ou un √©l√©ment important √† bien ma√Ætriser.\n\n\n\n\n\n\n\nBloc exercice\n\n\nIl comprend un court exercice de r√©vision √† la fin de chaque chapitre.",
    "crumbs": [
      "Pr√©face"
    ]
  },
  {
    "objectID": "index.html#sect003",
    "href": "index.html#sect003",
    "title": "Traitement d‚Äôimages satellites avec Python",
    "section": "Comment utiliser les donn√©es du livre pour reproduire les exemples?",
    "text": "Comment utiliser les donn√©es du livre pour reproduire les exemples?\nCe livre comprend des exemples d√©taill√©s et appliqu√©s en Python pour chacune des m√©thodes abord√©es. Ces exemples se basent sur des jeux de donn√©es ouverts et mis √† disposition avec le livre. Ils sont disponibles sur le repo github dans le sous-dossier data, √† l‚Äôadresse https://github.com/serie-tele-pyton/TraitementImagesVol1/tree/main/data.\nUne autre option est de t√©l√©charger le repo complet du livre directement sur github (https://github.com/serie-tele-pyton/TraitementImagesVol1) en cliquant sur le bouton Code, puis le bouton Download ZIP (figure¬†2). Les donn√©es se trouvent alors dans le sous-dossier nomm√© data.\n\n\n\n\n\n\nFigure¬†2: T√©l√©chargement de l‚Äôint√©gralit√© du livre",
    "crumbs": [
      "Pr√©face"
    ]
  },
  {
    "objectID": "index.html#sect004",
    "href": "index.html#sect004",
    "title": "Traitement d‚Äôimages satellites avec Python",
    "section": "Structure du livre",
    "text": "Structure du livre\nLe livre est organis√© autour de quatre grandes parties.\nPartie 1. Importation et manipulation de donn√©es spatiales. Dans cette premi√®re partie, nous voyons comment importer, manipuler, visualiser et exporter des donn√©es spatiales de type image (ou de type matriciel) avec Python, principalement avec les packages rasterio, xarray et numpy (chapitre¬†2¬† Importation et manipulation de donn√©es spatiales). Ce chapitre vous permettra de ma√Ætriser la manipulation √† bas niveau de diff√©rents types d‚Äôimagerie. Diff√©rents exemples et exercises sont disponibles avec diff√©rents capteurs satellites (multi-spectral, RGB-NIR, SAR, etc.)\nPartie 2. Transformations des donn√©es spatiales. Cette deuxi√®me partie comprend deux chapitres¬†: les transformations spectrales (chapitre¬†4¬† Transformations spectrales) et les transformations spatiales (chapitre¬†5¬† Transformations spatiales).\nPartie 3. Classifications d‚Äôimages Cette troisi√®me partie comprend deux chapitres¬†: les classifications supervis√©es (chapitre¬†6¬† Classifications d‚Äôimages supervis√©es) et non supervis√©es (chapitre¬†7¬† Classifications d‚Äôimages non supervis√©es).\nPartie 4. Donn√©es massives. Cette quatri√®me et derni√®re partie comprend un seul chapitre qui est d√©di√© aux plateformes de m√©gadonnes ?sec-chap07, notammment Google Earth Engine.",
    "crumbs": [
      "Pr√©face"
    ]
  },
  {
    "objectID": "index.html#sect005",
    "href": "index.html#sect005",
    "title": "Traitement d‚Äôimages satellites avec Python",
    "section": "Remerciements",
    "text": "Remerciements\nDe nombreuses personnes ont contribu√© √† l‚Äô√©laboration de ce manuel.\nCe projet a b√©n√©fici√© du soutien p√©dagogique et financier de la fabriqueREL (ressources √©ducatives libres). Les diff√©rentes rencontres avec le comit√© de suivi nous ont permis de comprendre l‚Äôunivers des ressources √©ducatives libres (REL) et notamment leurs fameux 5R (Retenir ‚Äî R√©utiliser ‚Äî R√©viser ‚Äî Remixer ‚Äî Redistribuer), de mieux d√©finir le besoin p√©dagogique vis√© par ce manuel, d‚Äôidentifier des ressources p√©dagogiques et des outils pertinents pour son √©laboration. Ainsi, nous remercions chaleureusement les membres de la fabriqueREL pour leur soutien inconditionnel¬†:\n\nMyriam Beaudet, biblioth√©caire √† l‚ÄôUniversit√© de Sherbrooke.\nMarianne Dub√©, coordonnatrice de la fabriqueREL, Universit√© de Sherbrooke.\nSerge Pich√©, conseiller p√©dagogique, Universit√© de Sherbrooke.\nClaude Potvin, conseiller en formation, Service de soutien √† l‚Äôenseignement, Universit√© Laval.\n\nNous remercions chaleureusement les personnes √©tudiantes des cours √† modifier plus tard du Baccalaur√©at en g√©omatique appliqu√©e √† l‚Äôenvironnement et du Microprogramme de 1er cycle en g√©omatique appliqu√©e du D√©partement de g√©omatique appliqu√©e de l‚ÄôUniversit√© de Sherbrooke de la session d‚Äô√©t√© 2023¬†: √† modifier plus tard.\nNous remercions aussi les membres du comit√© de r√©vision pour leurs commentaires et suggestions tr√®s constructifs. Ce comit√© est compos√© de quatre personnes √©tudiantes du D√©partement de g√©omatique appliqu√©e de l‚ÄôUniversit√© de Sherbrooke¬†:\n\n√Ä compl√©ter plus tard.\n√Ä compl√©ter plus tard.\n\nFinalement, nous remercions Denise Latreille, r√©viseure linguistique et charg√©e de cours √† l‚ÄôUniversit√© Sherbrooke, pour la r√©vision du manuel.",
    "crumbs": [
      "Pr√©face"
    ]
  },
  {
    "objectID": "index.html#sect006",
    "href": "index.html#sect006",
    "title": "Traitement d‚Äôimages satellites avec Python",
    "section": "Introduction aux images de t√©l√©d√©tection",
    "text": "Introduction aux images de t√©l√©d√©tection\nL‚Äôimagerie num√©rique a pris une place importante dans notre vie de tous les jours depuis une quinzaine d‚Äôann√©e. Ces images sont prises g√©n√©ralement au niveau du sol (imagerie proximale) avec seulement trois couleurs dans le domaine de la vision humaine (rouge, vert et bleu). Dans la suite du manuel, on parlera d‚Äôimages du domaine de la vision par ordinateur ou images en vision pour faire plus court.\nLes images de t√©l√©d√©tection ont des particularit√©s et des propri√©t√©s qui les diff√©rencient des images de tous les jours. On peut souligner au moins cinq caract√©ristiques principales:\n\nLes images sont g√©or√©f√©renc√©es : Cela veut dire que pour chaque pixel nous pouvons y associer une position g√©ographique ou cartographique.\nLe point de vue est tr√®s diff√©rent : Ces images sont prises avec une vue d‚Äôen haut (Nadir) ou oblique avec une distance qui peut √™tre tr√®s grande (On parle d‚Äôimages distales).\nElles poss√®dent plus que 3 bandes : Contrairement aux images en vision, les images de t√©l√©d√©tection poss√®dent bien souvent plus que 3 bandes. Il n‚Äôest pas rare de trouver 4 bandes (Pl√©iade), 13 bandes (Sentinel-2, Landsat) et m√™me 200 bandes pour des capteurs hyperspectraux.\nElles peuvent √™tre calibr√©es : Les valeurs num√©rique de l‚Äôimage peuvent √™tre converties en quantit√©s physiques (luminance, r√©flectance, section efficace, etc.) via une fonction de calibration.\nElles sont de grande taille : Il n‚Äôest pas rare de manipuler des images qui font plusieurs dizaines de milliers de pixels en dimension.\n\n\nRessources en ligne\n\n\nListes des librairies utilis√©s\nDans ce livre, nous utilisons de nombreux packages Python que vous pouvez installer en une seule fois (voir section¬†1.3.1 Cr√©ation d‚Äôun environnement virtuel) ou chapitre par chapitre.",
    "crumbs": [
      "Pr√©face"
    ]
  },
  {
    "objectID": "00-auteurs.html",
    "href": "00-auteurs.html",
    "title": "√Ä propos des auteurs",
    "section": "",
    "text": "Samuel Foucher est professeur titulaire au D√©partement de g√©omatique appliqu√©e de l‚ÄôUniversit√© de Sherbrooke. Il y enseigne aux programmes de 1er et 2e cycles de g√©omatique les cours Transport et mobilit√© durable, Mod√©lisation et analyse spatiale et G√©omatique appliqu√©e √† la gestion urbaine. Durant les derni√®res ann√©es, il a offert plusieurs formations aux √âcoles d‚Äô√©t√© du Centre interuniversitaire qu√©b√©cois de statistiques sociales (CIQSS). G√©ographe de formation, ses int√©r√™ts de recherche incluent la justice et l‚Äô√©quit√© environnementale, la mobilit√© durable, les pollutions atmosph√©rique et sonore, et le v√©lo en ville. Il a publi√© une centaine d‚Äôarticles scientifiques dans diff√©rents domaines des √©tudes urbaines et de la g√©ographie mobilisant la g√©omatique et l‚Äôanalyse spatiale.\nPhilippe Apparicio est professeur titulaire au D√©partement de g√©omatique appliqu√©e de l‚ÄôUniversit√© de Sherbrooke. Il y enseigne aux programmes de 1er et 2e cycles de g√©omatique les cours Transport et mobilit√© durable, Mod√©lisation et analyse spatiale et G√©omatique appliqu√©e √† la gestion urbaine. Durant les derni√®res ann√©es, il a offert plusieurs formations aux √âcoles d‚Äô√©t√© du Centre interuniversitaire qu√©b√©cois de statistiques sociales (CIQSS). G√©ographe de formation, ses int√©r√™ts de recherche incluent la justice et l‚Äô√©quit√© environnementale, la mobilit√© durable, les pollutions atmosph√©rique et sonore, et le v√©lo en ville. Il a publi√© une centaine d‚Äôarticles scientifiques dans diff√©rents domaines des √©tudes urbaines et de la g√©ographie mobilisant la g√©omatique et l‚Äôanalyse spatiale.\nMicka√´l Germain est professeur titulaire au D√©partement de g√©omatique appliqu√©e de l‚ÄôUniversit√© de Sherbrooke. Il y enseigne aux programmes de 1er et 2e cycles de g√©omatique les cours Transport et mobilit√© durable, Mod√©lisation et analyse spatiale et G√©omatique appliqu√©e √† la gestion urbaine. Durant les derni√®res ann√©es, il a offert plusieurs formations aux √âcoles d‚Äô√©t√© du Centre interuniversitaire qu√©b√©cois de statistiques sociales (CIQSS). G√©ographe de formation, ses int√©r√™ts de recherche incluent la justice et l‚Äô√©quit√© environnementale, la mobilit√© durable, les pollutions atmosph√©rique et sonore, et le v√©lo en ville. Il a publi√© une centaine d‚Äôarticles scientifiques dans diff√©rents domaines des √©tudes urbaines et de la g√©ographie mobilisant la g√©omatique et l‚Äôanalyse spatiale.\nYacine Bouroubi est professeur titulaire au D√©partement de g√©omatique appliqu√©e de l‚ÄôUniversit√© de Sherbrooke. Il y enseigne aux programmes de 1er et 2e cycles de g√©omatique les cours Transport et mobilit√© durable, Mod√©lisation et analyse spatiale et G√©omatique appliqu√©e √† la gestion urbaine. Durant les derni√®res ann√©es, il a offert plusieurs formations aux √âcoles d‚Äô√©t√© du Centre interuniversitaire qu√©b√©cois de statistiques sociales (CIQSS). G√©ographe de formation, ses int√©r√™ts de recherche incluent la justice et l‚Äô√©quit√© environnementale, la mobilit√© durable, les pollutions atmosph√©rique et sonore, et le v√©lo en ville. Il a publi√© une centaine d‚Äôarticles scientifiques dans diff√©rents domaines des √©tudes urbaines et de la g√©ographie mobilisant la g√©omatique et l‚Äôanalyse spatiale.",
    "crumbs": [
      "√Ä propos des auteurs"
    ]
  },
  {
    "objectID": "00-PriseEnMainPython.html",
    "href": "00-PriseEnMainPython.html",
    "title": "1¬† Introduction au langage Python",
    "section": "",
    "text": "1.1 Les distributions\nIl existe plusieurs distributions du langage Python, ces distributions sont comme diff√©rentes saveurs de votre glace pr√©f√©r√©e - chacune a ses propres caract√©ristiques uniques, mais elles sont toutes fondamentalement Python. Voici un aper√ßu des principales distributions :\nChaque distribution a ses forces, que ce soit la simplicit√©, la vitesse ou des fonctionnalit√©s sp√©cifiques. Le choix d√©pend de vos besoins, comme choisir entre une glace simple ou un banana split √©labor√©.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction au langage Python</span>"
    ]
  },
  {
    "objectID": "00-PriseEnMainPython.html#les-distributions",
    "href": "00-PriseEnMainPython.html#les-distributions",
    "title": "1¬† Introduction au langage Python",
    "section": "",
    "text": "CPython : C‚Äôest la distribution ‚Äúvanille‚Äù officielle, comme la recette originale de Python. C‚Äôest le choix id√©al pour la compatibilit√© et la conformit√© aux standards.\nAnaconda : Pensez-y comme √† un sundae tout garni. Il vient avec de nombreuses biblioth√®ques scientifiques pr√©install√©es, id√©al pour l‚Äôanalyse de donn√©es et le machine learning.\nMiniconda : est une distribution l√©g√®re de Python qui vous permet d‚Äôajouter les librairies au besoin.\nPyPy : C‚Äôest comme une version turbo de Python, optimis√©e pour la vitesse.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction au langage Python</span>"
    ]
  },
  {
    "objectID": "00-PriseEnMainPython.html#les-styles-de-programmation-en-python",
    "href": "00-PriseEnMainPython.html#les-styles-de-programmation-en-python",
    "title": "1¬† Introduction au langage Python",
    "section": "1.2 Les styles de programmation en Python",
    "text": "1.2 Les styles de programmation en Python\nIl existe plusieurs approches pour programmer en Python. La plus directe est en version interactive en tapant python et de rentrer des commandes ligne par ligne.\n\n1.2.1 Les outils de programmation\nUn code python prend la forme d‚Äôun simple fichier texte avec l‚Äôextension .py et peut √™tre modifi√© avec un simple √©diteur de texte. Cependant, il n‚Äôy aura pas de r√©troactions imm√©diates de l‚Äôinterpr√©teur Python ce qui rend la correction d‚Äôerreurs (d√©bogage) beaucoup plus laborieux.\nUn IDE (Integrated Developement Environnement) est comme une bo√Æte √† outils compl√®te pour les programmeurs, vous trouverez :\n\nUn √©diteur de texte am√©lior√© pour √©crire votre code, avec des fonctionnalit√©s comme la coloration syntaxique qui rend le code plus lisible.\nUn compilateur qui transforme votre code en instructions que l‚Äôordinateur peut comprendre.\nUn d√©bogueur pour trouver et corriger les erreurs, tel un d√©tective num√©rique.\nDes outils d‚Äôautomatisation qui effectuent des t√¢ches r√©p√©titives, comme un assistant virtuel pour le codage.\nL‚Äôacc√®s √† la documentation des diff√©rentes librairies.\n\nCes outils int√©gr√©s permettent aux d√©veloppeurs de travailler plus efficacement, en passant moins de temps √† jongler entre diff√©rentes applications et plus de temps √† produire du code.\nVoici quelques options populaires :\n\nPyCharm : C‚Äôest un des outils les plus utilis√©s dans l‚Äôindustrie. Il offre une multitude de fonctionnalit√©s comme l‚Äôautocompl√©tion intelligente et le d√©bogage int√©gr√©, id√©al pour les grands projets. Cepednant, cet outil peut √™tre assez gourmand en m√©moire et en CPU.\nVisual Studio Code : Gratuit, l√©ger mais puissant, il est personnalisable avec des extensions pour Python.\nSpyder : Logiciel libre et gratuit, orient√© vers les applications scientifiques.\nJupyter Notebooks : Imaginez un cahier interactif pour le code. Id√©al pour l‚Äôanalyse de donn√©es et l‚Äôapprentissage, il permet de m√©langer code, texte et visualisations. Des services gratuits dans le cloud sont disponibles comme Google Colab et Kaggle. Ces environnements sont n√©anmoins moins appropri√©es pour des grands projets et le d√©bogage.\nSublime Text : C‚Äôest comme un stylo √©l√©gant et rapide. L√©ger et rapide, il est appr√©ci√© pour sa simplicit√© et sa vitesse. Le choix d√©pend de vos besoins, que vous soyez d√©butant ou d√©veloppeur chevronn√©. L‚Äôimportant est de trouver l‚Äô√©diteur qui vous convient le mieux pour coder confortablement.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction au langage Python</span>"
    ]
  },
  {
    "objectID": "00-PriseEnMainPython.html#bonnes-pratiques",
    "href": "00-PriseEnMainPython.html#bonnes-pratiques",
    "title": "1¬† Introduction au langage Python",
    "section": "1.3 Bonnes pratiques",
    "text": "1.3 Bonnes pratiques\nPython est un langage tr√®s dynamique, qui √©volue constamment. Cela pose certains d√©fis pour la gestion du code √† long terme. Il est fortement conseill√© d‚Äôutiliser des environnements virtuels pour g√©rer vos diff√©rentes librairies. Voici quelques bonnes pratiques √† suivre :\n\nN‚Äôinstallez par la toute derni√®re version de Python : installez toujours une version ou deux qui pr√©c√®de la derni√®re version. Les versions trop r√©centes peuvent √™tre instables. La version de python d√©sir√©e peut √™tre sp√©cifi√©e au moment de la cr√©ation d‚Äôun environnement virtuel (voir plus bas). Vous pouvez afficher la liste des versions de python avec la commande conda search --full-name python. Il est recommand√© d‚Äôinstaller 1 ou 2 version ant√©rieure, par exemple si 3.13 est la version plus r√©cente, installer plut√¥t la version 3.11.\nN‚Äôutilisez pas de version obsol√®te de Python : cela peut sembler contradictoire avec le point 1 mais c‚Äôest l‚Äôexc√®s inverse. Si vous utilisez une version trop ancienne alors toutes vos librairies vont cessez d‚Äô√©voluer et peuvent devenir obsol√®te.\nUtilisez des environnements virtuels : Pensez-y comme √† des compartiments s√©par√©es pour chaque projet. Cela √©vite les conflits entre les diff√©rentes versions de biblioth√®ques et garde votre syst√®me propre. Par exemple, si vous souhaitez v√©rifier une nouvelle version de Python, utilisez un environnement : conda create --name test python=3.11\nV√©rifiez l‚Äôinstallation : Apr√®s l‚Äôinstallation, ouvrez un terminal et tapez python --version pour vous assurer que tout fonctionne correctement.\n\n\n1.3.1 Cr√©ation d‚Äôun environnement virtuel\nIl y a deux fa√ßons d‚Äôinstaller un environnement virtuel selon votre distribution de Python:\n\nOption 1 : vous utilisez Anaconda ou Miniconda, dans ce cas la commande conda est utilis√©e pour cr√©er un environnement test avec Python 3.10:\n\nconda env -n test python=3.10\nconda activate test\n\nOption 2 : vous utilisez CPython\n\nconda env -n test python=3.10\nconda activate test\n\n\n1.3.2 Cr√©ation d‚Äôun environnement de travail local (avanc√©)\nNote: les notebooks peuvent fonctionner localement uniquement sous Linux ou avec WSL2.\nLes notebooks Python fonctionnent par d√©faut dans l‚Äôenvironnement Google Colab. Si vous souhaitez faire fonctionner ces notebook localement, vous pouvez installer un environnement local avec un serveur Jupyter. Il suffit de suivre les √©tapes suivantes: 1. Installer WSL2 sous Windows 2. Installer vscode 3. Installer Miniconda 4. Faire une installation du contenu du livre soit en utilisant une commande git clone ou en r√©cup√©rant le .zip du livre 5. Ouvrir WSL2 et placer vous dans le r√©pertoire du livre TraitementImagesPythonVol1. Assurez vous que vous avez acc√®s √† conda en tapant conda --version 6. Lancer la commande conda env create -f jupyter_env.yaml 7. Activer le nouvel environnement: conda activate jupyter_env 8. Le serveur jupyter peut ensuite √™tre lanc√© avec la commande suivante: jupyter lab --ip='*' --NotebookApp.token='' --NotebookApp.password='' Une fen√™tre devrait alors appara√Ætre dans votre fureteur. Dans le menu de gauche vous pouvez acc√©der aux notebooks dans le r√©pertoire notebooks:\n\n\n\n\n\n\nFigure¬†1.1: La librairie NumPy est le fondement de nombreuses librairies scientifiques (d‚Äôapr√®s (Harris 2020)).",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction au langage Python</span>"
    ]
  },
  {
    "objectID": "00-PriseEnMainPython.html#les-structures-de-base-en-python",
    "href": "00-PriseEnMainPython.html#les-structures-de-base-en-python",
    "title": "1¬† Introduction au langage Python",
    "section": "1.4 Les structures de base en Python",
    "text": "1.4 Les structures de base en Python\nIl y a essentiellement deux structures de donn√©es que Python manipule : les listes et les dictionnaires.\n\n1.4.1 Les listes\nLes listes sont comme des boites extensibles o√π vous pouvez ranger diff√©rents types d‚Äôobjets :\n\nRepr√©sent√©es par des crochets : [1, 2, 3, \"python\"].\nOrdonn√©es et modifiables (mutables), vous pouvez r√©cup√©rer une valeur par sa position avec [].\nPermettent les doublons (deux fois la m√™me valeur).\nId√©ales pour stocker des collections d‚Äô√©l√©ments que vous voulez modifier\n\n\n\n1.4.2 Les tuples\nLes tuples sont similaires aux listes, mais les bo√Ætes sont scell√©es :\n\nRepr√©sent√©s par des parenth√®ses : (1, 2, 3, \"python\").\nOrdonn√©s mais non modifiables (immutables).\nPermettent les doublons.\nSouvent utilis√© pour stocker des donn√©es qui ne doivent pas changer (comme des param√®tres).\n\n\n\n1.4.3 Les ensembles (Sets)\nLes ensembles sont comme des boites magiques qui ne gardent qu‚Äôun exemplaire de chaque objet :\n\nRepr√©sent√©s par des accolades : {1, 2, 3}.\nNon ordonn√©s et modifiables.\nN‚Äôautorisent pas les doublons.\nUtiles pour √©liminer les doublons et effectuer des op√©rations math√©matiques sur des ensembles.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction au langage Python</span>"
    ]
  },
  {
    "objectID": "00-PriseEnMainPython.html#dictionnaires",
    "href": "00-PriseEnMainPython.html#dictionnaires",
    "title": "1¬† Introduction au langage Python",
    "section": "1.5 Dictionnaires",
    "text": "1.5 Dictionnaires\nLes dictionnaires sont comme des boites avec des √©tiquettes sur chcune d‚Äôelle :\n\nRepr√©sent√©s par des accolades avec des paires cl√©-valeur : {\"nom\": \"Python\", \"ann√©e\": 1991}.\nNon ordonn√©s et modifiables.\nLes cl√©s doivent √™tre uniques, mais les valeurs peuvent √™tre dupliqu√©es\nUtiles pour stocker des donn√©es associatives ou pour cr√©er des tables de recherche rapide",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction au langage Python</span>"
    ]
  },
  {
    "objectID": "00-PriseEnMainPython.html#programmation-objet",
    "href": "00-PriseEnMainPython.html#programmation-objet",
    "title": "1¬† Introduction au langage Python",
    "section": "1.6 Programmation objet",
    "text": "1.6 Programmation objet\nLa programmation orient√©e objet (POO) en Python est comme construire avec des blocs LEGO. Chaque objet est un bloc LEGO avec ses propres caract√©ristiques (attributs) et capacit√©s (m√©thodes). Les classes sont les plans pour cr√©er ces blocs. Par exemple, une classe ‚ÄúVoiture‚Äù pourrait avoir des attributs comme ‚Äúcouleur‚Äù et ‚Äúvitesse‚Äù, et des m√©thodes comme ‚Äúd√©marrer‚Äù et ‚Äúacc√©l√©rer‚Äù.\nPython rend la POO accessible avec des fonctionnalit√©s conviviales :\n\nEncapsulation : Comme emballer un cadeau, elle cache les d√©tails internes d‚Äôun objet.\nH√©ritage : Permet de cr√©er de nouvelles classes bas√©es sur des classes existantes, comme un enfant h√©ritant des traits de ses parents.\nPolymorphisme : Permet √† diff√©rents objets de r√©pondre au m√™me message de mani√®re unique, comme si diff√©rents animaux r√©pondaient diff√©remment √† ‚Äúfais du bruit‚Äù.\n\nCes caract√©ristiques font de Python un excellent choix pour apprendre et appliquer les concepts de la POO, rendant le code plus organis√© et r√©utilisable\n\n\n\n\n\nListe des packages utilis√©s dans ce chapitre\n\n\n\nPour importer et manipuler des fichiers g√©ographiques¬†:\n\nnumpy pour manipuler des donn√©es matricielles.\nrasterio pour importer et manipuler des donn√©es matricielles.\n\nPour construire des cartes et des graphiques¬†:\n\ntmap est certainement le meilleur package pour la cartographie.\nggplot2 pour construire des graphiques.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction au langage Python</span>"
    ]
  },
  {
    "objectID": "00-PriseEnMainPython.html#sec-016",
    "href": "00-PriseEnMainPython.html#sec-016",
    "title": "1¬† Introduction au langage Python",
    "section": "1.7 Cahier de r√©vision (notebook)",
    "text": "1.7 Cahier de r√©vision (notebook)\n\n\n\n\nHarris, Millman, C. R. 2020. ¬´¬†Array programming with NumPy.¬†¬ª Nature: 357‚Äë362. https://doi.org/10.1038/s41586-020-2649-2.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction au langage Python</span>"
    ]
  },
  {
    "objectID": "01-ImportationManipulationImages.html",
    "href": "01-ImportationManipulationImages.html",
    "title": "2¬† Importation et manipulation de donn√©es spatiales",
    "section": "",
    "text": "2.1 üöÄ Pr√©ambule\nAssurez-vous de lire ce pr√©ambule avant d‚Äôex√©cutez le reste du notebook. ### üéØ Objectifs Dans ce chapitre, nous abordons quelques formats d‚Äôimages ainsi que leur lecture. Ce chapitre est aussi disponible sous la forme d‚Äôun notebook Python:",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Importation et manipulation de donn√©es spatiales</span>"
    ]
  },
  {
    "objectID": "01-ImportationManipulationImages.html#pr√©ambule",
    "href": "01-ImportationManipulationImages.html#pr√©ambule",
    "title": "2¬† Importation et manipulation de donn√©es spatiales",
    "section": "",
    "text": "2.1.1 Librairies\nLes librairies qui vont √™tre explor√©es dans ce chapitre sont les suivantes:\n\nSciPy -\nNumPy -\nopencv-python ¬∑ PyPI\nscikit-image\nRasterio\nXarray\nrioxarray\n\nDans l‚Äôenvironnement Google Colab, seul rioxarray et gdal doivent √™tre install√©:\n\n!apt-get update\n!apt-get install gdal-bin libgdal-dev\n!pip install -q rioxarray\n\nV√©rifier les importations:\n\nimport numpy as np\nimport rioxarray as rxr\nfrom scipy import signal\nimport xarray as xr\nimport xrscipy\nimport matplotlib.pyplot as plt\n\n\n\n2.1.2 Donn√©es\nNous allons utilis√©s ces images dans ce chapitre:\n\n!wget https://github.com/sfoucher/TraitementImagesPythonVol1/raw/refs/heads/main/data/chapitre01/subset_RGBNIR_of_S2A_MSIL2A_20240625T153941_N0510_R011_T18TYR_20240625T221903.tif -O RGBNIR_of_S2A.tif\n!wget https://github.com/sfoucher/opengeos-data/raw/refs/heads/main/raster/landsat7.tif -O landsat7.tif\n!wget https://github.com/sfoucher/opengeos-data/raw/refs/heads/main/images/berkeley.jpg -O berkeley.jpg\n!wget https://raw.githubusercontent.com/sfoucher/TraitementImagesPythonVol1/refs/heads/main/images/modis-aqua.PNG -O modis-aqua.PNG\n\nV√©rifiez que vous √™tes capable de les lire :\n\nwith rxr.open_rasterio('berkeley.jpg', mask_and_scale= True) as img_rgb:\n    print(img_rgb)\nwith rxr.open_rasterio('RGBNIR_of_S2A.tif', mask_and_scale= True) as img_rgbnir:\n    print(img_rgbnir)",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Importation et manipulation de donn√©es spatiales</span>"
    ]
  },
  {
    "objectID": "01-ImportationManipulationImages.html#importation-dimages",
    "href": "01-ImportationManipulationImages.html#importation-dimages",
    "title": "2¬† Importation et manipulation de donn√©es spatiales",
    "section": "2.2 Importation d‚Äôimages",
    "text": "2.2 Importation d‚Äôimages\nLa premi√®re √©tape avant tout traitement est d‚Äôacc√©der √† la donn√©e image pour qu‚Äôelle soit manipul√©e par le programme Python. L‚Äôimagerie satellite pr√©sente certains d√©fis notamment en raison de la taille parfois tr√®s importante des images. Il existe maintenant certaines librairies, comme Xarray, qui on cherch√©es √† optimiser la lecture et l‚Äô√©criture de grandes images. Il est donc conseiller de toujours garder un oeil sur l‚Äôespace m√©moire occup√© par les variables Python reli√©es aux images. La librairie principale en g√©omatique qui va nous permettre d‚Äôimporter (et d‚Äôexporter) de l‚Äôimagerie est la librairie GDAL qui rassemble la plupart des formats sous forme de driver (ou pilote en fran√ßais).\nDans le domaine de la g√©omatique, il faut pr√™ter attention √† trois caract√©ristiques principales des images: 1. La matrice des donn√©es elle-m√™me qui contient les valeurs brutes des pixels. Cette matrice sera souvent un cube √† trois dimensions. En Python, ce cube sera le plus souvent un objet de la librairie NumPy (voir section). 2. La dynamique des images c.√†.d le format de stockage des valeurs individuelles (octet, entier, double, etc.). Ce format d√©cide principalement de la r√©solution radiom√©trique et des valeurs minimales et maximales support√©es. 3. La m√©tadonn√©e qui va transporter l‚Äôinformation auxiliaire de l‚Äôimage comme les dimensions et la position de l‚Äôimage, la date, etc. Cette donn√©e auxiliaire prendra souvent la forme d‚Äôun dictionnaire Python.\nLes diff√©rents formats se distinguent principalement sur la mani√®re dont ces trois caract√©ristiques sont g√©r√©es.\n\n2.2.1 Formats des images\nIl existe maintenant de nombreux formats num√©riques pour la donn√©e de type image parfois appel√© donn√©e matricielle ou donn√©e raster. La librairie GDAL rassemble la plupart des formats matriciels rencontr√©s en g√©omatique (voir Raster drivers ‚Äî GDAL documentation pour une liste compl√®te).\nOn peut distinguer deux grandes familles de format: 1. Les formats de type RVB issus de l‚Äôimagerie num√©rique grand publique comme JPEG, png, etc. Ces formats ne supportent g√©n√©ralement que trois bandes au maximum (rouge, vert et bleu) et des valeurs de niveaux de gris entre 0 et 255 (format dit 8 bit). 2. Les g√©o-formats issus des domaines scientifiques ou techniques comme GeoTIFF, HDF5, etc. qui peuvent inclure plus que trois bandes et des dynamiques plus √©lev√©es (16 bit ou m√™me float).\nLes formats RVB restent tr√®s utilis√©s en Python notamment par les librairies dites de vision par ordinateur (Computer Vision) comme OpenCV et sickit-image ainsi que les grandes librairies en apprentissage profond (PyTorch, Tensorflow).\n\n\n\n\n\nInstallation de gdal dans un syst√®me Linux \n\n\n\nPour installer GDAL¬†:\n\n!apt-get update\n!apt-get install gdal-bin libgdal-dev\n\n\n\n2.2.1.1 Formats de type RVB\nLes premiers formats pour de l‚Äôimagerie √† une bande (monochrome) et √† trois bandes (image couleur rouge-vert-bleu) sont issus du domaine des sciences de l‚Äôordinateur. On trouvera, entre autres, les formats pbm, png et jpeg. Ces formats supportent peu de m√©tadonn√©es et sont plac√©es dans un ent√™te (header) tr√®s limit√©. Cependant, ces formats restent tr√®s populaires dans le domaine de la vision par ordinateur et sont tr√®s utilis√©s en apprentissage profond en particulier. Pour la lecture des images RVB, on peut utiliser les librairies Rasterio, PIL ou OpenCV.\n\n2.2.1.1.1 Lecture avec la librairie PIL\nLa librairie PIL retourne un objet de type PngImageFile, l‚Äôaffichage de l‚Äôimage se fait directement dans la cellule de sortie.\n\n\n\n\nBloc de code¬†2.1: Lecture d‚Äôune image en format PNG avec PIL\n\n\nfrom PIL import Image\nimg = Image.open('modis-aqua.PNG')\nimg\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.2.1.1.2 Lecture avec la librairie OpenCV\nLa librairie OpenCV est aussi tr√®s populaire en vision par ordinateur. La fonction imread donne directement un objet de type NumPy en sortie.\n\n\n\n\nBloc de code¬†2.2: Lecture d‚Äôune image en format PNG avec OpenCV\n\n\nimport cv2\nimg = cv2.imread('modis-aqua.PNG')\nimg\n\n\n\n\n\n\n2.2.1.1.3 Lecture avec la librairie RasterIO\nRien ne nous emp√™che de lire une image de format RVB avec RasterIO comme d√©crit dans (bloc¬†2.3). Vous noterez cependant les avertissements concernant l‚Äôabsence de g√©or√©f√©rence pour ce type d‚Äôimage.\n\n\n\n\nBloc de code¬†2.3: Lecture d‚Äôune image en format PNG avec OpenCV\n\n\nimport rasterio\nimg= rasterio.open('modis-aqua.PNG')\nimg\n\n\n\n\n\n\n\n2.2.1.2 Le format GeoTiff\nLe format GeoTIFF est une extension du format TIFF (Tagged Image File Format) qui permet d‚Äôincorporer des m√©tadonn√©es g√©ospatiales directement dans un fichier image. D√©velopp√© initialement par Dr.¬†Niles Ritter au Jet Propulsion Laboratory de la NASA dans les ann√©es 1990, GeoTIFF est devenu un standard de facto pour le stockage et l‚Äô√©change d‚Äôimages g√©or√©f√©renc√©es dans les domaines de la t√©l√©d√©tection et des syst√®mes d‚Äôinformation g√©ographique (SIG). Ce format supporte plus que trois bandes aussi longtemps que ces bandes sont de m√™me dimension.\nLe format GeoTIFF est tr√®s utilis√© et est largement support√© par les biblioth√®ques et logiciels g√©ospatiaux, notamment GDAL (Geospatial Data Abstraction Library), qui offre des capacit√©s de lecture et d‚Äô√©criture pour ce format. Cette compatibilit√© √©tendue a contribu√© √† son adoption g√©n√©ralis√©e dans la communaut√© g√©ospatiale.\n\n2.2.1.2.1 Standardisation par l‚ÄôOGC\nLe standard GeoTIFF propos√© par l‚ÄôOpen Geospatial Consortium (OGC) en 2019 formalise et √©tend les sp√©cifications originales du format GeoTIFF, offrant une norme robuste pour l‚Äô√©change d‚Äôimages g√©or√©f√©renc√©es. Cette standardisation, connue sous le nom d‚ÄôOGC GeoTIFF 1.1 (2019), apporte plusieurs am√©liorations et clarifications importantes.\n\n\n\n2.2.1.3 Le format COG\nUne innovation r√©cente dans l‚Äô√©cosyst√®me GeoTIFF est le format Cloud Optimized GeoTIFF (COG), con√ßu pour faciliter l‚Äôutilisation de fichiers GeoTIFF h√©berg√©s sur des serveurs web HTTP. Le COG permet aux utilisateurs et aux logiciels d‚Äôacc√©der √† des parties sp√©cifiques du fichier sans avoir √† le t√©l√©charger enti√®rement, ce qui est particuli√®rement utile pour les applications bas√©es sur le cloud.\n\n\n\n2.2.2 M√©tadonn√©es des images\nLa mani√®re la plus directe d‚Äôacc√©der √† la m√©tadonn√©e d‚Äôune image est d‚Äôutiliser les commandes rio info de la librairie Rasterio ou gdalinfo de la librairie gdal. Le r√©sultat est imprim√© dans la sortie standard ou sous forme d‚Äôun dictionnaire Python.\n\n\n\n\nBloc de code¬†2.4: Collecte d‚Äôinformation sur une image avec gdal\n\n\n!gdalinfo RGBNIR_of_S2A.tif\n\n\n\n\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nDriver: GTiff/GeoTIFF\nFiles: RGBNIR_of_S2A.tif\n       RGBNIR_of_S2A.tif.aux.xml\nSize is 2074, 1926\nCoordinate System is:\nPROJCS[\"WGS 84 / UTM zone 18N\",\n    GEOGCS[\"WGS 84\",\n        DATUM[\"WGS_1984\",\n            SPHEROID[\"WGS 84\",6378137,298.257223563,\n                AUTHORITY[\"EPSG\",\"7030\"]],\n            AUTHORITY[\"EPSG\",\"6326\"]],\n        PRIMEM[\"Greenwich\",0,\n            AUTHORITY[\"EPSG\",\"8901\"]],\n        UNIT[\"degree\",0.0174532925199433,\n            AUTHORITY[\"EPSG\",\"9122\"]],\n        AUTHORITY[\"EPSG\",\"4326\"]],\n    PROJECTION[\"Transverse_Mercator\"],\n    PARAMETER[\"latitude_of_origin\",0],\n    PARAMETER[\"central_meridian\",-75],\n    PARAMETER[\"scale_factor\",0.9996],\n    PARAMETER[\"false_easting\",500000],\n    PARAMETER[\"false_northing\",0],\n    UNIT[\"metre\",1,\n        AUTHORITY[\"EPSG\",\"9001\"]],\n    AXIS[\"Easting\",EAST],\n    AXIS[\"Northing\",NORTH],\n    AUTHORITY[\"EPSG\",\"32618\"]]\nOrigin = (731780.000000000000000,5040800.000000000000000)\nPixel Size = (10.000000000000000,-10.000000000000000)\nMetadata:\n  AREA_OR_POINT=Area\n  TIFFTAG_IMAGEDESCRIPTION=subset_RGBNIR_of_S2A_MSIL2A_20240625T153941_N0510_R011_T18TYR_20240625T221903\n  TIFFTAG_RESOLUTIONUNIT=1 (unitless)\n  TIFFTAG_XRESOLUTION=1\n  TIFFTAG_YRESOLUTION=1\nImage Structure Metadata:\n  INTERLEAVE=BAND\nCorner Coordinates:\nUpper Left  (  731780.000, 5040800.000) ( 72d 2' 3.11\"W, 45d28'55.98\"N)\nLower Left  (  731780.000, 5021540.000) ( 72d 2'35.69\"W, 45d18'32.70\"N)\nUpper Right (  752520.000, 5040800.000) ( 71d46' 9.19\"W, 45d28'30.08\"N)\nLower Right (  752520.000, 5021540.000) ( 71d46'44.67\"W, 45d18' 6.95\"N)\nCenter      (  742150.000, 5031170.000) ( 71d54'23.16\"W, 45d23'31.71\"N)\nBand 1 Block=2074x1926 Type=UInt16, ColorInterp=Gray\n  Min=86.000 Max=15104.000 \n  Minimum=86.000, Maximum=15104.000, Mean=1426.625, StdDev=306.564\n  Metadata:\n    STATISTICS_MAXIMUM=15104\n    STATISTICS_MEAN=1426.6252674912\n    STATISTICS_MINIMUM=86\n    STATISTICS_STDDEV=306.56427126942\n    STATISTICS_VALID_PERCENT=100\nBand 2 Block=2074x1926 Type=UInt16, ColorInterp=Undefined\n  Min=1139.000 Max=14352.000 \n  Minimum=1139.000, Maximum=14352.000, Mean=1669.605, StdDev=310.919\n  Metadata:\n    STATISTICS_MAXIMUM=14352\n    STATISTICS_MEAN=1669.6050060032\n    STATISTICS_MINIMUM=1139\n    STATISTICS_STDDEV=310.91935787639\n    STATISTICS_VALID_PERCENT=100\nBand 3 Block=2074x1926 Type=UInt16, ColorInterp=Undefined\n  Min=706.000 Max=15280.000 \n  Minimum=706.000, Maximum=15280.000, Mean=1471.392, StdDev=385.447\n  Metadata:\n    STATISTICS_MAXIMUM=15280\n    STATISTICS_MEAN=1471.3923473736\n    STATISTICS_MINIMUM=706\n    STATISTICS_STDDEV=385.44654593014\n    STATISTICS_VALID_PERCENT=100\nBand 4 Block=2074x1926 Type=UInt16, ColorInterp=Undefined\n  Min=1067.000 Max=15642.000 \n  Minimum=1067.000, Maximum=15642.000, Mean=4393.945, StdDev=1037.934\n  Metadata:\n    STATISTICS_MAXIMUM=15642\n    STATISTICS_MEAN=4393.94485025\n    STATISTICS_MINIMUM=1067\n    STATISTICS_STDDEV=1037.933939728\n    STATISTICS_VALID_PERCENT=100\n\n\nLe plus simple est d‚Äôutiliser la fonction rio info:\n\n\n\n\nBloc de code¬†2.5: Collecte d‚Äôinformation sur une image avec rasterio\n\n\n!rio info RGBNIR_of_S2A.tif --indent 2 --verbose\n\n\n\n\nWARNING:rasterio._env:CPLE_AppDefined in RGBNIR_of_S2A.tif: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWARNING:rasterio._env:CPLE_AppDefined in RGBNIR_of_S2A.tif: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWARNING:rasterio._env:CPLE_AppDefined in TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n{\n  \"blockxsize\": 2074,\n  \"blockysize\": 1926,\n  \"bounds\": [\n    731780.0,\n    5021540.0,\n    752520.0,\n    5040800.0\n  ],\n  \"checksum\": [\n    18623,\n    42114,\n    31774,\n    54171\n  ],\n  \"colorinterp\": [\n    \"gray\",\n    \"undefined\",\n    \"undefined\",\n    \"undefined\"\n  ],\n  \"count\": 4,\n  \"crs\": \"EPSG:32618\",\n  \"descriptions\": [\n    null,\n    null,\n    null,\n    null\n  ],\n  \"driver\": \"GTiff\",\n  \"dtype\": \"uint16\",\n  \"height\": 1926,\n  \"indexes\": [\n    1,\n    2,\n    3,\n    4\n  ],\n  \"interleave\": \"band\",\n  \"lnglat\": [\n    -71.90643373271799,\n    45.39214029576973\n  ],\n  \"mask_flags\": [\n    [\n      \"all_valid\"\n    ],\n    [\n      \"all_valid\"\n    ],\n    [\n      \"all_valid\"\n    ],\n    [\n      \"all_valid\"\n    ]\n  ],\n  \"nodata\": null,\n  \"res\": [\n    10.0,\n    10.0\n  ],\n  \"shape\": [\n    1926,\n    2074\n  ],\n  \"stats\": [\n    {\n      \"max\": 15104.0,\n      \"mean\": 1426.6252674912,\n      \"min\": 86.0,\n      \"std\": 306.56427126942\n    },\n    {\n      \"max\": 14352.0,\n      \"mean\": 1669.6050060032,\n      \"min\": 1139.0,\n      \"std\": 310.91935787639\n    },\n    {\n      \"max\": 15280.0,\n      \"mean\": 1471.3923473736,\n      \"min\": 706.0,\n      \"std\": 385.44654593014\n    },\n    {\n      \"max\": 15642.0,\n      \"mean\": 4393.94485025,\n      \"min\": 1067.0,\n      \"std\": 1037.933939728\n    }\n  ],\n  \"tiled\": false,\n  \"transform\": [\n    10.0,\n    0.0,\n    731780.0,\n    0.0,\n    -10.0,\n    5040800.0,\n    0.0,\n    0.0,\n    1.0\n  ],\n  \"units\": [\n    null,\n    null,\n    null,\n    null\n  ],\n  \"width\": 2074\n}",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Importation et manipulation de donn√©es spatiales</span>"
    ]
  },
  {
    "objectID": "01-ImportationManipulationImages.html#manipulation-des-images",
    "href": "01-ImportationManipulationImages.html#manipulation-des-images",
    "title": "2¬† Importation et manipulation de donn√©es spatiales",
    "section": "2.3 Manipulation des images",
    "text": "2.3 Manipulation des images\n\n2.3.1 Manipulation de la matrice de pixels\nLa donn√©e brute de l‚Äôimage est g√©n√©ralement contenue dans un cube matricielle √† trois dimensions (deux dimensions spatiales et une dimension spectrale). Comme expos√© pr√©c√©demment, la librairie dite ‚Äúfondationnelle‚Äù pour la manipulation de matrices en Python est NumPy. Cette librairie contient un nombre tr√®s important de fonctionnalit√©s couvrant l‚Äôalg√®bre lin√©aires, les statistiques, etc. et constitue la fondation de nombreuses librairies (voir (figure¬†2.1))\n\n\n\n\n\n\nFigure¬†2.1: La librairie NumPy est le fondement de nombreuses librairies scientifiques (d‚Äôapr√®s (Harris 2020)).\n\n\n\n\n\n2.3.2 Information de base\nLes deux informations de base √† afficher sur une matrice sont 1) les dimensions de la matrice et 2) le format de stockage (le type). Pour cela, on peut utiliser le (bloc¬†2.6), le r√©sultat nous informe que la matrice a 3 dimensions et une taille de (442, 553, 3) et un type uint8 qui repr√©sente 1 octet (8 bit). Par cons√©quent, la matrice a 442 lignes, 553 colonnes et 3 canaux ou bandes. Il faut pr√™ter une attention particuli√®re aux valeurs minimales et maximales tol√©r√©es par le type de la donn√©e comme indiqu√© dans le (tableau¬†2.1) (voir aussi Data types ‚Äî NumPy v2.1 Manual).\n\n\n\n\nBloc de code¬†2.6: Lecture d‚Äôune image en format PNG avec OpenCV\n\n\nimport cv2\nimg = cv2.imread('modis-aqua.PNG')\nprint('Nombre de dimensions: ',img.ndim)\nprint('Dimensions de la matrice: ',img.shape)\nprint('Type de la donn√©e: ',img.dtype)\n\n\n\n\nNombre de dimensions:  3\nDimensions de la matrice:  (442, 553, 3)\nType de la donn√©e:  uint8\n\n\n\nfrom IPython.display import Markdown\nfrom tabulate import tabulate\ntable = [[\"uint8\", \"char\", 8, 0, 255],\n        [\"int8\", \"signed char\", 8, -127, +128],\n        [\"uint16\", \"unsigned short\", 16, 0, -32768, +32767],\n        [\"int16\", \"short\", 16, 0, 655355]]\nMarkdown(tabulate(table, headers=[\"dtype\", \"Nom\", \"Taille (bits)\", \"Min\", \"Max\"], tablefmt=\"pipe\"))\n\n\n\nTableau¬†2.1: Type de donn√©es de NumPy\n\n\n\n\n\n\ndtype\nNom\nTaille (bits)\nMin\nMax\n\n\n\n\nuint8\nchar\n8\n0\n255\n\n\nint8\nsigned char\n8\n-127\n128\n\n\nuint16\nunsigned short\n16\n0\n-32768\n\n\nint16\nshort\n16\n0\n655355\n\n\n\n\n\n\n\n\n\n\n\n\n\nLes diff√©rents types de donn√©es en dans NumPy\n\n\nIl comprend des r√©f√©rences ou des extensions d‚Äôune m√©thode abord√©e dans une section.\n\n\n\n\n2.3.3 D√©coupage et indexation de la matrice\nL‚Äôindexation et le d√©coupage des matrices dans NumPy sont des techniques essentielles pour manipuler efficacement les donn√©es multidimensionnelles en Python, offrant une syntaxe puissante et flexible pour acc√©der et modifier des sous-ensembles sp√©cifiques d‚Äô√©l√©ments dans les tableaux (voir figure¬†2.2). Indexer une matrice consiste √† acc√©der √† une valeur dans la matrice pour une position particuli√®re, la syntaxe g√©n√©rale est matrice[ligne, colonne, bande] et est similaire √† la manipulation des listes en Python. Les indices commencent √† 0 et se termine √† la taille-1 de l‚Äôaxe consid√©r√©.\n\n\n\n\n\n\nFigure¬†2.2: Vue d‚Äôensemble des op√©rations de base des matrices avec NumPy\n\n\n\nLe d√©coupage (ou slicing en anglais) consiste √† produire une nouvelle matrice qui est un sous-ensemble de la matrice d‚Äôorigine. Un d√©coupage se fait avec le symbole ‚Äò:‚Äô, la syntaxe g√©n√©rale pour d√©finir un d√©coupage est [d√©but:fin:pas]. Si on ne sp√©cifie pas d√©but ou fin alors les valeurs 0 ou dimension-1 sont consid√©r√©es implicitement. Quelques exemples: * choisir un pixel en particulier avec toutes les bandes: matrice[1,1,:] * choisir la colonne 2: matrice[:,2,:]\nLa syntaxe de base pour le d√©coupage (slicing) des tableaux NumPy repose sur l‚Äôutilisation des deux-points (:) √† l‚Äôint√©rieur des crochets d‚Äôindexation. Cette notation permet de s√©lectionner des plages d‚Äô√©l√©ments de mani√®re concise et intuitive. La structure g√©n√©rale du d√©coupage est matrice[start:stop:step], o√π : 1. start repr√©sente l‚Äôindex de d√©part (inclus) 2. stop indique l‚Äôindex de fin (exclu) 3. step d√©finit l‚Äôintervalle entre chaque √©l√©ment s√©lectionn√©\nSi l‚Äôun de ces param√®tres est omis, NumPy utilise des valeurs par d√©faut : 0 pour start, la taille du tableau pour stop, et 1 pour step. Par exemple, pour un tableau unidimensionnel array, on peut extraire les √©l√©ments du deuxi√®me au quatri√®me avec array[1:4]. Pour s√©lectionner tous les √©l√©ments √† partir du troisi√®me, on utiliserait array[2:]. Cette syntaxe s‚Äôapplique √©galement aux tableaux multidimensionnels, o√π chaque dimension est s√©par√©e par une virgule. Ainsi, pour une matrice 2D m, m[0:2, 1:3] s√©lectionnerait une sous-matrice 2x2 compos√©e des deux premi√®res lignes et des deuxi√®me et troisi√®me colonnes. L‚Äôindexation n√©gative est √©galement support√©e, permettant de compter √† partir de la fin du tableau. Par exemple, a[-3:] s√©lectionnerait les trois derniers √©l√©ments d‚Äôun tableau.\n\nimport cv2\nimg = cv2.imread('modis-aqua.PNG')\nimg_col = img[:,1,:]\nprint('Nombre de dimensions: ',img_col.ndim)\nprint('Dimensions de la matrice: ',img_col.shape)\n\nNombre de dimensions:  2\nDimensions de la matrice:  (442, 3)\n\n\n\n\n\n\n\nUne vue versus une copie\n\n\nAvec NumPy, les manipulations peuvent cr√©er des vues ou des copies. Une vue est une simple repr√©sentation de la m√™me donn√©e originale alors qu‚Äôune copie est un nouvel espace m√©moire.\nPar d√©faut, un d√©coupage cr√©√© une vue.\nOn peut v√©rifier si l‚Äôespace m√©moire est partag√© avec np.shares_memory(arr, slice_arr).\nOn peut toujours forcer une copie avec la m√©thode copy()\n\n\n\n2.3.3.1 Exemple 1: calcul d‚Äôun rapport de bande\n\n\n2.3.3.2 Exemple 2: application d‚Äôun filtrage spatial\n\n\n\n2.3.4 Mosa√Øquage, masquage et d√©coupage\n\n2.3.4.1 Masquage\nL‚Äôutilisation d‚Äôun masque est un outil important en traitement d‚Äôimage car la plupart des images de t√©l√©d√©tection contiennent des pixels non valides qu‚Äôil faut exclure des traitements (ce que l‚Äôon appelle le no data en Anglais). Il y a plusieurs raison possibles pour la pr√©sence de pixels non valides: 1. L‚Äôimage est projet√©e dans une grille cartographique et certaines zones, g√©n√©ralement situ√©es en dehors de l‚Äôempreinte au sol du capteur, sont √† exclure. 2. La pr√©sence de nuages que l‚Äôon veut exclure. 3. La pr√©sence de pixels erron√©s d√ªs √† des probl√®mes de capteurs. 4. La pr√©sence de valeurs non num√©riques (not a number ou nan)\nLa librairie NumPy fournit des m√©canismes pour exclure automatiquement certaines valeurs.\n\n\n\n2.3.5 Changement de projection cartographique\n\n\n2.3.6 Recalage d‚Äôimages et co-registration",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Importation et manipulation de donn√©es spatiales</span>"
    ]
  },
  {
    "objectID": "01-ImportationManipulationImages.html#donn√©es-en-g√©oscience",
    "href": "01-ImportationManipulationImages.html#donn√©es-en-g√©oscience",
    "title": "2¬† Importation et manipulation de donn√©es spatiales",
    "section": "2.4 Donn√©es en g√©oscience",
    "text": "2.4 Donn√©es en g√©oscience\nLes donn√©es en g√©oscience contiennent beaucoup de m√©tadonn√©es et peuvent √™tre compos√©es de diff√©rentes variables avec diff√©rentes unit√©s, r√©solution, etc. Ces donn√©es sont aussi souvent √©tiquet√©es avec des dates sur certains axes, des coordonn√©es g√©ographiques, des identifiants d‚Äôexp√©riences, etc. Par cons√©quent, utiliser seulement des matrices est souvent incomplet (Hoyer et Hamman 2017).\nCalibration, unit√©s, donn√©es manquantes, donn√©es √©parses.\n\n2.4.1 xarray\nXarray est une puissante biblioth√®que Python qui am√©liore les matrices multidimensionnelles de type numpy en y ajoutant des √©tiquettes, des dimensions, des coordonn√©es et des attributs. Elle fournit deux structures de donn√©es principales : DataArray (un tableau √©tiquet√© √† N dimensions) et Dataset (une base de donn√©es de tableaux multidimensionnels en m√©moire).\nLes caract√©ristiques principales sont les suivantes:\n\nOp√©rations sur les dimensions nomm√©es au lieu des num√©ros d‚Äôaxe\nS√©lection et op√©rations bas√©es sur les √©tiquettes\nDiffusion automatique de tableaux bas√©e sur les noms de dimensions\nAlignement de type base de donn√©es avec des √©tiquettes de coordonn√©es\nSuivi des m√©tadonn√©es gr√¢ce aux dictionnaires Python\n\n\n2.4.1.1 Avantages\nLa biblioth√®que r√©duit consid√©rablement la complexit√© du code et am√©liore la lisibilit√© du code pour les applications de calcul scientifique dans divers domaines, notamment la physique, l‚Äôastronomie, les g√©osciences, la bio-informatique, l‚Äôing√©nierie, la finance et l‚Äôapprentissage profond. Elle s‚Äôint√®gre de mani√®re transparente avec NumPy et pandas tout en restant compatible avec l‚Äô√©cosyst√®me Python au sens large.\n\n\n2.4.1.2 DataArray\nUn tableau multidimensionnel √©tiquet√© avec des propri√©t√©s cl√©es :\n\nvaleurs : Les donn√©es r√©elles du tableau\ndims : Dimensions nomm√©es (par exemple, ¬´ x ¬ª, ¬´ y ¬ª, ¬´ z ¬ª)\ncoords : Dictionnaire de tableaux √©tiquetant chaque point\nattrs : Stockage de m√©tadonn√©es arbitraires\nname : Identifiant facultatif\n\n\n\n2.4.1.3 Dataset\nUn conteneur de type dictionnaire de DataArrays avec des dimensions align√©es, contenant :\n\ndims : Dictionnaire de correspondance entre les noms des dimensions et les longueurs\ndata_vars : Dictionnaire des variables du DataArray\ncoords : Dictionnaire des variables de coordonn√©es\nattrs : Stockage des m√©tadonn√©es\n\nLes principales diff√©rences sont les suivantes :\n\nDataArray contient un seul tableau avec des √©tiquettes\nLe Dataset contient plusieurs DataArrays align√©s.\n\nCes trois structures prennent en charge les op√©rations de type dictionnaire et les calculs de coordination tout en conservant les m√©tadonn√©es.\n\n\n\nOrganisation d‚Äôun Dataset dans xarray\n\n\nnetcdf, xarray, GRIB.\nDonn√©es m√©t√©os, exemple avec SWOT.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Importation et manipulation de donn√©es spatiales</span>"
    ]
  },
  {
    "objectID": "01-ImportationManipulationImages.html#importation-de-donn√©es-vectorielles",
    "href": "01-ImportationManipulationImages.html#importation-de-donn√©es-vectorielles",
    "title": "2¬† Importation et manipulation de donn√©es spatiales",
    "section": "2.5 Importation de donn√©es vectorielles",
    "text": "2.5 Importation de donn√©es vectorielles\n\n2.5.1 Importation d‚Äôun fichier shapefile\n\n\n2.5.2 Importation d‚Äôune couche dans un GeoPackage\n\n\n2.5.3 Importation d‚Äôune couche dans une geodatabase d‚ÄôESRI\n\n\n2.5.4 Importation d‚Äôun fichier shapefile",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Importation et manipulation de donn√©es spatiales</span>"
    ]
  },
  {
    "objectID": "01-ImportationManipulationImages.html#manipulation-de-donn√©es-vectorielles",
    "href": "01-ImportationManipulationImages.html#manipulation-de-donn√©es-vectorielles",
    "title": "2¬† Importation et manipulation de donn√©es spatiales",
    "section": "2.6 Manipulation de donn√©es vectorielles",
    "text": "2.6 Manipulation de donn√©es vectorielles\n\n2.6.1 Requ√™tes attributaires",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Importation et manipulation de donn√©es spatiales</span>"
    ]
  },
  {
    "objectID": "01-ImportationManipulationImages.html#exercices-de-r√©vision",
    "href": "01-ImportationManipulationImages.html#exercices-de-r√©vision",
    "title": "2¬† Importation et manipulation de donn√©es spatiales",
    "section": "2.7 Exercices de r√©vision",
    "text": "2.7 Exercices de r√©vision\n\n\n\n\nHarris, Millman, C. R. 2020. ¬´¬†Array programming with NumPy.¬†¬ª Nature: 357‚Äë362. https://doi.org/10.1038/s41586-020-2649-2.\n\n\nHoyer, S. et J. Hamman. 2017. ¬´¬†xarray: N-D labeled Arrays and Datasets in Python.¬†¬ª Journal of Open Research Software 5 (1): 10. https://doi.org/10.5334/jors.148.\n\n\nOGC. 2019. ¬´¬†OGC GeoTIFF Standard.¬†¬ª https://docs.ogc.org/is/19-008r4/19-008r4.html/.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Importation et manipulation de donn√©es spatiales</span>"
    ]
  },
  {
    "objectID": "02-RehaussementVisualisationImages.html",
    "href": "02-RehaussementVisualisationImages.html",
    "title": "3¬† R√©haussement et visualisation d‚Äôimages",
    "section": "",
    "text": "3.1 üöÄ Pr√©ambule",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>R√©haussement et visualisation d'images</span>"
    ]
  },
  {
    "objectID": "02-RehaussementVisualisationImages.html#pr√©ambule",
    "href": "02-RehaussementVisualisationImages.html#pr√©ambule",
    "title": "3¬† R√©haussement et visualisation d‚Äôimages",
    "section": "",
    "text": "3.1.1 üéØ Objectifs\nDans ce chapitre, nous abordons quelques techniques de r√©haussement et de visualisation d‚Äôimages. Ce chapitre est aussi disponible sous la forme d‚Äôun notebook Python:\n\n\n\n3.1.2 Librairies\nLes librairies qui vont √™tre explor√©es dans ce chapitre sont les suivantes:\n\nSciPy -\nNumPy -\nopencv-python ¬∑ PyPI\nscikit-image\nRasterio\nXarray\nrioxarray\n\nDans l‚Äôenvironnement Google Colab, seul rioxarray et GDAL doivent √™tre install√©s:\n\n%%capture\n!apt-get update\n!apt-get install gdal-bin libgdal-dev\n!pip install -q rioxarray\n!pip install -qU \"geemap[workshop]\"\n\nV√©rifier les importations:\n\nimport numpy as np\nimport rioxarray as rxr\nfrom scipy import signal\nimport xarray as xr\nimport xrscipy\nimport matplotlib.pyplot as plt\n\n\n\n3.1.3 Donn√©es\nNous allons utilisez les images suivantes dans ce chapitre:\n\n%%capture\n!wget https://github.com/sfoucher/TraitementImagesPythonVol1/raw/refs/heads/main/data/chapitre01/subset_RGBNIR_of_S2A_MSIL2A_20240625T153941_N0510_R011_T18TYR_20240625T221903.tif -O RGBNIR_of_S2A.tif\n!wget https://github.com/sfoucher/opengeos-data/raw/refs/heads/main/raster/landsat7.tif -O landsat7.tif\n!wget https://github.com/sfoucher/opengeos-data/raw/refs/heads/main/images/berkeley.jpg -O berkeley.jpg\n!wget https://github.com/sfoucher/TraitementImagesPythonVol1/raw/refs/heads/main/data/chapitre01/subset_0_of_S1A_split_NR_Cal_Deb_ML_Spk_SRGR.tif -O SAR.tif\n\nV√©rifiez que vous √™tes capable de les lire :\n\nwith rxr.open_rasterio('berkeley.jpg', mask_and_scale= True) as img_rgb:\n    print(img_rgb)\nwith rxr.open_rasterio('RGBNIR_of_S2A.tif', mask_and_scale= True) as img_rgbnir:\n    print(img_rgbnir)\nwith rxr.open_rasterio('SAR.tif', mask_and_scale= True) as img_SAR:\n    print(img_SAR)",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>R√©haussement et visualisation d'images</span>"
    ]
  },
  {
    "objectID": "02-RehaussementVisualisationImages.html#r√©haussements-visuels",
    "href": "02-RehaussementVisualisationImages.html#r√©haussements-visuels",
    "title": "3¬† R√©haussement et visualisation d‚Äôimages",
    "section": "3.2 R√©haussements visuels",
    "text": "3.2 R√©haussements visuels\nLe but du r√©haussement visuel d‚Äôune image vise principalement √† am√©liorer la qualit√© visuelle d‚Äôune image en am√©liorant le contraste, la dynamique ou la texture d‚Äôune image. De mani√®re g√©n√©rale, ce r√©haussement ne modifie pas la donn√©e d‚Äôorigine mais est plut√¥t appliqu√©e dynamiquement √† l‚Äôaffichage pour des fins d‚Äôinspection visuelle.\n\n3.2.1 Statistiques d‚Äôune image\nOn peut consid√©rer un ensemble de statistique globales pour chacune des bandes d‚Äôune image: - valeurs minimales et maximales - valeurs moyennes, m√©dianes et quantiles - √©cart-types, skewness et kurtosis Ces statistiques doivent √™tre calcul√©es pour chaque bande d‚Äôune image multispectrale.\nEn ligne de commande, gdalinfo permet d‚Äôinterroger rapidement un fichier image pour connaitre les statistiques de base:\n\n\n\n\nBloc de code¬†3.1: Statistiques d‚Äôune image avec gdal\n\n\n!gdalinfo -stats landsat7.tif\n\n\n\n\nDriver: GTiff/GeoTIFF\nFiles: landsat7.tif\n       landsat7.tif.aux.xml\nSize is 2181, 1917\nCoordinate System is:\nPROJCS[\"WGS 84 / Pseudo-Mercator\",\n    GEOGCS[\"WGS 84\",\n        DATUM[\"WGS_1984\",\n            SPHEROID[\"WGS 84\",6378137,298.257223563,\n                AUTHORITY[\"EPSG\",\"7030\"]],\n            AUTHORITY[\"EPSG\",\"6326\"]],\n        PRIMEM[\"Greenwich\",0,\n            AUTHORITY[\"EPSG\",\"8901\"]],\n        UNIT[\"degree\",0.0174532925199433,\n            AUTHORITY[\"EPSG\",\"9122\"]],\n        AUTHORITY[\"EPSG\",\"4326\"]],\n    PROJECTION[\"Mercator_1SP\"],\n    PARAMETER[\"central_meridian\",0],\n    PARAMETER[\"scale_factor\",1],\n    PARAMETER[\"false_easting\",0],\n    PARAMETER[\"false_northing\",0],\n    UNIT[\"metre\",1,\n        AUTHORITY[\"EPSG\",\"9001\"]],\n    AXIS[\"X\",EAST],\n    AXIS[\"Y\",NORTH],\n    EXTENSION[\"PROJ4\",\"+proj=merc +a=6378137 +b=6378137 +lat_ts=0.0 +lon_0=0.0 +x_0=0.0 +y_0=0 +k=1.0 +units=m +nadgrids=@null +wktext +no_defs\"],\n    AUTHORITY[\"EPSG\",\"3857\"]]\nOrigin = (-13651650.000000000000000,4576290.000000000000000)\nPixel Size = (30.000000000000000,-30.000000000000000)\nMetadata:\n  AREA_OR_POINT=Area\n  OVR_RESAMPLING_ALG=NEAREST\n  TIFFTAG_RESOLUTIONUNIT=1 (unitless)\n  TIFFTAG_XRESOLUTION=1\n  TIFFTAG_YRESOLUTION=1\nImage Structure Metadata:\n  COMPRESSION=DEFLATE\n  INTERLEAVE=PIXEL\nCorner Coordinates:\nUpper Left  (-13651650.000, 4576290.000) (122d38' 5.49\"W, 37d58'40.08\"N)\nLower Left  (-13651650.000, 4518780.000) (122d38' 5.49\"W, 37d34'10.00\"N)\nUpper Right (-13586220.000, 4576290.000) (122d 2'49.53\"W, 37d58'40.08\"N)\nLower Right (-13586220.000, 4518780.000) (122d 2'49.53\"W, 37d34'10.00\"N)\nCenter      (-13618935.000, 4547535.000) (122d20'27.51\"W, 37d46'26.05\"N)\nBand 1 Block=512x512 Type=Byte, ColorInterp=Red\n  Min=19.000 Max=233.000 \n  Minimum=19.000, Maximum=233.000, Mean=98.433, StdDev=21.164\n  NoData Value=0\n  Overviews: 1091x959, 546x480\n  Metadata:\n    STATISTICS_MAXIMUM=233\n    STATISTICS_MEAN=98.433096940153\n    STATISTICS_MINIMUM=19\n    STATISTICS_STDDEV=21.164021026458\nBand 2 Block=512x512 Type=Byte, ColorInterp=Green\n  Min=19.000 Max=178.000 \n  Minimum=19.000, Maximum=178.000, Mean=55.068, StdDev=22.204\n  NoData Value=0\n  Overviews: 1091x959, 546x480\n  Metadata:\n    STATISTICS_MAXIMUM=178\n    STATISTICS_MEAN=55.067787534804\n    STATISTICS_MINIMUM=19\n    STATISTICS_STDDEV=22.203571974581\nBand 3 Block=512x512 Type=Byte, ColorInterp=Blue\n  Min=19.000 Max=187.000 \n  Minimum=19.000, Maximum=187.000, Mean=43.341, StdDev=20.330\n  NoData Value=0\n  Overviews: 1091x959, 546x480\n  Metadata:\n    STATISTICS_MAXIMUM=187\n    STATISTICS_MEAN=43.340507443056\n    STATISTICS_MINIMUM=19\n    STATISTICS_STDDEV=20.32987736339\n\n\nLes librairies de base comme xarray et numpy peuvent facilement produire des statistiques comme avec la fonction stats:\n\nimport rasterio as rio\nimport numpy as np\nwith rio.open('landsat7.tif') as src:\n    stats= src.stats()\n    print(stats)\n\nLa librairie xarray donne acc√®s √† des fonctionnalit√©s plus sophistiqu√©es comme le calcul des quantiles:\n\nimport rioxarray as riox\nwith riox.open_rasterio('landsat7.tif', masked= True) as src:\n    print(src)\nquantiles = src.quantile(dim=['x','y'], q=[.025,.25,.5,.75,.975])\nquantiles\n\n&lt;xarray.DataArray (band: 3, y: 1917, x: 2181)&gt; Size: 50MB\n[12542931 values with dtype=float32]\nCoordinates:\n  * band         (band) int64 24B 1 2 3\n  * x            (x) float64 17kB -1.365e+07 -1.365e+07 ... -1.359e+07\n  * y            (y) float64 15kB 4.576e+06 4.576e+06 ... 4.519e+06 4.519e+06\n    spatial_ref  int64 8B 0\nAttributes:\n    AREA_OR_POINT:           Area\n    OVR_RESAMPLING_ALG:      NEAREST\n    TIFFTAG_RESOLUTIONUNIT:  1 (unitless)\n    TIFFTAG_XRESOLUTION:     1\n    TIFFTAG_YRESOLUTION:     1\n    STATISTICS_MAXIMUM:      233\n    STATISTICS_MEAN:         98.433096940153\n    STATISTICS_MINIMUM:      19\n    STATISTICS_STDDEV:       21.164021026458\n    scale_factor:            1.0\n    add_offset:              0.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray (quantile: 5, band: 3)&gt; Size: 120B\narray([[ 54.,  19.,  19.],\n       [ 85.,  38.,  27.],\n       [ 99.,  54.,  38.],\n       [111.,  69.,  57.],\n       [140., 102.,  89.]])\nCoordinates:\n  * band      (band) int64 24B 1 2 3\n  * quantile  (quantile) float64 40B 0.025 0.25 0.5 0.75 0.975xarray.DataArrayquantile: 5band: 354.0 19.0 19.0 85.0 38.0 27.0 ... 111.0 69.0 57.0 140.0 102.0 89.0array([[ 54.,  19.,  19.],\n       [ 85.,  38.,  27.],\n       [ 99.,  54.,  38.],\n       [111.,  69.,  57.],\n       [140., 102.,  89.]])Coordinates: (2)band(band)int641 2 3array([1, 2, 3])quantile(quantile)float640.025 0.25 0.5 0.75 0.975array([0.025, 0.25 , 0.5  , 0.75 , 0.975])Indexes: (2)bandPandasIndexPandasIndex(Index([1, 2, 3], dtype='int64', name='band'))quantilePandasIndexPandasIndex(Index([0.025, 0.25, 0.5, 0.75, 0.975], dtype='float64', name='quantile'))Attributes: (0)\n\n\n\n3.2.1.1 Calcul de l‚Äôhistogramme\nLe calcul d‚Äôun histogramme pour une image (une bande) permet d‚Äôavoir une vue plus d√©taill√©e de la r√©partition des valeurs radiom√©triques. Le calcul d‚Äôun histogramme n√©cessite minimalement de faire le choix d‚Äôune valeur du nombre de bins (ou de la largeur). Un bin est un intervalle de valeurs pour lequel on peut calculer le nombre de valeurs observ√©es dans l‚Äôimage. La fonction de base pour ce type de calcul est la fonction numpy.histogram():\n\nimport numpy as np\narray = np.random.randint(0,10,100) # 100 valeurs al√©atoires entre 0 et 10\nhist, bin_limites = np.histogram(array, density=True)\nprint('valeurs :',hist)\nprint(';imites :',bin_limites)\n\nvaleurs : [0.11111111 0.16666667 0.12222222 0.06666667 0.07777778 0.11111111\n 0.11111111 0.08888889 0.16666667 0.08888889]\n;imites : [0.  0.9 1.8 2.7 3.6 4.5 5.4 6.3 7.2 8.1 9. ]\n\n\nLe calcul se fait avec 10 intervalles par d√©faut.\nPour des besoins de visualisation, le calcul des valeurs extr√™mes de l‚Äôhistogramme peut aussi se faire via les quantiles comme discut√©s auparavant.\n\n3.2.1.1.1 Visualisation des histogrammes\nLa librarie rasterio est probablement l‚Äôoutil le plus simples pour visualiser rapidement des histogrammes sur une image multi-spectrale:\n\nimport rasterio as rio\nfrom rasterio.plot import show_hist\nwith rio.open('RGBNIR_of_S2A.tif') as src:\n  show_hist(src, bins=50, lw=0.0, stacked=False, alpha=0.3,histtype='stepfilled', title=\"Histogram\")\n\n\n\n\n\n\n\n\n\n\n\n\n3.2.2 R√©haussements lin√©aires\nLe r√©haussement lin√©aire d‚Äôune image est la forme la plus simple de r√©haussement, elle consiste 1) √† optimiser les valeurs des pixels d‚Äôune image afin de maximiser la dynamique disponibles √† l‚Äôaffichage, ou 2) changer le format de stockage des valeurs (e.g.¬†de 8 bit √† 16 bit):\n\\[ \\text{nouvelle valeur d'un pixel} = \\frac{\\text{valeur d'un pixel} - min_0}{max_0 - min_0}\\times (max_1 - min_1)+min_1 \\tag{3.1}\\]\nPar cette op√©ration, on passe de la dynamique de d√©part (\\(max_0 - min_0\\)) vers la dynamique cible (\\(max_1 - min_1\\)). Bien que cette op√©ration semble triviale, il est important d‚Äô√™tre conscient des trois contraintes suivantes: 1. Faire attention √† la dynamique cible, ainsi, pour sauvegarder une image en format 8 bit, on utilisera alors \\(max_1=255\\) et \\(min_1=0\\). 2. Pr√©servation de la valeur de no data : il faut faire attention √† la valeur \\(min_1\\) dans le cas d‚Äôune valeur pr√©sente pour no_data. Par exemple, si no_data=0 alors il faut s‚Äôassurer que \\(min_1&gt;0\\). 3. Pr√©cision du calcul : si possible r√©aliser la division ci-dessus en format float\n\n\n3.2.3 R√©haussements non lin√©aires\nCalcul d‚Äôhistogrammes, √©tirement, √©galisation, styling\n\n\n3.2.4 Compos√©s couleurs\nLe syst√®me visuel humain est sensible seulement √† la partie visible du spectre √©lectromagn√©tique qui compose les couleurs de l‚Äôarc-en-ciel du bleu au rouge. L‚Äôensemble des couleurs du spectre visible peut √™tre obtenu √† partir du m√©lange de trois couleurs primaires (rouge, vert et bleu). Ce syst√®me de d√©composition √† trois couleurs est √† la base de la plupart des syst√®mes de visualisation ou de repr√©sentation de l‚Äôinformation de couleur. On peut trouver des variantes comme le syst√®me HSV (Hue-Saturation-Value) utilis√© en encodage de donn√©es vid√©os.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>R√©haussement et visualisation d'images</span>"
    ]
  },
  {
    "objectID": "02-RehaussementVisualisationImages.html#visualisation",
    "href": "02-RehaussementVisualisationImages.html#visualisation",
    "title": "3¬† R√©haussement et visualisation d‚Äôimages",
    "section": "3.3 Visualisation",
    "text": "3.3 Visualisation\n\n3.3.1 Visualisation en Python\nIl faut d‚Äôentr√©e mentionner que Python n‚Äôest pas vraiment fait pour visualiser de la donn√©e de grande taille, le niveau d‚Äôinteractivit√© est aussi plus limit√©. N√©anmoins, il est possible de visualiser de petites images avec la librairie Matplotlib.\n\n\n3.3.2 Outils de visualisation\nIl existe plusieurs outils gratuits de visualisation d‚Äôune image satellite, on peut mentionner les deux principaux: - QGIS - ESA Snap\n\n\n3.3.3 Visualisation sur le Web\nUne des meilleures pratiques pour visualiser une image de grande taille est d‚Äôutiliser un service de type Web Mapping Service (WMS). Cependant, type de service n√©cessite une architecture client-serveur qui est plus complexe √† mettre en place.\nGoogle Earth Engine offre des moyens de visualiser de la donn√©e locale: Working with Local Geospatial Data ‚Äî via 17. Geemap ‚Äî Introduction to GIS Programming\nvia data/raster at main ¬∑ opengeos/data\n\n\n3.3.4 Visualisation 3D\ndrapper une image satellite sur un DEM",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>R√©haussement et visualisation d'images</span>"
    ]
  },
  {
    "objectID": "02-RehaussementVisualisationImages.html#exercices-de-r√©vision",
    "href": "02-RehaussementVisualisationImages.html#exercices-de-r√©vision",
    "title": "3¬† R√©haussement et visualisation d‚Äôimages",
    "section": "3.4 Exercices de r√©vision",
    "text": "3.4 Exercices de r√©vision",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>R√©haussement et visualisation d'images</span>"
    ]
  },
  {
    "objectID": "03-TransformationSpectrales.html",
    "href": "03-TransformationSpectrales.html",
    "title": "4¬† Transformations spectrales",
    "section": "",
    "text": "4.1 üöÄ Pr√©ambule\nAssurez-vous de lire ce pr√©ambule avant d‚Äôex√©cutez le reste du notebook. ### üéØ Objectifs Dans ce chapitre, nous abordons quelques techniques de r√©haussement et de visualisation d‚Äôimages. Ce chapitre est aussi disponible sous la forme d‚Äôun notebook Python:",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Transformations spectrales</span>"
    ]
  },
  {
    "objectID": "03-TransformationSpectrales.html#pr√©ambule",
    "href": "03-TransformationSpectrales.html#pr√©ambule",
    "title": "4¬† Transformations spectrales",
    "section": "",
    "text": "4.1.1 Librairies\nLes librairies qui vont √™tre explor√©es dans ce chapitre sont les suivantes:\n\nSciPy -\nNumPy -\nopencv-python ¬∑ PyPI\nscikit-image\nRasterio\nXarray\nrioxarray\n\nDans l‚Äôenvironnement Google Colab, seul rioxarray et GDAL doivent √™tre install√©s:\n\n%%capture\n!apt-get update\n!apt-get install gdal-bin libgdal-dev\n!pip install -q rioxarray\n!pip install -qU \"geemap[workshop]\"\n\nV√©rifier les importations:\n\nimport numpy as np\nimport rioxarray as rxr\nfrom scipy import signal\nimport xarray as xr\nimport xrscipy\nimport matplotlib.pyplot as plt\n\n\n\n4.1.2 Images utilis√©es\nNous allons utilisez les images suivantes dans ce chapitre:\n\n%%capture\n!wget https://github.com/sfoucher/TraitementImagesPythonVol1/raw/refs/heads/main/data/chapitre01/subset_RGBNIR_of_S2A_MSIL2A_20240625T153941_N0510_R011_T18TYR_20240625T221903.tif -O RGBNIR_of_S2A.tif\n!wget https://github.com/sfoucher/opengeos-data/raw/refs/heads/main/raster/landsat7.tif -O landsat7.tif\n!wget https://github.com/sfoucher/opengeos-data/raw/refs/heads/main/images/berkeley.jpg -O berkeley.jpg\n!wget https://github.com/sfoucher/TraitementImagesPythonVol1/raw/refs/heads/main/data/chapitre01/subset_1_of_S2A_MSIL2A_20240625T153941_N0510_R011_T18TYR_20240625T221903_resampled.tif -O sentinel2.tif\n\nV√©rifiez que vous √™tes capable de les lire :\n\nwith rxr.open_rasterio('berkeley.jpg', mask_and_scale= True) as img_rgb:\n    print(img_rgb)\nwith rxr.open_rasterio('RGBNIR_of_S2A.tif', mask_and_scale= True) as img_rgbnir:\n    print(img_rgbnir)\nwith rxr.open_rasterio('sentinel2.tif', mask_and_scale= True) as img_s2:\n    print(img_s2)",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Transformations spectrales</span>"
    ]
  },
  {
    "objectID": "03-TransformationSpectrales.html#quest-ce-que-linformation-spectrale",
    "href": "03-TransformationSpectrales.html#quest-ce-que-linformation-spectrale",
    "title": "4¬† Transformations spectrales",
    "section": "4.2 Qu‚Äôest ce que l‚Äôinformation spectrale?",
    "text": "4.2 Qu‚Äôest ce que l‚Äôinformation spectrale?\nL‚Äôinformation spectrale touche √† l‚Äôexploitation de la dimension spectrale des images (c.√†.d le long des bandes spectrales de l‚Äôimage). La taille de cette dimension spectrale d√©pend du type de capteurs consid√©r√©. Un capteur √† tr√®s haute r√©solution spatiale par exemple aura tr√®s peu de bandes (4 ou 5). Un capteur multispectral pourra contenir une quinzaine de bande. √Ä l‚Äôautre extr√™me, on trouvera les capteurs hyperspectraux qui peuvent contenir des centaines de bandes spectrales.\nPour une surface donn√©e, la forme des valeurs le long de l‚Äôaxe spectrale caract√©rise le type de mat√©riau observ√© ainsi que son √©tat. On parle souvent alors de signature spectrale. On peut voir celle-ci comme une g√©n√©ralisation de la couleur d‚Äôun mat√©riau au del√† des bandes visibles du spectre. L‚Äôexploitation de ces signatures spectrales est probablement un des principes les plus importants en t√©l√©d√©tection qui le distingue de la vison par ordinateur.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Transformations spectrales</span>"
    ]
  },
  {
    "objectID": "03-TransformationSpectrales.html#indices-spectraux",
    "href": "03-TransformationSpectrales.html#indices-spectraux",
    "title": "4¬† Transformations spectrales",
    "section": "4.3 Indices spectraux",
    "text": "4.3 Indices spectraux\nIl existe une vaste litt√©rature sur les indices spectraux, le choix d‚Äôun indice plut√¥t qu‚Äôun autre d√©pend fortement de l‚Äôapplication vis√©e, nous allons simplement couvrir les principes de base ici.\nLe principe d‚Äôun indice spectral consiste √† mettre en valeur certaines caract√©ristiques du spectre comme des pentes, des gradients, etc.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Transformations spectrales</span>"
    ]
  },
  {
    "objectID": "03-TransformationSpectrales.html#r√©duction-de-dimension",
    "href": "03-TransformationSpectrales.html#r√©duction-de-dimension",
    "title": "4¬† Transformations spectrales",
    "section": "4.4 R√©duction de dimension",
    "text": "4.4 R√©duction de dimension\nLa r√©duction de dimension vise √† ne retenir que l‚Äôinformation principale d‚Äôun jeu de donn√©es. L‚Äôobjectif est parfois d‚Äô√©liminer le bruit d‚Äôun capteur ou de faciliter la visualisation en ne retenant que 3 bandes principales. Le degr√© d‚Äôinformation est souvent mesur√© par la variance d‚Äôune bande, c‚Äôest √† dire son contraste. L‚Äôanalyse en composante principale vise alors √† ranger l‚Äôinformation contenue dans une image en ordre de variance d√©croissante.\n\n4.4.1 Analyses en composantes principales\nL‚Äôanalyse en composantes principales (ACP) est probablement la plus employ√©e. En th√©orie, l‚ÄôACP n‚Äôest valide seulement que sur des donn√©es Gaussiennes c‚Äôest √† dire que le nuage de points des donn√©es a la forme d‚Äôune ellipse √† N dimensions. Cette ellipse est caract√©ris√©e par des directions principales (grand axe versus petit axe). La premi√®re composante est celle du grand axe de l‚Äôellipse pour laquelle la donn√©e pr√©sente le maximum de variation. L‚ÄôACP est une d√©composition lin√©aire, c‚Äôest √† dire que les composantes principales sont des sommes pond√©r√©es des valeurs originales.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Transformations spectrales</span>"
    ]
  },
  {
    "objectID": "03-TransformationSpectrales.html#exercices-de-r√©vision",
    "href": "03-TransformationSpectrales.html#exercices-de-r√©vision",
    "title": "4¬† Transformations spectrales",
    "section": "4.5 Exercices de r√©vision",
    "text": "4.5 Exercices de r√©vision",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Transformations spectrales</span>"
    ]
  },
  {
    "objectID": "04-TransformationSpatiales.html",
    "href": "04-TransformationSpatiales.html",
    "title": "5¬† Transformations spatiales",
    "section": "",
    "text": "5.1 üöÄ Pr√©ambule\nAssurez-vous de lire ce pr√©ambule avant d‚Äôex√©cutez le reste du notebook.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Transformations spatiales</span>"
    ]
  },
  {
    "objectID": "04-TransformationSpatiales.html#pr√©ambule",
    "href": "04-TransformationSpatiales.html#pr√©ambule",
    "title": "5¬† Transformations spatiales",
    "section": "",
    "text": "5.1.1 üéØ Objectifs\nDans ce chapitre, nous abordons quelques techniques de traitement d‚Äôimages dans le domaine spatial uniquement. Ce chapitre est aussi disponible sous la forme d‚Äôun notebook Python sur Google Colab:\n\n\n\n5.1.2 Librairies\nLes librairies qui vont √™tre explor√©es dans ce chapitre sont les suivantes:\n\nSciPy -\nNumPy -\nopencv-python ¬∑ PyPI\nscikit-image\nRasterio\nXarray\nrioxarray\n\nDans l‚Äôenvironnement Google Colab, seul rioxarray doit √™tre install√©s:\n\n%%capture\n!pip install -qU matplotlib rioxarray xrscipy scikit-image\n\nV√©rifier les importations:\n\nimport numpy as np\nimport rioxarray as rxr\nfrom scipy import signal\nimport xarray as xr\nimport xrscipy\nimport matplotlib.pyplot as plt\nfrom skimage import data, measure, graph, segmentation, color\nimport pandas as pd\n\n\n\n5.1.3 Images utilis√©es\nNous allons utilisez les images suivantes dans ce chapitre:\n\n%%capture\nimport gdown\n\ngdown.download('https://drive.google.com/uc?export=download&confirm=pbef&id=1a6Ypg0g1Oy4AJt9XWKWfnR12NW1XhNg_', output= 'RGBNIR_of_S2A.tif')\ngdown.download('https://drive.google.com/uc?export=download&confirm=pbef&id=1a4PQ68Ru8zBphbQ22j0sgJ4D2quw-Wo6', output= 'landsat7.tif')\ngdown.download('https://drive.google.com/uc?export=download&confirm=pbef&id=1_zwCLN-x7XJcNHJCH6Z8upEdUXtVtvs1', output= 'berkeley.jpg')\ngdown.download('https://drive.google.com/uc?export=download&confirm=pbef&id=1dM6IVqjba6GHwTLmI7CpX8GP2z5txUq6', output= 'SAR.tif')\n\nV√©rifiez que vous √™tes capable de les lire :\n\nwith rxr.open_rasterio('berkeley.jpg', mask_and_scale= True) as img_rgb:\n    print(img_rgb)\nwith rxr.open_rasterio('RGBNIR_of_S2A.tif', mask_and_scale= True) as img_rgbnir:\n    print(img_rgbnir)\nwith rxr.open_rasterio('SAR.tif', mask_and_scale= True) as img_SAR:\n    print(img_SAR)",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Transformations spatiales</span>"
    ]
  },
  {
    "objectID": "04-TransformationSpatiales.html#analyse-fr√©quentielle",
    "href": "04-TransformationSpatiales.html#analyse-fr√©quentielle",
    "title": "5¬† Transformations spatiales",
    "section": "5.2 Analyse fr√©quentielle",
    "text": "5.2 Analyse fr√©quentielle\nL‚Äôanalyse fr√©quentielle, issue du traitement du signal, permet d‚Äôavoir un autre point de vue sur les donn√©es √† partir de ses composantes harmoniques. La modifications de ces composantes de Fourier modifie l‚Äôensemble de l‚Äôimage et permet de corriger des probl√®mes syst√©matiques comme des artefacts ou du bruit de capteur. Bien que ce domaine soit un peu √©loign√© de la t√©l√©d√©tection, les images fourniment par les capteurs sont tous sujets √† des √©tapes de traitement du signal et il faut donc en conna√Ætre les grands principes afin de pouvoir comprendre certains enjeux lors des traitements.\n\n5.2.1 La transform√©e de Fourier\nLa transform√©e de Fourier permet de transformer une image dans un espace fr√©quentielle. Cette transform√©e est compl√®tement reversible. Dans le cas des images num√©riques, on parle de 2D-DFT (2D-Discrete Fourier Transform) qui est un algorithme optimis√© pour le calcul fr√©quentiel (Cooley et Tukey 1965). La 1D-DFT peu s‚Äô√©crire simplement comme une projection sur une s√©rie d‚Äôexponentielles complexes:\n\\[X[k] = \\sum_{n=0 \\ldots N-1} x[n] \\times \\exp(-j \\times 2\\pi \\times k \\times n/N)) \\tag{5.1}\\]\nLa transform√©e inverse prend une forme similaire:\n\\[x[k] = \\frac{1}{N}\\sum_{n=0 \\ldots N-1} X[n] \\times \\exp(j \\times 2\\pi \\times k \\times n/N)) \\tag{5.2}\\]\nLe signal d‚Äôorigine est donc reconstruit √† partir d‚Äôune somme de sinuso√Øde complexe \\(\\exp(j2\\pi \\frac{k}{N}n))\\) de fr√©quence \\(k/N\\). Noter qu‚Äô√† partir de \\(k=N/2\\), les sinuso√Ødes se r√©p√®tent √† un signe pr√®s et forme un miroir des composantes, la convention est lors de mettre ces composantes dans une espace n√©gatif \\([-N/2,\\ldots,-1]\\).\nDans le cas d‚Äôun simple signal p√©riodique √† une dimension avec une fr√©quence de 4/16 (donc 4 p√©riodes sur 16) on obtient deux pics de fr√©quence √† la position de 4 cycles observ√©s sur \\(N=16\\) observations. Les puissances de Fourier sont affich√©s dans un espace fr√©quentiel en cycles par unit√© d‚Äôespacement de l‚Äô√©chantillon (avec z√©ro au d√©but) variant entre -1 et +1. Par exemple, si l‚Äôespacement des √©chantillons est en secondes, l‚Äôunit√© de fr√©quence est cycles/seconde (ou Hz). Dans le cas de N √©chantillons, le pic sera observ√© √† la fr√©quence \\(+/- 4/16=0.25\\) cycles/secondes. La fr√©quence d‚Äô√©chantillonnage \\(F_s\\) du signal a aussi beaucoup d‚Äôimportance aussi et doit √™tre au moins a deux fois la plus haute fr√©quence observ√©e (ici \\(F_s &gt; 0.5\\)) sinon un ph√©nom√®ne de repliement appel√© aliasing sera observ√©.\n\nimport math\nFs= 2.0\nTs= 1/Fs\nN= 16\narr = xr.DataArray(np.sin(2*math.pi*np.arange(0,N,Ts)*4/16),\n                   dims=('x'), coords={'x': np.arange(0,N,Ts)})\nfourier = np.fft.fft(arr)\nfreq = np.fft.fftfreq(fourier.size, d=Ts)\nfourier = xr.DataArray(fourier,\n                   dims=('f'), coords={'f': freq})\n\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\nplt.subplot(1, 2, 1)\narr.plot.line(color='red', linestyle='dashed', marker='o', markerfacecolor='blue')\naxes[0].set_title(\"Signal p√©riodique\")\nplt.subplot(1, 2, 2)\nnp.abs(fourier).plot.line(color='red', linestyle='dashed', marker='o', markerfacecolor='blue')\naxes[1].set_title(\"Composantes de Fourier (amplitude)\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n5.2.2 Filtrage fr√©quentielle\nUn filtrage fr√©quentielle consiste √† modifier le spectre de Fourier afin d‚Äô√©liminer ou de r√©duire certaines composantes fr√©quentielles. On peut distinguer trois grandes cat√©gories de filtres fr√©quentielles:\n\nLes filtres passe-bas qui ne pr√©servent que les basses fr√©quences pour, par exemple, lisser une image.\nLes filtres passe-haut qui ne pr√©servent que les hautes fr√©quences pour ne pr√©server que les d√©tails.\nLes filtres passe-bandes qui vont pr√©server les fr√©quences dans une bandes particuli√®res.\n\nLa librairie Scipy contient diff√©rents filtres fr√©quentielles. Notez, qu‚Äôun filtrage fr√©quentielle est une simple multiplication de la r√©ponse du filtre \\(F[k]\\) par les composantes fr√©quentielles du signal √† filtrer \\(X[k]\\):\n\\[\nX_f[k] = F[k] \\times X[k]\n\\tag{5.3}\\]\n√Ä noter que cette multiplication dans l‚Äôespace de Fourier est √©quivalente √† une op√©ration de convolution dans l‚Äôespace originale du signal \\(x\\):\n\\[\nx_f = IDFT^{-1}[F]*x\n\\tag{5.4}\\]\n\nfrom scipy import ndimage\nimport numpy.fft\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\ninput_ = numpy.fft.fft2(img_rgb.to_numpy()) \nresult = [ndimage.fourier_gaussian(input_[b], sigma=4) for b in range(3)] # on filtre chaque bande avec un filtre Gaussien\nresult = numpy.fft.ifft2(result)\nax1.imshow(img_rgb.to_numpy().transpose(1, 2, 0).astype('uint8'))\nax1.set_title('Originale')\nax2.imshow(result.real.transpose(1, 2, 0).astype('uint8'))  # La partie imaginaire n'est pas utile ici\nax2.set_title('Filtrage Gaussien')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n5.2.3 L‚Äôaliasing\nL‚Äôaliasing est un probl√®me fr√©quent en traitement du signal. Il r√©sulte d‚Äôune fr√©quence d‚Äô√©chantillonnage trop faible par rapport au contenu fr√©quentielle du signal. Ceci peut se produire lorsque vous sous-√©chantillonner fortement une image avec un facteur de d√©cimation (par exemple 1 pixel sur 2). En prenant un pixel sur 2, on r√©duit la fr√©quence d‚Äô√©chantillonnage d‚Äôun facteur 2 ce qui nous impose de r√©duire le contenu fr√©quentielle de l‚Äôimage et donc les fr√©quences maximales de l‚Äôimage. L‚Äôimage pr√©sente alors un aspect faussement textur√©e avec beaucoup de haute fr√©quences:\n\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\nplt.subplot(1, 2, 1)\nimg_rgb.astype('int').plot.imshow(rgb=\"band\")\naxes[0].set_title(\"Originale\")\nplt.subplot(1, 2, 2)\nimg_rgb[:,::4,::4].astype('int').plot.imshow(rgb=\"band\")\naxes[1].set_title(\"D√©cim√©e par un facteur 4\")\nplt.show()\n\n\n\n\n\n\n\n\nUne fa√ßon de r√©duire le contenu fr√©quentiel est de filtrer par un filtre passe-bas pour r√©duire les hautes fr√©quences par exemple avec un filtre Gaussien:\n\nfrom scipy.ndimage import gaussian_filter\n\nq= 4\nsigma= q*1.1774/math.pi\narr = xr.DataArray(gaussian_filter(img_rgb.to_numpy(), sigma= (0,sigma,sigma)), dims=('band',\"y\", \"x\"), coords= {'x': img_rgb.coords['x'], 'y': img_rgb.coords['y'], 'spatial_ref': 0})\n\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\nplt.subplot(1, 2, 1)\nimg_rgb.astype('int').plot.imshow(rgb=\"band\")\naxes[0].set_title(\"Originale\")\nplt.subplot(1, 2, 2)\narr[:,::q,::q].astype('int').plot.imshow(rgb=\"band\")\naxes[1].set_title(\"D√©cim√©e par un facteur 4\")\nplt.show()\n\n\n\n\n\n\n\n\n\nimport xrscipy.signal as dsp\n\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\nplt.subplot(1, 2, 1)\nimg_rgb.astype('int').plot.imshow(rgb=\"band\")\naxes[0].set_title(\"Originale\")\nplt.subplot(1, 2, 2)\ndsp.decimate(img_rgb, q=4, dim='x').astype('int').plot.imshow(rgb=\"band\")\naxes[1].set_title(\"D√©cim√©e par un facteur 4\")\n\nText(0.5, 1.0, 'D√©cim√©e par un facteur 4')",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Transformations spatiales</span>"
    ]
  },
  {
    "objectID": "04-TransformationSpatiales.html#filtrage-dimage",
    "href": "04-TransformationSpatiales.html#filtrage-dimage",
    "title": "5¬† Transformations spatiales",
    "section": "5.3 Filtrage d‚Äôimage",
    "text": "5.3 Filtrage d‚Äôimage\nLe filtrage d‚Äôimage a plusieurs objectifs en t√©l√©d√©tection:\n\nLa r√©duction du bruit afin d‚Äôam√©liorer la r√©solution radiom√©trique et am√©liorer la lisibilit√© de l‚Äôimage.\nLe r√©haussement de l‚Äôimage afin d‚Äôam√©liorer le contraste ou faire ressortir les contours.\nLa production de nouvelles caract√©ristiques: c.√†.d d√©river de nouvelles images mettant en valeur certaines informations dans l‚Äôimage comme la texture, les contours, etc.\n\nIl existe de nombreuses m√©thodes de filtrage dans la litt√©rature, on peut rassembler ces filtres en quatre grandes cat√©gories:\n\nLe filtrage peut-√™tre global ou local, c.√†.d prendre en compte toute l‚Äôimage pour filtrer (ex: filtrage par Fourier) ou seulement localement avec une fen√™tre ou un voisinage local.\nLa fonction de filtrage peut-√™tre lin√©aire ou non lin√©aire.\nLa fonction de filtrage peut √™tre stationnaire ou adaptative\nLe filtrage peut-√™tre mono-√©chelle ou multi-√©chelles\n\nLa librairie Scipy (Multidimensional image processing (scipy.ndimage)) contient une panoplie compl√®te de filtres.\n\n5.3.1 Filtrage lin√©aire stationnaire\nUn filtrage lin√©aire stationnaire consiste √† appliquer une m√™me pond√©ration locale des valeurs des pixels dans une fen√™tre glissante. La taille de cette fen√™tre est g√©n√©ralement impaire (3,5, etc.) afin de d√©finir une position centrale et une fen√™tre sym√©trique.\n\n\n\n\n\n\nNote\n\n\n\nMettre une figure ici\n\n\nLe filtre le plus simple est certainement le filtre moyen qui consiste √† appliquer le m√™me poids uniforme dans la fen√™tre glissante.\n\\[\nF= \\frac{1}{25}\\left[\n\\begin{array}{c|c|c|c|c}\n1 & 1 & 1 & 1 & 1 \\\\\n\\hline\n1 & 1 & 1 & 1 & 1 \\\\\n\\hline\n1 & 1 & 1 & 1 & 1 \\\\\n\\hline\n1 & 1 & 1 & 1 & 1 \\\\\n\\hline\n1 & 1 & 1 & 1 & 1\n\\end{array}\n\\right]\n\\tag{5.5}\\]\nEn python, on dispose des fonctions rolling et sliding_window d√©finis dans la librairie numpy. Par exemple pour le cas du filtre moyen on peut construire une nouvelle vue de l‚Äôimage avec deux nouvelles dimensions x_win et y_win:\n\nimport rioxarray as rxr\nrolling_win = img_rgb.rolling(x=5, y=5,  min_periods= 3, center= True).construct(x=\"x_win\", y=\"y_win\", keep_attrs= True)\nprint(rolling_win[0,0,1,...])\nprint(rolling_win.shape)\n\n&lt;xarray.DataArray (x_win: 5, y_win: 5)&gt; Size: 100B\narray([[ nan,  nan,  nan,  nan,  nan],\n       [ nan,  nan, 209., 210., 209.],\n       [ nan,  nan, 213., 214., 212.],\n       [ nan,  nan, 213., 212., 210.],\n       [ nan,  nan, 210., 209., 206.]], dtype=float32)\nCoordinates:\n    band         int64 8B 1\n    x            float64 8B 1.5\n    y            float64 8B 0.5\n    spatial_ref  int64 8B 0\nDimensions without coordinates: x_win, y_win\n(3, 771, 1311, 5, 5)\n\n\nL‚Äôavantage de cette approche est qu‚Äôil n‚Äôy a pas d‚Äôutilisation inutile de la m√©moire. Noter les nan sur les bords de l‚Äôimage car la fen√™tre d√©borde sur les bordures de l‚Äôimage. Par la suite un op√©rateur moyenne peut √™tre appliqu√©.\n\nfiltre_moyen= rolling_win.mean(dim= ['x_win', 'y_win'], skipna= True)\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8, 4))\nfiltre_moyen.astype('int').plot.imshow(rgb=\"band\")\nax.set_title(\"Filtre moyen 5x5\")\n\nText(0.5, 1.0, 'Filtre moyen 5x5')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nFiltre de Sobel, filtre Prewitt\n\n\n\n5.3.1.1 Filtrage par convolution\nLa fa√ßon la plus efficace d‚Äôappliquer un filtre lin√©aire est d‚Äôappliquer une convolution. La convolution est g√©n√©ralement tr√®s efficace car elle est peut √™tre calcul√©e dans le domaine fr√©quentielle. Prenons l‚Äôexemple du filtre de Scharr (Jahne et S. 1999), ce filtre permet de d√©tecter les contours horizontaux et verticaux:\n\\[\nF= \\left[\n\\begin{array}{ccc}\n-3-3j & 0-10j & +3-3j \\\\\n-10+0j & 0+0j & +10+0j \\\\\n-3+3j & 0+10j & +3+3j\n\\end{array}\n\\right]\n\\tag{5.6}\\]\nRemarquez l‚Äôutilisation de chiffres complexes afin de passer deux filtres diff√©rents sur la partie r√©elle et imaginaire.\n\nscharr = np.array([[ -3-3j, 0-10j,  +3 -3j],\n                   [-10+0j, 0+ 0j, +10 +0j],\n                   [ -3+3j, 0+10j,  +3 +3j]]) # Gx + j*Gy\nprint(img_rgb.isel(band=0).shape)\ngrad = signal.convolve2d(img_rgb.isel(band=0), scharr, boundary='symm', mode='same')\n# on reconstruit un xarray √† partir du r√©sultat:\narr = xr.DataArray(np.abs(grad), dims=(\"y\", \"x\"), coords= {'x': img_rgb.coords['x'], 'y': img_rgb.coords['y'], 'spatial_ref': 0})\nprint(arr)\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8, 4))\narr.plot.imshow()\nax.set_title(\"Amplitude du filtre de Scharr\")\n\n(771, 1311)\n&lt;xarray.DataArray (y: 771, x: 1311)&gt; Size: 8MB\narray([[  65.96969001,   58.85575588,   54.91812087, ..., 1474.        ,\n        1037.01205393,  389.99487176],\n       [  61.07372594,   39.8246155 ,   89.18520057, ..., 1763.79647352,\n         864.92543031,  270.20362692],\n       [  98.48857802,  112.44554237,  168.10710871, ..., 2110.61365484,\n         870.36658943,  204.40156555],\n       ...,\n       [ 143.17821063,  597.00753764, 2479.42977315, ...,  216.00925906,\n         248.33847869,  200.89798406],\n       [ 106.07544485,  393.67245268, 2188.78824924, ...,  124.96399481,\n         159.90622252,  346.34087255],\n       [  41.59326869,  229.05894438, 1845.1216762 , ...,  175.16278143,\n          33.37663854,  414.3911196 ]])\nCoordinates:\n  * x            (x) float64 10kB 0.5 1.5 2.5 ... 1.308e+03 1.31e+03 1.31e+03\n  * y            (y) float64 6kB 0.5 1.5 2.5 3.5 4.5 ... 767.5 768.5 769.5 770.5\n    spatial_ref  int64 8B 0\n\n\nText(0.5, 1.0, 'Amplitude du filtre de Scharr')\n\n\n\n\n\n\n\n\n\n\n5.3.1.1.1 Gestion des bordures\nL‚Äôapplication de filtres √† l‚Äôint√©rieur de fen√™tres glissantes implique de g√©rer les bords de l‚Äôimage car la fen√™tre de traitement va n√©cessairement d√©border de quelques pixels en dehors de l‚Äôimage (g√©n√©ralement la moiti√© de la fen√™tre d√©borde). On peut soit d√©cider d‚Äôignorer les valeurs en dehors de l‚Äôimage en imposant une valeur nan, prolonger l‚Äôimage de quelques lignes et colonnes avec des valeurs mirroirs ou constantes.\n\n\n\n5.3.1.2 Filtrage par une couche convolutionnelle\n\n\n\n\n\n\nImportant\n\n\n\nCette section n√©cessite la librairie Pytorch avec un GPU et ne fonctionnera que sur Colab.\n\n\nUne couche convolutionnelle est simplement un ensemble de filtres appliqu√©s sur la donn√©e d‚Äôentr√©e. Ce type de filtrage est √† la base des r√©seaux dits convolutionnels qui seront abord√©s dans le tome 2. On peut ici imposer les m√™mes filtres de gradient dans la couche convolutionnelle:\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport matplotlib.pyplot as plt\nnormalized_img= torch.tensor(img_rgb.to_numpy())\nnchannels= normalized_img.size()[0] # nombre de canaux de l'image\n\n# Define a conv2d layer\nconv_layer = nn.Conv2d(in_channels= nchannels, out_channels=2, kernel_size=3, padding=1, stride=1, dilation= 1)\n\n# Filtre de Sobel\nsobel_x = np.array([[-3, 0, 3], [-10, 0, 10], [-3, 0, 3]])\nsobel_y = np.array([[-3, -10, -3], [0, 0, 0], [3, 10, 3]])\n\nkernel = np.stack([sobel_x, sobel_y])\nkernel = kernel.reshape(2, 1, 3, 3)\n\nkernel = np.tile(kernel,(1,nchannels,1,1))\nprint(kernel.shape)\nkernel = torch.as_tensor(kernel,dtype=torch.float32)\nconv_layer.weight = nn.Parameter(kernel)\nconv_layer.bias = nn.Parameter(torch.zeros(2,))\n\ninput= normalized_img.unsqueeze(0) # il faut ajouter une dimension pour le nombre d'√©chantillons\nprint(input.shape)\n# Visualize the filters\nfig, axs = plt.subplots(1, 2, figsize=(8, 5))\nfor i in range(2):\n    axs[i].imshow(conv_layer.weight.data.numpy()[i, 0])\n    axs[i].set_title(f'Filtre {i+1}')\nplt.show()\n\n(2, 3, 3, 3)\ntorch.Size([1, 3, 771, 1311])\n\n\n\n\n\n\n\n\n\nLe r√©sultat est alors calcul√© sur GPU (si disponible):\n\nimport torch\nimport matplotlib.pyplot as plt\n\noutput = conv_layer(input)\nprint(f'Image (BxCxHxW): {input.shape}')\nprint(f'Sortie (BxFxHxW): {output.shape}')\n\nfig, axs = plt.subplots(1, 2, figsize=(20, 5))\nfor i in range(2):\n    axs[i].imshow(output.detach().data.numpy()[0,i], vmin=-5000, vmax=5000, cmap= 'gray')\n    axs[i].set_title(f'Filtrage {i+1}')\nplt.show()\n\nImage (BxCxHxW): torch.Size([1, 3, 771, 1311])\nSortie (BxFxHxW): torch.Size([1, 2, 771, 1311])\n\n\n\n\n\n\n\n\n\n\n\n\n5.3.2 Filtrage adaptatif\nLes filtrages adaptatifs consistent √† appliquer un traitement en fonction du contenu local d‚Äôune image. Le filtre n‚Äôest alors plus stationnaire et sa r√©ponse peut varier en fonction du contenu local. Ce type de filtre est tr√®s utilis√© pour filtrer les images SAR (Synthetic Aperture Radar) qui sont d√©grad√©es par un bruit multiplicatif que l‚Äôon appelle speckle. On peut voir un exemple d‚Äôune image Sentinel-1 (bande HH) sur la r√©gion de Montr√©al, remarqu√©e que l‚Äôimage est affich√©e en dB en appliquant la fonction log10.\n\nprint(img_SAR.rio.resolution())\nprint(img_SAR.rio.crs)\nfig, axs = plt.subplots(1, 1, figsize=(6, 4))\nxr.ufuncs.log10(img_SAR.sel(band=1).drop(\"band\")).plot()\naxs.set_title(\"Image SAR Sentinel-1 (dB)\")\n\n(0.00029254428869762705, -0.000287092818453516)\nEPSG:4326\n\n\nText(0.5, 1.0, 'Image SAR Sentinel-1 (dB)')\n\n\n\n\n\n\n\n\n\nUn des filtres les plus simples pour r√©duire le bruit est d‚Äôappliquer un filtre moyenne, par exemple un \\(5x5\\) ci dessous:\n\nrolling_win = img_SAR.sel(band=2).rolling(x=5, y=5,  min_periods= 3, center= True).construct(x=\"x_win\", y=\"y_win\", keep_attrs= True)\nfiltre_moyen= rolling_win.mean(dim= ['x_win', 'y_win'], skipna= True)\nfig, axs = plt.subplots(1, 1, figsize=(6, 4))\nxr.ufuncs.log10(filtre_moyen).plot.imshow()\naxs.set_title(\"Filtrage moyen 5x5 (dB)\")\n\nText(0.5, 1.0, 'Filtrage moyen 5x5 (dB)')\n\n\n\n\n\n\n\n\n\nAu lieu d‚Äôappliquer un filtre moyen de mani√®re indiscrimin√©e, le filtre de Lee (Lee 1986) applique une pond√©ration en fonction du contenu local de l‚Äôimage \\(I\\) dans sa forme la plus simple:\n\\[\n\\begin{aligned}\nI_F & = I_M + K \\times (I - I_M) \\\\\nK & = \\frac{\\sigma^2_I}{\\sigma^2_I + \\sigma^2_{bruit}}\n\\end{aligned}\n\\tag{5.7}\\]\nAinsi si la variance locale est √©lev√©e \\(K\\) s‚Äôapproche de \\(1\\) pr√©servant ainsi les d√©tails de l‚Äôimage sinon l‚Äôimage moyenne \\(I_M\\) est appliqu√©e.\n\nrolling_win = img_SAR.sel(band=2).rolling(x=5, y=5,  min_periods= 3, center= True).construct(x=\"x_win\", y=\"y_win\", keep_attrs= True)\nfiltre_moyen= rolling_win.mean(dim= ['x_win', 'y_win'], skipna= True)\necart_type= rolling_win.std(dim= ['x_win', 'y_win'], skipna= True)\ncv= ecart_type/filtre_moyen\nponderation = (cv - 0.25) / cv\n\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 4), sharex=True, sharey=True)\nplt.subplot(1, 2, 1)\ncv.plot.imshow( vmin=0, vmax=2)\naxes[0].set_title(\"CV\")\nplt.subplot(1, 2, 2)\nponderation.plot.imshow( vmin=0, vmax=1) \naxes[1].set_title(\"Pond√©ration\")\nplt.tight_layout()\n\n\n\n\n\n\n\n\nOn zoomant sur l‚Äôimage on peut clairement voir que les d√©tails de l‚Äôimage sont mieux pr√©serv√©s:",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Transformations spatiales</span>"
    ]
  },
  {
    "objectID": "04-TransformationSpatiales.html#segmentation",
    "href": "04-TransformationSpatiales.html#segmentation",
    "title": "5¬† Transformations spatiales",
    "section": "5.4 Segmentation",
    "text": "5.4 Segmentation\nLa segmentation d‚Äôimage consiste √† s√©parer une image en r√©gions homog√®nes spatialement connexes (segments) o√π les valeurs sont uniformes selon un certain crit√®re (couleurs, texture, etc.). Une image pr√©sente g√©n√©ralement beaucoup de pixels redondants, l‚Äôint√©r√™t de ce type de m√©thode est essentiellement de r√©duire la quantit√© de pxiels n√©cessaire. En t√©l√©d√©tection, on parle souvent d‚Äôapproche objet. En vision par ordinateur, on parle parfois de super-pixel. Il existe de nombreuses m√©thodes de segmentation, la librairie sickit-image rend disponible plusieurs impl√©mentations sur des images RVB (Comparison of segmentation and superpixel algorithms ‚Äî skimage 0.25.0 documentation).\n\n5.4.1 Super-pixel\nCe type de m√©thode cherche √† former des r√©gions homog√®nes et compactes dans l‚Äôimage (Achanta et S√ºsstrunk 2012). Une des m√©thodes les plus simples est la m√©thode SLIC (Simple Linear Iterative Clustering), elle combine un regroupement de type K-moyenne avec une distance hybride qui prend en compte les diff√©rences de couleur entre pixels mais aussi leur distance par rapport centre du super-pixel:\n\nD√©composer l‚Äôimage en N r√©gions r√©guli√®res de taille \\(S \\times S\\)\nInitialiser les centres \\(C_k\\) de chaque segment \\(k\\)\nRechercher les pixels qui ont la distance la plus petite dans une r√©gion \\(2S \\times 2S\\):\n\n\\[\nD_{SLIC}= d_{couleur} + \\frac{m}{S}d_{xy}\n\\]\n\nMettre √† jour les centre \\(C_k\\) de chaque segment \\(k\\), retourner √† l‚Äô√©tape 3\n\nLes r√©gions √©voluent rapidement avec les it√©rations, plus le poids \\(m\\) est √©lev√©, plus la forme du super-pixel est contrainte et ne suivra pas vraiment le contenu de l‚Äôimage:\n\nfrom skimage.color import rgb2gray\n\nfrom skimage.segmentation import slic, mark_boundaries\n\nimg = img_rgb.to_numpy().astype('uint8').transpose(1,2,0) \n\nsegments_slic1 = slic(img, n_segments=250, compactness=10, sigma=1, start_label=1, max_num_iter=1)\nsegments_slic2 = slic(img, n_segments=250, compactness=10, sigma=1, start_label=1, max_num_iter=2)\nsegments_slic100 = slic(img, n_segments=250, compactness=100, sigma=1, start_label=1, max_num_iter=10)\nsegments_slic100b = slic(img, n_segments=250, compactness=10, sigma=1, start_label=1, max_num_iter=10)\n\nprint(f'SLIC nombre de segments: {len(np.unique(segments_slic1))}')\n\nfig, ax = plt.subplots(2, 2, figsize=(10, 6), sharex=True, sharey=True)\n\nax[0, 0].imshow(mark_boundaries(img, segments_slic1))\nax[0, 0].set_title(\"Initialisation\")\nax[0, 1].imshow(mark_boundaries(img, segments_slic2))\nax[0, 1].set_title('2 it√©rations')\nax[1, 0].imshow(mark_boundaries(img, segments_slic100))\nax[1, 0].set_title('10 it√©rations avec m=100')\nax[1, 1].imshow(mark_boundaries(img, segments_slic100b))\nax[1, 1].set_title('10 it√©rations avec m=10')\n\nfor a in ax.ravel():\n    a.set_axis_off()\n\nplt.tight_layout()\nplt.show()\n\nSLIC nombre de segments: 240\n\n\n\n\n\n\n\n\n\nLe nombre de segments initial est probablement le param√®tre le plus important. Une mani√®re de l‚Äôestimer est d‚Äô√©valuer l‚Äô√©chelle moyenne des segments homog√®nes dans l‚Äôimage √† analyser. On peut observer ci-dessous l‚Äôimpact de passer d‚Äôune √©chelle \\(40 \\times 40\\) √† \\(20 \\times 20\\). En prenant la moyenne de chaque segment, on peut voir tout de suite que \\(40 \\times 40\\) r√©sulte en des segments trop grands m√©langeant diff√©rentes classes.\n\nfrom skimage import color, segmentation\nn_regions = int((img.shape[0] * img.shape[1])/(40*40))\nprint('Nb segments: ',n_regions)\nsegments_slic_40 = slic(img, n_segments=n_regions, compactness=10, sigma=1, start_label=1, max_num_iter=10)\nprint(f'SLIC nombre de segments: {len(np.unique(segments_slic_40))}')\nout = color.label2rgb(segments_slic_40, img, kind='avg', bg_label=0)\nout_40 = segmentation.mark_boundaries(out, segments_slic_40, (0, 0, 0))\n\nn_regions = int((img.shape[0] * img.shape[1])/(20*20))\nprint('Nb segments: ',n_regions)\nsegments_slic_20 = slic(img, n_segments=n_regions, compactness=10, sigma=1, start_label=1, max_num_iter=10)\nprint(f'SLIC nombre de segments: {len(np.unique(segments_slic_20))}')\nout = color.label2rgb(segments_slic_20, img, kind='avg', bg_label=0)\nout_20 = segmentation.mark_boundaries(out, segments_slic_20, (0, 0, 0))\n\nfig, ax = plt.subplots(2, 1, figsize=(6, 8), sharex=True, sharey=True)\n\nax[0].imshow(out_40)\nax[0].set_title(\"Initialisation avec 631 segments\")\nax[1].imshow(out_20)\nax[1].set_title('Initialisation avec 2526 segments')\nfor a in ax.ravel():\n    a.set_axis_off()\nplt.tight_layout()\nplt.show()\n\nNb segments:  631\nSLIC nombre de segments: 459\nNb segments:  2526\nSLIC nombre de segments: 2201\n\n\n\n\n\n\n\n\n\n\n\n5.4.2 Fusion des segments par graphe de proximit√©\nUne segmentation peut produire beaucoup trop de segments. On parle alors de sur-segmentation. Ceci est recherch√© dans certains cas pour permettre de bien capturer les d√©tails fins de l‚Äôimage. Cependant, afin de r√©duire le nombre de segments, un post-traitement possible est de fusionner les segments similaires selon certaines r√®gles ou distances. Un graphe d‚Äôadjacence de r√©gions (voir figure¬†5.1) est form√© √† partir des segments connect√©s o√π chaque noeud repr√©sente un segment et un lien une proximit√© (Jaworek-Korjakowska (2018)). √Ä partir de ce graphe, on peut fusionner les noeuds similaires √† partir de leur distance radiom√©trique.\n\n\n\n\n\n\nFigure¬†5.1: Graphe d‚Äôadjacence de r√©gions, d‚Äôapr√®s (Jaworek-Korjakowska (2018)). Chaque noeud est un segment, un lien est form√© uniquement si les segments se touchent (par exemple le segment 6 ne touche que la r√©gion 5).\n\n\n\n\ndef _weight_mean_color(graph, src, dst, n):\n    \"\"\"Fonction pour g√©rer la fusion des n≈ìuds en recalculant la couleur moyenne.\n    La m√©thode suppose que la couleur moyenne de `dst` est d√©j√† calcul√©e.\n    \"\"\"\n    diff = graph.nodes[dst]['mean color'] - graph.nodes[n]['mean color']\n    diff = np.linalg.norm(diff)\n    #print(diff)\n    return {'weight': diff}\n\n\ndef merge_mean_color(graph, src, dst):\n    \"\"\"Fonction appel√©e avant la fusion de deux n≈ìuds d'un graphe de distance de couleur moyenne.\n      Cette m√©thode calcule la couleur moyenne de `dst`.\n    \"\"\"\n    graph.nodes[dst]['total color'] += graph.nodes[src]['total color']\n    graph.nodes[dst]['pixel count'] += graph.nodes[src]['pixel count']\n    graph.nodes[dst]['mean color'] = (\n        graph.nodes[dst]['total color'] / graph.nodes[dst]['pixel count']\n    )\ng = graph.rag_mean_color(img, segments_slic_20)\nprint('Nombre de segments:',len(g))\nlabels2 = graph.merge_hierarchical(\n    segments_slic_20,\n    g,\n    thresh=20,\n    rag_copy=False,\n    in_place_merge=True,\n    merge_func=merge_mean_color,\n    weight_func=_weight_mean_color,\n)\nprint('Nombdre de segments:',len(g))\n\nout1 = color.label2rgb(segments_slic_20, img, kind='avg', bg_label=0)\nout1 = segmentation.mark_boundaries(out1, segments_slic_20, (0, 0, 0))\nout2 = color.label2rgb(labels2, img, kind='avg', bg_label=0)\nout2 = segmentation.mark_boundaries(out2, labels2, (0, 0, 0))\n\nfig, ax = plt.subplots(nrows=2, sharex=True, sharey=True, figsize=(6, 8))\n\nax[0].imshow(out1)\nax[0].set_title(\"Avant fusion\")\nax[1].imshow(out2)\nax[1].set_title(\"Apr√®s fusion\")\nfor a in ax:\n    a.axis('off')\n\nplt.tight_layout()\n\nNombre de segments: 2201\nNombdre de segments: 1187\n\n\n\n\n\n\n\n\n\n\n\n5.4.3 Approche objet\nL‚Äôapproche objet consiste √† traiter chaque segment comme un objet avec un ensemble de propri√©t√©s. La librairie skimage offre la possibilit√© d‚Äôenrichir chaque segment avec des propri√©t√©s et de former un tableau:\n\nproperties = ['label', 'area', 'centroid', 'num_pixels', 'intensity_mean', 'intensity_std']\n\ntable=   measure.regionprops_table(labels2, intensity_image= img_rgb.to_numpy().transpose(1,2,0), properties=properties)\n\ntable = pd.DataFrame(table)\ntable.head(10)\n\n\n\n\n\n\n\n\nlabel\narea\ncentroid-0\ncentroid-1\nnum_pixels\nintensity_mean-0\nintensity_mean-1\nintensity_mean-2\nintensity_std-0\nintensity_std-1\nintensity_std-2\n\n\n\n\n0\n1\n641.0\n15.466459\n69.489860\n641\n136.730103\n132.851791\n117.126366\n32.289547\n31.451048\n37.421638\n\n\n1\n2\n480.0\n10.997917\n92.614583\n480\n201.208328\n198.262497\n188.483337\n14.184592\n14.151334\n15.475913\n\n\n2\n3\n712.0\n16.683989\n114.776685\n712\n185.349716\n183.113770\n170.994385\n25.453747\n26.184948\n28.128426\n\n\n3\n4\n1803.0\n31.974487\n139.379368\n1803\n117.897392\n108.367722\n97.769829\n31.086676\n26.577900\n28.297256\n\n\n4\n5\n448.0\n5.004464\n166.542411\n448\n183.511154\n181.276779\n167.720978\n29.824030\n30.625013\n31.297607\n\n\n5\n6\n459.0\n9.934641\n191.668845\n459\n133.557739\n133.821350\n129.697174\n22.902142\n23.013086\n22.428919\n\n\n6\n7\n355.0\n5.160563\n222.895775\n355\n148.574646\n148.802811\n142.580276\n21.334805\n21.457472\n20.931572\n\n\n7\n8\n334.0\n4.904192\n255.904192\n334\n125.973053\n121.197601\n108.973053\n23.151978\n24.556480\n25.351229\n\n\n8\n9\n1481.0\n32.279541\n292.865631\n1481\n204.102631\n172.359894\n137.501007\n13.884891\n14.092896\n15.865581\n\n\n9\n10\n445.0\n8.013483\n308.053933\n445\n145.373032\n138.182022\n121.402245\n18.543356\n18.644655\n22.237881\n\n\n\n\n\n\n\nCe tableau pourra √™tre exploiter pour une t√¢che de classification par la suite (on parle alors de classification objet).\n\n\n\n\nAchanta, Kevin Smith adhakrishna, Appu Shaji et Sabine S√ºsstrunk. 2012. ¬´¬†SLIC Superpixels Compared to State-of-the-art Superpixel Methods.¬†¬ª TPAMI: 636‚Äë643. https://doi.org/10.1109/TPAMI.2012.120.\n\n\nCooley, James W. et John W. Tukey. 1965. ¬´¬†An algorithm for the machine calculation of complex Fourier series.¬†¬ª Math. Comput.: 297‚Äë301. https://web.stanford.edu/class/cme324/classics/cooley-tukey.pdf.\n\n\nJahne, Scharr, B et Korkel S. 1999. Principles of filter design. Handbook of Computer Vision; Applications; Academic Press.\n\n\nJaworek-Korjakowska, P., J.; K≈Çeczek. 2018. ¬´¬†Region Adjacency Graph Approach for Acral Melanocytic Lesion Segmentation.¬†¬ª Applied Sciences 8: 1430. 10.3390/app8091430.\n\n\nLee, J. S. 1986. ¬´¬†Speckle suppression and analysis for synthetic aperture radar images.¬†¬ª Opt. Eng. 25 (5): 636‚Äë643. https://doi.org/10.1117/12.7973877.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Transformations spatiales</span>"
    ]
  },
  {
    "objectID": "05-ClassificationsSupervisees.html",
    "href": "05-ClassificationsSupervisees.html",
    "title": "6¬† Classifications d‚Äôimages supervis√©es",
    "section": "",
    "text": "6.1 üöÄ Pr√©ambule\nAssurez-vous de lire ce pr√©ambule avant d‚Äôex√©cutez le reste du notebook.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Classifications d'images supervis√©es</span>"
    ]
  },
  {
    "objectID": "05-ClassificationsSupervisees.html#pr√©ambule",
    "href": "05-ClassificationsSupervisees.html#pr√©ambule",
    "title": "6¬† Classifications d‚Äôimages supervis√©es",
    "section": "",
    "text": "6.1.1 üéØ Objectifs\nDans ce chapitre, nous ferons une introduction g√©n√©rale √† l‚Äôapprentissage automatique et abordons quelques techniques fondamentales. La librairie centrale utilis√©e dans ce chapitre sera sickit-learn. Ce chapitre est aussi disponible sous la forme d‚Äôun notebook Python sur Google Colab:\n\n\n\n6.1.2 Librairies\nLes librairies utilis√©es dans ce chapitre sont les suivantes:\n\nSciPy\nNumPy\nopencv-python ¬∑ PyPI\nscikit-image\nRasterio\nxarray\nrioxarray\ngeopandas\nscikit-learn\n\nDans l‚Äôenvironnement Google Colab, seul rioxarray et xrscipy doit √™tre install√©s:\n\n%%capture\n!pip install -qU matplotlib rioxarray xrscipy\n\nV√©rifier les importations n√©cessaires en premier:\n\nimport numpy as np\nimport rioxarray as rxr\nfrom scipy import signal\nimport xarray as xr\nimport rasterio\nimport xrscipy\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nimport geopandas\nfrom shapely.geometry import Point\nimport pandas as pd\nfrom numba import jit\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.inspection import DecisionBoundaryDisplay\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.datasets import make_blobs, make_classification, make_gaussian_quantiles\n\n\n\n6.1.3 Images utilis√©es\nNous allons utilisez les images suivantes dans ce chapitre:\n\n%%capture\nimport gdown\n\ngdown.download('https://drive.google.com/uc?export=download&confirm=pbef&id=1a6Ypg0g1Oy4AJt9XWKWfnR12NW1XhNg_', output= 'RGBNIR_of_S2A.tif')\ngdown.download('https://drive.google.com/uc?export=download&confirm=pbef&id=1a4PQ68Ru8zBphbQ22j0sgJ4D2quw-Wo6', output= 'landsat7.tif')\ngdown.download('https://drive.google.com/uc?export=download&confirm=pbef&id=1_zwCLN-x7XJcNHJCH6Z8upEdUXtVtvs1', output= 'berkeley.jpg')\ngdown.download('https://drive.google.com/uc?export=download&confirm=pbef&id=1dM6IVqjba6GHwTLmI7CpX8GP2z5txUq6', output= 'SAR.tif')\ngdown.download('https://drive.google.com/uc?export=download&confirm=pbef&id=1aAq7crc_LoaLC3kG3HkQ6Fv5JfG0mswg', output= 'carte.tif')\n\nV√©rifiez que vous √™tes capable de les lire :\n\nwith rxr.open_rasterio('berkeley.jpg', mask_and_scale= True) as img_rgb:\n    print(img_rgb)\nwith rxr.open_rasterio('RGBNIR_of_S2A.tif', mask_and_scale= True) as img_rgbnir:\n    print(img_rgbnir)\nwith rxr.open_rasterio('SAR.tif', mask_and_scale= True) as img_SAR:\n    print(img_SAR)\nwith rxr.open_rasterio('carte.tif', mask_and_scale= True) as img_carte:\n    print(img_carte)",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Classifications d'images supervis√©es</span>"
    ]
  },
  {
    "objectID": "05-ClassificationsSupervisees.html#principes-g√©n√©raux",
    "href": "05-ClassificationsSupervisees.html#principes-g√©n√©raux",
    "title": "6¬† Classifications d‚Äôimages supervis√©es",
    "section": "6.2 Principes g√©n√©raux",
    "text": "6.2 Principes g√©n√©raux\nUne classification supervis√©e ou dirig√©e consiste √† attribuer une √©tiquette (une classe) de mani√®re automatique √† chaque point d‚Äôun jeu de donn√©es. Cette classification peut se faire √† l‚Äôaide d‚Äôune cascade de r√®gles pr√©-√©tablies (arbre de d√©cision) ou √† l‚Äôaide de techniques d‚Äôapprentissage automatique (machine learning). L‚Äôutilisation de r√®gles pr√©-√©tablies atteint vite une limite car ces r√®gles doivent √™tre fournies manuellement par un expert. Ainsi, l‚Äôavantage de l‚Äôapprentissage automatique est que les r√®gles de d√©cision sont d√©riv√©es automatiquement du jeu de donn√©es via une phase dite d‚Äôentra√Ænement. On parle souvent de solutions g√©n√©r√©es par les donn√©es (Data Driven Solutions). Cet ensemble de r√®gles est souvent appel√© mod√®le. On visualise souvent ces r√®gles sous la forme de fronti√®res de d√©cisions dans l‚Äôespace des donn√©es. Cependant, un des d√©fis majeur de ce type de technique est d‚Äô√™tre capable de produire des r√®gles qui soient g√©n√©ralisables au-del√† du jeu d‚Äôentra√Ænement.\nLes classifications supervis√©es ou dirig√©es pr√©supposent donc que nous avons √† disposition un jeu d‚Äôentra√Ænement d√©j√† √©tiquet√©. Celui-ci va nous permettre de construire un mod√®le. Afin que ce mod√®le soit repr√©sentatif et robuste, il nous faut assez de donn√©es d‚Äôentra√Ænement. Les algorithmes d‚Äôapprentissage automatique sont tr√®s nombreux et plus ou moins complexes pouvant produire des fronti√®res de d√©cision tr√®s complexes et non lin√©aires.\ncurse of dimensionnality, capacit√© d‚Äôun mod√®le, sur-aprrentissage, sous-apprentissage\n\n6.2.1 Comportement d‚Äôun mod√®le\nCet exemple tir√© de sickit-learn illustre les probl√®mes d‚Äôajustement insuffisant ou sous-apprentissage (underfitting) et d‚Äôajustement excessif ou sur-apprentissage (overfitting) et montre comment nous pouvons utiliser la r√©gression lin√©aire avec un mod√®le polynomiale pour approximer des fonctions non lin√©aires. La figure¬†6.1 montre la fonction que nous voulons approximer, qui est une partie de la fonction cosinus (couleur orange). En outre, les √©chantillons de la fonction r√©elle et les approximations de diff√©rents mod√®les sont affich√©s en bleu. Les mod√®les ont des caract√©ristiques polynomiales de diff√©rents degr√©s. Nous pouvons constater qu‚Äôune fonction lin√©aire (polyn√¥me de degr√© 1) n‚Äôest pas suffisante pour s‚Äôadapter aux √©chantillons d‚Äôapprentissage. C‚Äôest ce qu‚Äôon appelle un sous-ajustement (underfitting) qui produit un biais syst√©matique quelque soit les points d‚Äôentra√Ænement. Un polyn√¥me de degr√© 4 se rapproche presque parfaitement de la fonction r√©elle. Cependant, pour des degr√©s plus √©lev√©s, le mod√®le s‚Äôadaptera trop aux donn√©es d‚Äôapprentissage, c‚Äôest-√†-dire qu‚Äôil apprendra le bruit des donn√©es d‚Äôapprentissage. Nous √©valuons quantitativement le sur-apprentissage et le sous-apprentissage √† l‚Äôaide de la validation crois√©e. Nous calculons l‚Äôerreur quadratique moyenne (EQM) sur l‚Äôensemble de validation. Plus elle est √©lev√©e, moins le mod√®le est susceptible de se g√©n√©raliser correctement √† partir des donn√©es d‚Äôapprentissage.\n\n\n\n\n\n\n\n\nFigure¬†6.1: Exemples de sur et sous-apprentissage.\n\n\n\n\n\nOn constate aussi que sans les √©chantillons de validation, nous serions incapable de d√©terminer la situation de sur-apprentissage, l‚Äôerreur sur les points d‚Äôentra√Ænement seul √©tant excellente pour un degr√© 15.\n\n\n6.2.2 Pipeline\nLa construction d‚Äôun mod√®le implique g√©n√©ralement toujours les m√™mes √©tapes illustr√©es sur la figure figure¬†6.2:\n\nLa pr√©paration des donn√©es implique parfois un pr√©-traitement afin de normaliser les donn√©es.\nPartage des donn√©es en trois groupes: entra√Ænement, validation et test\nL‚Äôapprentissage du mod√®le sur l‚Äôensemble d‚Äôentra√Ænement. Cet apprentissage n√©cessite de d√©terminer les valeurs des hyper-param√®tres du mod√®le par l‚Äôusager.\nLa validation du mod√®le sur l‚Äôensemble de validation. Cette √©tape vise √† v√©rifier que les hyper-param√®tres du mod√®le sont ad√©quate.\nEnfin le test du mod√®le sur un ensemble de donn√©e ind√©pendant\n\n\n\n\n\n\n\nflowchart TD\n    A[fa:fa-database Donn√©es] --&gt; B(fa:fa-gear Pr√©traitement)\n    B --&gt; C(fa:fa-folder-tree Partage des donn√©es) -.-&gt; D(fa:fa-gears Entra√Ænement)\n    H[[Hyper-param√®tres]] --&gt; D\n    D --&gt; |Mod√®le| E&gt;Validation]\n    E --&gt; |Mod√®le| G&gt;Test]\n    C -.-&gt; E\n    C -.-&gt; G\n\n\n\n\nFigure¬†6.2: √âtapes standards dans un entra√Ænement.\n\n\n\n\n\n\n\n6.2.3 Construction d‚Äôun ensemble d‚Äôentra√Ænement\nLes donn√©es d‚Äôentra√Ænement vont permettre de construire un mod√®le. Ces donn√©es peuvent prendre des formes tr√®s vari√©es mais on peut voir cela sous la forme d‚Äôun tableau \\(N \\times D\\):\n\nLa taille \\(N\\) du jeu de donn√©e\nChaque entr√©e d√©finit un √©chantillon ou un point dans un espace √† plusieurs dimension.\nChaque √©chantillon est d√©crit par \\(D\\) dimensions ou caract√©ristiques (features).\n\nUne fa√ßon simple de construire un ensemble d‚Äôentra√Ænement est d‚Äô√©chantillonner un produit existant. Nous allons utiliser la carte d‚Äôoccupation des sols suivante qui contient 12 classes diff√©rentes.\n\ncouleurs_classes= {'NoData': 'black', 'Commercial': 'yellow', 'Nuages': 'lightgrey', \n                    'Foret': 'darkgreen', 'Faible_v√©g√©tation': 'green', 'Sol_nu': 'saddlebrown',\n                  'Roche': 'dimgray', 'Route': 'red', 'Urbain': 'orange', 'Eau': 'blue', 'Tourbe': 'salmon', 'V√©g√©tation √©parse': 'darkgoldenrod', 'Roche avec v√©g√©tation': 'darkseagreen'}\nnom_classes= [*couleurs_classes.keys()]\ncouleurs_classes= [*couleurs_classes.values()]\n\nOn peut visualiser la carte de la fa√ßon suivante:\n\nimport matplotlib.pyplot as plt\nimport rioxarray as rxr\ncmap_classes = ListedColormap(couleurs_classes)\n\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8, 6))\nimg_carte.squeeze().plot.imshow(cmap=cmap_classes, vmin=0, vmax=12)\nax.set_title(\"Carte d'occupation des sols\", fontsize=\"small\")\n\nText(0.5, 1.0, \"Carte d'occupation des sols\")\n\n\n\n\n\n\n\n\n\nOn peut facilement calculer la fr√©quence d‚Äôoccurrence des 12 classes dans l‚Äôimage √† l‚Äôaide de numpy:\n\nimg_carte= img_carte.squeeze() # n√©cessaire pour ignorer la dimension du canal\ncompte_classe = np.unique(img_carte.data, return_counts=True)\nprint(compte_classe)\n\n(array([ 1.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 11., 12., nan],\n      dtype=float32), array([ 193598, 2129001,  679481,   29986,   14769,   95462,  751682,\n        125023,    9149,    4379,      10]))\n\n\nLa fr√©quence d‚Äôapparition de chaque classe varie grandement, on parle alors d‚Äôun ensemble d√©s√©quilibr√©. Ceci est tr√®s commun dans la plupart des ensembles d‚Äôentra√Ænement, les classes n‚Äôapparaissent pas avec la m√™me fr√©quence.\n\nvaleurs, comptes = compte_classe\n\n# Create the histogram\nplt.figure(figsize=(5, 3))\nplt.bar(valeurs, comptes/comptes.sum()*100)\nplt.xlabel(\"Classes\")\nplt.ylabel(\"%\")\nplt.title(\"Fr√©quences des classes\", fontsize=\"small\")\nplt.xticks(range(len(nom_classes)), nom_classes, rotation=45, ha='right')\nplt.show()\n\n\n\n\n\n\n\n\nOn peut √©chantillonner 100 points al√©atoires pour chaque classe:\n\nimg_carte= img_carte.squeeze()\nclass_counts = np.unique(img_carte.data, return_counts=True)\n\n# Liste vide des points √©chantillonn√©es\nsampled_points = []\nclass_labels= [] # contient les √©tiquettes des classes\nfor class_label in range(1,13): # pour chacune des 12 classes\n  # On cherche tous les pixels pour cette √©tiquette\n  class_pixels = np.argwhere(img_carte.data == class_label)\n\n  # On se limite √† 100 pixels par classe\n  n_samples = min(100, len(class_pixels))\n\n  # On les choisit les positions al√©atoirement\n  np.random.seed(0) # ceci permet de r√©pliquer le tirage al√©atoire\n  sampled_indices = np.random.choice(len(class_pixels), n_samples, replace=False)\n\n  # On prends les positions en lignes, colonnes\n  sampled_pixels = class_pixels[sampled_indices]\n\n  # On ajoute les points √† la liste\n  sampled_points.extend(sampled_pixels)\n  class_labels.extend(np.array([class_label]*n_samples)[:,np.newaxis])\n\n# Conversion en NumPy array\nsampled_points = np.array(sampled_points)\nclass_labels = np.array(class_labels)\n# On peut naviguer les points √† l'aide de la g√©or√©f√©rence\ntransformer = rasterio.transform.AffineTransformer(img_carte.rio.transform())\ntransform_sampled_points= transformer.xy(sampled_points[:,0], sampled_points[:,1])\n\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8, 6))\nimg_carte.squeeze().plot.imshow(cmap=cmap_classes, vmin=0, vmax=12)\nax.scatter(transform_sampled_points[0], transform_sampled_points[1], c='w', s=1)  # Plot sampled points\nax.set_title(\"Carte d'occupation des sols avec les points √©chantillonn√©s\", fontsize=\"small\")\nplt.show()\n\n\n\n\n\n\n\n\nUne fois les points s√©lectionn√©s, il faut ajouter les valeurs des bandes provenant d‚Äôune image satellite. Pour cela, on peut utiliser la m√©thodes sample() de rasterio. √âventuellement, la librairie geopandas permet de g√©rer les donn√©es d‚Äôentra√Ænement sous la forme d‚Äôun tableau transportant aussi l‚Äôinformation de g√©or√©f√©rence. Afin de pouvoir classifier ces points, nous allons ajouter les valeurs radiom√©triques provenant de l‚Äôimage Sentinel-2 √† 4 bandes RGBNIR_of_S2A.tif. Ces valeurs seront stock√©es dans la colonne value sous la forme d‚Äôun vecteur en format string:\n\npoints = [Point(xy) for xy in zip(transform_sampled_points[0], transform_sampled_points[1])]\ngdf = geopandas.GeoDataFrame(range(1,len(points)+1), geometry=points, crs=img_carte.rio.crs)\ncoord_list = [(x, y) for x, y in zip(gdf[\"geometry\"].x, gdf[\"geometry\"].y)]\nwith rasterio.open('RGBNIR_of_S2A.tif') as src:\n  gdf[\"value\"] = [x for x in src.sample(coord_list)]\ngdf['class']= class_labels\ngdf.to_csv('sampling_points.csv') # sauvegarde sous forme d'un format csv\ngdf.head()\n\n\n\n\n\n\n\n\n0\ngeometry\nvalue\nclass\n\n\n\n\n0\n1\nPOINT (749009.567 5035771.5)\n[2404, 2664, 2796, 2983]\n1\n\n\n1\n2\nPOINT (737761.905 5031580.999)\n[1960, 2334, 2232, 3315]\n1\n\n\n2\n3\nPOINT (737791.766 5030117.808)\n[2696, 2836, 2792, 3211]\n1\n\n\n3\n4\nPOINT (734138.764 5027221.286)\n[1856, 2096, 2108, 3878]\n1\n\n\n4\n5\nPOINT (738339.219 5039404.594)\n[1727, 2028, 1913, 4023]\n1",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Classifications d'images supervis√©es</span>"
    ]
  },
  {
    "objectID": "05-ClassificationsSupervisees.html#analyse-pr√©liminaire-des-donn√©es",
    "href": "05-ClassificationsSupervisees.html#analyse-pr√©liminaire-des-donn√©es",
    "title": "6¬† Classifications d‚Äôimages supervis√©es",
    "section": "6.3 Analyse pr√©liminaire des donn√©es",
    "text": "6.3 Analyse pr√©liminaire des donn√©es\nUne bonne pratique avant d‚Äôappliquer une technique d‚Äôapprentissage automatique est de regarder les caract√©ristiques de vos donn√©es:\n\nLe nombre de dimensions (features)\nCertaines dimensions sont informatives (discriminantes) et d‚Äôautres ne le sont pas\nLe nombre classes\nLe nombre de modes (clusters) par classes\nLe nombre d‚Äô√©chantillons par classe\nLa forme des groupes\nLa s√©parabilit√© des classes ou des groupes\n\nUne mani√®re d‚Äô√©valuer la s√©parabilit√© de vos classes est d‚Äôappliquer des mod√®les Gaussiens sur chacune des classes. Le mod√®le Gaussien multivari√© suppose que les donn√©es sont distribu√©es comme un nuage de points sym√©trique et unimodale. La distribution d‚Äôun point \\({\\bold x}\\) appartenant √† la classe \\(i\\) est la suivante:\n\\[\nP(\\bold{x} | Classe=i) = \\frac{1}{(2\\pi)^{D/2} |\\Sigma_i|^{1/2}}\\exp\\left(-\\frac{1}{2} (\\bold{x}-\\bold{m}_i)^t \\Sigma_k^{-1} (\\bold{x}-\\bold{m}_i)\\right)\n\\]\nLa m√©thode QuadraticDiscriminantAnalysis permet de calculer les param√®tres des Gaussiennes multivari√©es pour chacune des classes.\nOn peut calculer une distance entre deux nuages Gaussiens avec la distance dites de Jeffries-Matusita (JM) bas√©e sur la distance de Bhattacharyya \\(B\\):\n\\[\nJM_{ij}= 2(1-e^{-B})\\\\\nB=\\frac{1}{8}({\\bold m}_i-\\bold{m}_j)^t { \\frac{\\Sigma_i+\\Sigma_j}{2} }(\\bold{m}_i-\\bold{m}_j)+\\frac{1}{2}ln { \\frac{|(\\Sigma_i+\\Sigma_j)/2|}{|\\Sigma_i|^{1/2}|\\Sigma_j|^{1/2}}}\n\\]\nCette distance pr√©suppose que chaque classe \\(i\\) est d√©crite par son centre \\(\\bold{m}_i\\) et de sa dispersion dans l‚Äôespace √† \\(D\\) dimensions mesur√©e par la matrice de covariance \\(\\Sigma_i\\). On peut en faire facilement une fonction Python √† l‚Äôaide de numpy:\n\ndef bhattacharyya_distance(m1, s1, m2, s2):\n    # Calcul de la covariance moyenne\n    s = (s1 + s2) / 2\n    \n    # Calcul du premier terme (diff√©rence des moyennes)\n    m_diff = m1 - m2\n    term1 = np.dot(np.dot(m_diff.T, np.linalg.inv(s)), m_diff) / 8\n    \n    # Calcul du second terme (diff√©rence de covariances)\n    term2 = 0.5 * np.log(np.linalg.det(s) / np.sqrt(np.linalg.det(s1) * np.linalg.det(s2)))\n    \n    return term1 + term2\n\ndef jeffries_matusita_distance(m1, s1, m2, s2):\n    B = bhattacharyya_distance(m1, s1, m2, s2)\n    return 2 * (1 - np.exp(-B))\n\nLa figure ci-dessous illustre diff√©rentes situations avec des donn√©es artificielles:\n\n\n\n\n\n\n\n\n\nOn forme notre ensemble d‚Äôentrainement √† partir du fichier csv de la section Section 6.2.3.\n\ndf= pd.read_csv('sampling_points.csv')\n# Extraire la colonne 'value'.\n# 'value' est une cha√Æne de caract√®res repr√©sentation d'une liste de nombres.\n# Nous devons la convertir en donn√©es num√©riques r√©elles.\nX = df['value'].apply(lambda x: np.fromstring(x[1:-1], dtype=float, sep=' ')).to_list()\n\n# on obtient une liste de numpy array  qu'il faut convertir en un numpy array 2D\nX= np.array([row.tolist() for row in X])\nidx= X.sum(axis=-1)&gt;0 # on exclut certains points sans valeurs\nX= X[idx,...]\ny = df['class'].to_numpy()\ny= y[idx]\nclass_labels = np.unique(y).tolist() # on cherche √† savoir combien de classes uniques\nn_classes = len(class_labels)\nif max(class_labels) &gt; n_classes: # il se peut que certaines classes soit absentes\n  y_new= []\n  for i,l in enumerate(class_labels):\n    y_new.extend([i]*sum(y==l))\n  y_new = np.array(y_new)\n\ncouleurs_classes2= [couleurs_classes[c] for c in np.unique(y).tolist()] # couleurs des classes\ncmap_classes2 = ListedColormap(couleurs_classes2)\n\nOn peut faire une analyse de s√©parabilit√© sur notre ensemble d‚Äôentrainement de 10 classes. On obtient un tableau symm√©trique de 10x10 valeurs. On peut observer des valeurs inf√©rieures √† 1 ce qui indique des s√©parabilit√©s faibles entre ces classes sous l‚Äôhypoth√®se du mod√®le Gaussien:\n\nqda= QuadraticDiscriminantAnalysis(store_covariance=True)\nqda.fit(X, y_new) # calcul des param√®tres des distributions Gaussiennes\nJM= []\nclasses= np.unique(y_new).tolist() # √©tiquettes uniques des classes\nfor cl1 in classes:\n  for cl2 in classes:\n    JM.append(jeffries_matusita_distance(qda.means_[cl1], qda.covariance_[cl1], qda.means_[cl2], qda.covariance_[cl2]))\n\nJM= np.array(JM).reshape(len(classes),len(classes))\nJM= pd.DataFrame(JM, index=classes, columns=classes)\nJM.head(10)\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n\n0\n0.000000\n1.863868\n1.393761\n1.428029\n0.362315\n0.772851\n0.845905\n1.951047\n1.606257\n1.461732\n\n\n1\n1.863868\n0.000000\n1.305902\n1.010098\n1.888630\n1.633469\n1.445032\n1.970357\n1.055048\n1.232802\n\n\n2\n1.393761\n1.305902\n0.000000\n0.287302\n1.442510\n0.840757\n0.757233\n1.992850\n0.297199\n0.451062\n\n\n3\n1.428029\n1.010098\n0.287302\n0.000000\n1.438324\n1.012482\n0.849153\n1.976024\n0.349345\n0.458920\n\n\n4\n0.362315\n1.888630\n1.442510\n1.438324\n0.000000\n0.801518\n0.916320\n1.953300\n1.642287\n1.394732\n\n\n5\n0.772851\n1.633469\n0.840757\n1.012482\n0.801518\n0.000000\n0.175539\n1.998992\n1.134148\n0.961176\n\n\n6\n0.845905\n1.445032\n0.757233\n0.849153\n0.916320\n0.175539\n0.000000\n1.994316\n1.021801\n0.899154\n\n\n7\n1.951047\n1.970357\n1.992850\n1.976024\n1.953300\n1.998992\n1.994316\n0.000000\n1.996426\n1.954178\n\n\n8\n1.606257\n1.055048\n0.297199\n0.349345\n1.642287\n1.134148\n1.021801\n1.996426\n0.000000\n0.420495\n\n\n9\n1.461732\n1.232802\n0.451062\n0.458920\n1.394732\n0.961176\n0.899154\n1.954178\n0.420495\n0.000000",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Classifications d'images supervis√©es</span>"
    ]
  },
  {
    "objectID": "05-ClassificationsSupervisees.html#m√©thodes-non-param√©triques",
    "href": "05-ClassificationsSupervisees.html#m√©thodes-non-param√©triques",
    "title": "6¬† Classifications d‚Äôimages supervis√©es",
    "section": "6.4 M√©thodes non param√©triques",
    "text": "6.4 M√©thodes non param√©triques\nLes m√©thodes non param√©triques ne font pas d‚Äôhypoth√®ses particuli√®res sur les donn√©es. Un des inconv√©nients de ces mod√®les est que le nombre de param√®tres du mod√®les augmente avec la taille des donn√©es.\n\n6.4.1 M√©thode des parall√©l√©pip√®des\nLa m√©thode du parall√©l√©pip√®de est probablement la plus simple et consiste √† d√©limiter directement le domaine des points d‚Äôune classe par une boite (un parall√©l√©pip√®de) √† \\(D\\) dimensions. Les limites de ces parall√©l√©pip√®des forment alors des fronti√®res de d√©cision manuelles qui vont permettre d√©cider de la classe d‚Äôappartenance d‚Äôun nouveau point. Un des avantages de cette technique est que si un point n‚Äôest dans aucun parall√©l√©pip√®de alors on peut le laisser comme non classifi√©. Par contre, la construction de ces parall√©l√©pip√®des se complexifient grandement avec le nombre de bandes. √Ä une dimension, deux param√®tres, √©quivalents √† un seuillage d‚Äôhistogramme, sont suffisants. √Ä deux dimensions, vous devez d√©finir 4 segments par classe. Avec 3 bandes, vous devez d√©finir 6 plans par classes et √† D dimensions, D hyperplans √† D-1 dimensions par classe. Le mod√®le ici est donc une suite de valeurs min et max pour chacune des bandes et des classes:\n\ndef parrallepiped_train(X_train, y_train):\n  classes= np.unique(y_train).tolist()\n  clf= []\n  for cl in classes:\n      data_cl= X_train[y_train == cl,...] # on cherche les donn√©es pour la classe courante\n      \n      limits=[]\n      for b in range(data_cl.shape[1]):\n        limits.append([data_cl[:,b].min(), data_cl[:,b].max()]) # on calcul le min et max pour chaque bande\n      clf.append(np.array(limits))\n  return clf\nclf= parrallepiped_train(X, y_new)\n\nLa pr√©diction consiste √† trouver pour chaque point la premi√®re limite qui est satisfaite. Notez qu‚Äôil n‚Äôy a pas de moyen de d√©cider quelle est la meilleure classe si le point appartient √† plusieurs classes.\n\n@jit(nopython=True)\ndef parrallepiped_predict(clf, X_test):\n  y_pred= []\n  for data in X_test:\n    y_pred.append(np.nan)\n    for cl, limits in enumerate(clf):\n      inside= True\n      for b,limit in enumerate(limits):\n        inside = inside and (data[b] &gt;= limit[0]) & (data[b] &lt;= limit[1])\n        if ~inside:\n          break\n      if inside:\n        y_pred[-1]=cl\n  return np.array(y_pred)\n\nOn peut appliquer ensuite le mod√®le sur l‚Äôimage au complet. Les r√©sultats sont assez mauvais, seule la classe eau en bleu semble √™tre bien classifi√©e.\n\ndata_image= img_rgbnir.to_numpy().transpose(1,2,0).reshape(img_rgbnir.shape[1]*img_rgbnir.shape[2],4)\ny_image= parrallepiped_predict(clf, data_image)\ny_image= y_image.reshape(img_rgbnir.shape[1],img_rgbnir.shape[2])\n\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8, 6))\nplt.imshow(y_image, cmap=cmap_classes2)\nax.set_title(\"M√©thode des parrall√©l√©pip√®des\", fontsize=\"small\")\nplt.show()\n\n\n\n\n\n\n\n\n\n6.4.1.1 La mal√©diction de la haute dimension\nAugmenter le nombre de dimension ou de caract√©ristiques des donn√©es permet de r√©soudre des probl√®mes complexes comme la classification d‚Äôimage. Cependant, cela am√®ne beaucoup de contraintes sur le volume des donn√©es. Supposons que nous avons N points occupant un segment lin√©aire de taille d.¬†La densit√© de points est \\(N/d\\). Si nous augmentons le nombre de dimension D, la densit√© de points va diminuer exponentiellement en \\(1/d^D\\). Par cons√©quent, pour garder une densit√© constante et donc une bonne estimation des parall√©l√©pip√®des, il nous faudrait augmenter le nombre de points en puissance de D. Ceci porte le nom de la mal√©diction de la dimensionnalit√© (dimensionality curse). En r√©sum√©, l‚Äôespace vide augmente plus rapidement que le nombre de donn√©es d‚Äôentra√Ænement et l‚Äôespace des donn√©es devient de plus en plus parcimonieux (sparse). Pour contrecarrer ce probl√®me, on peut s√©lectionner les meilleures caract√©ristiques ou appliquer une r√©duction de dimension.\n\n\n\n6.4.2 Plus proches voisins\nLa m√©thode des plus proches voisins (K-Nearest-Neighbors en Anglais) est certainement la plus simple des m√©thodes pour classifier des donn√©es. Elle consiste √† comparer une nouvelle donn√©es avec ces voisins les plus proches en fonction d‚Äôune simple distance Euclidienne. Si une majorit√© de ces \\(K\\) voisins appartiennent √† une classe majoritaire alors cette classe est s√©lectionn√©e. Afin de permettre un vote majoritaire, on choisira un nombre impair pour la valeur de \\(K\\). Mallgr√© sa simplicit√©, cette technique peut devenir assez demandante en terme de calcul pour un nombre important de points avec un nombre √©lev√© de dimensions.\nReprensons l‚Äôensemble d‚Äôentra√Ænement form√© √† partir de notre image RGBNIR pr√©c√©dente:\n\ndf= pd.read_csv('sampling_points.csv')\n# Extraire la colonne 'value'.\n# 'value' est une cha√Æne de caract√®res comme repr√©sentation d'une liste de valeurs.\n# Nous devons la convertir en donn√©es num√©riques r√©elles.\nX = df['value'].apply(lambda x: np.fromstring(x[1:-1], dtype=float, sep=' ')).to_list()\n\n# on obtient une liste de numpy array  qu'il faut convertir en un numpy array 2D\nX= np.array([row.tolist() for row in X])\nidx= X.sum(axis=-1)&gt;0 # il se peut qu'il y ait des valeurs erron√©es\nX= X[idx,...]\ny = df['class'].to_numpy()\ny= y[idx]\nclass_labels = np.unique(y).tolist() # on cherche √† savoir combien de classes uniques\nn_classes = len(class_labels)\nif max(class_labels) &gt; n_classes: # il se peut que certaines classes soit absentes\n  y_new= []\n  for i,l in enumerate(class_labels):\n    y_new.extend([i]*sum(y==l))\n  y_new = np.array(y_new)\n\nIl est important de faire pr√©c√©der la m√©thode K-NN par une normalisation des donn√©es de fa√ßon √† ce que chaque caract√©ristique soit de moyenne 0 et d‚Äô√©cart-type √©gale √† 1 (on dit parfois que l‚Äôon blanchit les donn√©es). Cette normalisation permet √† ce que chaque dimension ait le m√™me poids dans le calcul des distances entre points. Cette op√©ration porte le nom de StandardScaler dans scikit-learn. On peut alors former un pipeline de traitement combinant les deux op√©rations:\n\nclf = Pipeline(\n    steps=[(\"scaler\", StandardScaler()), (\"knn\", KNeighborsClassifier(n_neighbors=1))]\n)\n\nAvant d‚Äôeffectuer un entra√Ænement, on met g√©n√©ralement une portion des donn√©es pour valider les performances:\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\nOn peut visualiser les fronti√®res de d√©cision du K-NN pour diff√©rentes valeurs de \\(K\\) lorsque seulement deux bandes sont utilis√©es (Rouge et proche infra-rouge ici):\n\n\nNumber of mislabeled points out of a total 200 points : 124\nNumber of mislabeled points out of a total 200 points : 122\nNumber of mislabeled points out of a total 200 points : 110\nNumber of mislabeled points out of a total 200 points : 110\n\n\n\n\n\n\n\n\n\nOn peut voir comment les diff√©rentes fronti√®res de d√©cision se forment dans l‚Äôespace des bandes Rouge-NIR. L‚Äôaugmentation de K rend ces fronti√®res plus complexes et le calcul plus long.\n\nclf.set_params(knn__weights='distance', knn__n_neighbors = 7).fit(X_train, y_train)\ny_pred = clf.predict(X_test)\nprint(\"Nombre de points misclassifi√©s sur %d points : %d\"\n  % (X_test.shape[0], (y_test != y_pred).sum()))\n\nNombre de points misclassifi√©s sur 200 points : 107\n\n\nL‚Äôapplication du mod√®le (la pr√©diction) peut se faire sur toute l‚Äôimage en transposant l‚Äôimage sous forme d‚Äôune matrice avec Largeur x Hauteur lignes et 4 colonnes:\n\ndata_image= img_rgbnir.to_numpy().transpose(1,2,0).reshape(img_rgbnir.shape[1]*img_rgbnir.shape[2],4)\ny_classe= clf.predict(data_image)\ny_classe= y_classe.reshape(img_rgbnir.shape[1],img_rgbnir.shape[2])\n\n\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8, 6))\nplt.imshow(y_classe, cmap=cmap_classes2)\nax.set_title(\"Carte d'occupation des sols avec K-NN\", fontsize=\"small\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n6.4.3 M√©thodes par arbre de d√©cision",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Classifications d'images supervis√©es</span>"
    ]
  },
  {
    "objectID": "05-ClassificationsSupervisees.html#m√©thodes-param√©triques",
    "href": "05-ClassificationsSupervisees.html#m√©thodes-param√©triques",
    "title": "6¬† Classifications d‚Äôimages supervis√©es",
    "section": "6.5 M√©thodes param√©triques",
    "text": "6.5 M√©thodes param√©triques\nLes m√©thodes param√©triques se basent sur des mod√©lisations statistiques des donn√©es pour permettre une classification. Contraitement au m√©thodes non param√©triques, elles ont un nombre fixe de param√®tres qui ne d√©pend pas de la taille du jeu de donn√©es. Par contre, des hypoth√®ses sont faites sur le comportement statistique des donn√©es. La classification consiste alors √† trouver la classe la plus vraisemblable dont le mod√®le statistique d√©crit le mieux les valeurs observ√©es. L‚Äôensemble d‚Äôentra√Ænement permettra alors de calculer les param√®tres de chaque Gaussienne pour chacune des classes d‚Äôint√©r√™t.\n\n6.5.1 M√©thode Bay√©sienne na√Øve\nLa m√©thode Bay√©sienne na√Øve Gaussienne consiste faire des hypoth√®ses simplificatrices sur les donn√©es, en particulier l‚Äôind√©pendance des donn√©es et des dimensions. Ceci permet un calcul plus simple.\n\nfrom sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\ny_pred = gnb.fit(X_train, y_train).predict(X_test)\nprint(\"Nombre de points erron√©s sur %d points : %d\"\n      % (X_test.shape[0], (y_test != y_pred).sum()))\n\nNombre de points erron√©s sur 200 points : 123\n\n\n\n\n\n\n\n\n\n\n\nOn peut observer que les fronti√®res de d√©cision sont beaucoup plus r√©guli√®res que pour K-NN.\n\ngnb.fit(X_train, y_train)\ny_pred = gnb.predict(X_test)\nprint(\"Nombre de points misclassifi√©s sur %d points : %d\"\n  % (X_test.shape[0], (y_test != y_pred).sum()))\n\nNombre de points misclassifi√©s sur 200 points : 123\n\n\nDe la m√™me mani√®re, la pr√©diction peut s‚Äôappliquer sur toute l‚Äôimage:\n\n\n\n\n\n\n\n\n\n\n\n6.5.2 Analyse Discriminante Quadratique (ADQ)\nL‚Äôanalyse discriminante quadratique peut-√™tre vue comme une g√©n√©ralisation de l‚Äôapproche Bay√©sienne naive qui suppose des mod√®les Gaussiens ind√©pendants pour chaque dimension et chaque point. Ici, on va consid√©rer un mod√®le Gaussien multivari√©.\n\nqda = QuadraticDiscriminantAnalysis(store_covariance=True)\nqda.fit(X_train, y_train)\ny_pred = qda.predict(X_test)\nprint(\"Nombre de points misclassifi√©s sur %d points : %d\"\n  % (X_test.shape[0], (y_test != y_pred).sum()))\n\nNombre de points misclassifi√©s sur 200 points : 122\n\n\nLes Gaussiennes multivari√©es peuvent √™tre visualiser sous forme d‚Äô√©llipses d√©crivant le domaine des valeurs de chaque classe:\n\n\n\n\n\n\n\n\n\nDe la m√™me mani√®re, la pr√©diction peut s‚Äôappliquer sur toute l‚Äôimage:\n\n\n\n\n\n\n\n\n\n\n\n6.5.3 R√©seaux de neurones\nLes r√©seaux de neurones artificiels (RNA) ont connu un essor tr√®s important depuis les ann√©es 2010 avec des approches dites profondes. Ces aspects seront surtout abord√©s dans le tome 2 consacr√© √† l‚Äôintelligence artificielle. On abordera ici seulement le perceptron simple et le perceptron multi-couches (MLP).\nLe perceptron est l‚Äôunit√© de base d‚Äôun RNA et consiste en N connections, une unit√© de calcul (le neurone) avec une fonction d‚Äôactivation et une sortie. Le perceptron ne permet de construire que des fronti√®res de d√©cision lin√©aires.\nLe perceptron multi-couches est un r√©seau dense (fully connected) avec des couches cach√©es entre la couche d‚Äôentr√©e et la couche de sortie. qui permet de construire des fronti√®res de d√©cision beaucoup plus complexes via une hi√©rarchie de fronti√®res de d√©cision.\nCes r√©seaux sont entra√Æn√©s via des techniques it√©ratives d‚Äôoptimisation de type descente en gradient avec une correction des param√®tres (les poids) √† l‚Äôaide de la r√©tro-propagation de l‚Äôerreur. L‚Äôerreur est mesur√©e via une fonction de co√ªt que l‚Äôon cherche √† r√©duire.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Classifications d'images supervis√©es</span>"
    ]
  },
  {
    "objectID": "06-ClassificationsNonSupervisees.html",
    "href": "06-ClassificationsNonSupervisees.html",
    "title": "7¬† Classifications d‚Äôimages non supervis√©es",
    "section": "",
    "text": "7.1 üöÄ Pr√©ambule\nAssurez-vous de lire ce pr√©ambule avant d‚Äôex√©cutez le reste du notebook.",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Classifications d'images non supervis√©es</span>"
    ]
  },
  {
    "objectID": "06-ClassificationsNonSupervisees.html#pr√©ambule",
    "href": "06-ClassificationsNonSupervisees.html#pr√©ambule",
    "title": "7¬† Classifications d‚Äôimages non supervis√©es",
    "section": "",
    "text": "7.1.1 üéØ Objectifs\nDans ce chapitre, nous abordons quelques techniques de traitement d‚Äôimages dans le domaine spatial uniquement. Ce chapitre est aussi disponible sous la forme d‚Äôun notebook Python sur Google Colab:\n\n\n\n7.1.2 Librairies\nLes librairies qui vont √™tre explor√©es dans ce chapitre sont les suivantes:\n\nSciPy -\nNumPy -\nopencv-python ¬∑ PyPI\nscikit-image\nRasterio\nXarray\nrioxarray\n\nDans l‚Äôenvironnement Google Colab, seul rioxarray doit √™tre install√©s:\n\n%%capture\n!pip install -qU matplotlib rioxarray xrscipy\n\nV√©rifier les importations:\n\nimport numpy as np\nimport rioxarray as rxr\nfrom scipy import signal\nimport xarray as xr\nimport xrscipy\nimport matplotlib.pyplot as plt\n\n\n\n7.1.3 Images utilis√©es\nNous allons utilisez les images suivantes dans ce chapitre:\n\n%%capture\n!wget https://github.com/sfoucher/TraitementImagesPythonVol1/raw/refs/heads/main/data/chapitre01/subset_RGBNIR_of_S2A_MSIL2A_20240625T153941_N0510_R011_T18TYR_20240625T221903.tif -O RGBNIR_of_S2A.tif\n!wget https://github.com/sfoucher/opengeos-data/raw/refs/heads/main/raster/landsat7.tif -O landsat7.tif\n!wget https://github.com/sfoucher/opengeos-data/raw/refs/heads/main/images/berkeley.jpg -O berkeley.jpg\n!wget https://github.com/sfoucher/TraitementImagesPythonVol1/raw/refs/heads/main/data/chapitre01/subset_0_of_S1A_split_NR_Cal_Deb_ML_Spk_SRGR.tif -O SAR.tif\n\nV√©rifiez que vous √™tes capable de les lire :\n\nwith rxr.open_rasterio('berkeley.jpg', mask_and_scale= True) as img_rgb:\n    print(img_rgb)\nwith rxr.open_rasterio('RGBNIR_of_S2A.tif', mask_and_scale= True) as img_rgbnir:\n    print(img_rgbnir)\nwith rxr.open_rasterio('SAR.tif', mask_and_scale= True) as img_SAR:\n    print(img_SAR)",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Classifications d'images non supervis√©es</span>"
    ]
  },
  {
    "objectID": "06-ClassificationsNonSupervisees.html#sec-061",
    "href": "06-ClassificationsNonSupervisees.html#sec-061",
    "title": "7¬† Classifications d‚Äôimages non supervis√©es",
    "section": "7.2 Classifications strictes",
    "text": "7.2 Classifications strictes\n\n7.2.1 K-means\n\n\n7.2.2 K-mediodes\n\n\n7.2.3 Isodata",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Classifications d'images non supervis√©es</span>"
    ]
  },
  {
    "objectID": "06-ClassificationsNonSupervisees.html#sec-062",
    "href": "06-ClassificationsNonSupervisees.html#sec-062",
    "title": "7¬† Classifications d‚Äôimages non supervis√©es",
    "section": "7.3 Classifications floues",
    "text": "7.3 Classifications floues",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Classifications d'images non supervis√©es</span>"
    ]
  },
  {
    "objectID": "06-ClassificationsNonSupervisees.html#sec-0621",
    "href": "06-ClassificationsNonSupervisees.html#sec-0621",
    "title": "7¬† Classifications d‚Äôimages non supervis√©es",
    "section": "7.4 C-Means",
    "text": "7.4 C-Means\n\n7.4.1 C-Means int√©grant une dimension spatiale",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Classifications d'images non supervis√©es</span>"
    ]
  },
  {
    "objectID": "06-ClassificationsNonSupervisees.html#sec-064",
    "href": "06-ClassificationsNonSupervisees.html#sec-064",
    "title": "7¬† Classifications d‚Äôimages non supervis√©es",
    "section": "7.5 Exercices de r√©vision",
    "text": "7.5 Exercices de r√©vision",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Classifications d'images non supervis√©es</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Bibliographie",
    "section": "",
    "text": "Achanta, Kevin Smith adhakrishna, Appu Shaji et Sabine S√ºsstrunk. 2012.\n¬´¬†SLIC Superpixels Compared to State-of-the-art Superpixel\nMethods.¬†¬ª TPAMI: 636‚Äë643. https://doi.org/10.1109/TPAMI.2012.120.\n\n\nCooley, James W. et John W. Tukey. 1965. ¬´¬†An algorithm for the\nmachine calculation of complex Fourier series.¬†¬ª Math.\nComput.: 297‚Äë301. https://web.stanford.edu/class/cme324/classics/cooley-tukey.pdf.\n\n\nHarris, Millman, C. R. 2020. ¬´¬†Array programming with\nNumPy.¬†¬ª Nature: 357‚Äë362. https://doi.org/10.1038/s41586-020-2649-2.\n\n\nHoyer, S. et J. Hamman. 2017. ¬´¬†xarray: N-D labeled Arrays and\nDatasets in Python.¬†¬ª Journal of Open Research\nSoftware 5 (1): 10. https://doi.org/10.5334/jors.148.\n\n\nJahne, Scharr, B et Korkel S. 1999. Principles of filter\ndesign. Handbook of Computer Vision; Applications; Academic Press.\n\n\nJaworek-Korjakowska, P., J.; K≈Çeczek. 2018. ¬´¬†Region Adjacency\nGraph Approach for Acral Melanocytic Lesion Segmentation.¬†¬ª\nApplied Sciences 8: 1430. 10.3390/app8091430.\n\n\nLee, J. S. 1986. ¬´¬†Speckle suppression and analysis for synthetic\naperture radar images.¬†¬ª Opt. Eng. 25 (5):\n636‚Äë643. https://doi.org/10.1117/12.7973877.\n\n\nOGC. 2019. ¬´¬†OGC GeoTIFF Standard.¬†¬ª https://docs.ogc.org/is/19-008r4/19-008r4.html/.",
    "crumbs": [
      "Bibliographie"
    ]
  }
]