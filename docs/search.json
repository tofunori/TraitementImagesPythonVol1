[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Traitement d’images satellites avec Python",
    "section": "",
    "text": "Préface\nRésumé : Ce livre vise à décrire une panoplie de méthodes de traitement d’images satellites avec le langage Python. Celles et ceux souhaitant migrer progressivement d’un autre logiciel d’imagerie et de télédétection vers Python trouveront dans cet ouvrage les éléments pour une transition en douceur. La philosophie de ce livre est de donner toutes les clefs de compréhension et de mise en œuvre des méthodes abordées dans Python. La présentation des méthodes est basée sur une approche compréhensive et intuitive plutôt que mathématique, sans pour autant négliger la rigueur mathématique ou statistique. Des rappels sur les fondements en télédétection pourront apparaître au besoin afin d’éclairer les approches techniques. Plusieurs éditions régulières sont prévues sachant que ce domaine évolue constamment.\nCe projet est en cours d’écriture et le contenu n’est pas complet\nRemerciements : Ce manuel a été réalisé avec le soutien de la fabriqueREL. Fondée en 2019, la fabriqueREL est portée par divers établissements d’enseignement supérieur du Québec et agit en collaboration avec les services de soutien pédagogique et les bibliothèques. Son but est de faire des ressources éducatives libres (REL) le matériel privilégié en enseignement supérieur au Québec.\nMise en page : Samuel Foucher, Philippe Apparicio et Marie-Hélène Gadbois Del Carpio.\n© Samuel Foucher, Philippe Apparicio, Yacine Bouroubi, Mickaël Germain et Étienne Clabaut.\nPour citer cet ouvrage : Foucher S., Apparicio P., Bouroubi Y., Germain M. et Clabaut, E. (2025). Traitement d’images satellites avec Python. Université de Sherbrooke, Département de géomatique appliquée. fabriqueREL. Licence CC BY-SA.",
    "crumbs": [
      "Préface"
    ]
  },
  {
    "objectID": "index.html#sect001",
    "href": "index.html#sect001",
    "title": "Traitement d’images satellites avec Python",
    "section": "Un manuel sous la forme d’une ressource éducative libre",
    "text": "Un manuel sous la forme d’une ressource éducative libre\nPourquoi un manuel sous licence libre?\nLes logiciels libres sont aujourd’hui très répandus. Comparativement aux logiciels propriétaires, l’accès au code source permet à quiconque de l’utiliser, de le modifier, de le dupliquer et de le partager. Le logiciel Python, dans lequel sont mises en œuvre les méthodes de traitement d’images satellites décrites dans ce livre, est d’ailleurs à la fois un langage de programmation et un logiciel libre (sous la licence publique générale GNU GPL2). Par analogie aux logiciels libres, il existe aussi des ressources éducatives libres (REL) « dont la licence accorde les permissions désignées par les 5R (Retenir — Réutiliser — Réviser — Remixer — Redistribuer) et donc permet nécessairement la modification » (fabriqueREL). La licence de ce livre, CC BY-SA (figure 1), permet donc de :\n\nRetenir, c’est-à-dire télécharger et imprimer gratuitement le livre. Notez qu’il aurait été plutôt surprenant d’écrire un livre payant sur un logiciel libre et donc gratuit. Aussi, nous aurions été très embarrassés que des personnes étudiantes avec des ressources financières limitées doivent payer pour avoir accès au livre, sans pour autant savoir préalablement si le contenu est réellement adapté à leurs besoins.\nRéutiliser, c’est-à-dire utiliser la totalité ou une section du livre sans limitation et sans compensation financière. Cela permet ainsi à d’autres personnes enseignantes de l’utiliser dans le cadre d’activités pédagogiques.\nRéviser, c’est-à-dire modifier, adapter et traduire le contenu en fonction d’un besoin pédagogique précis puisqu’aucun manuel n’est parfait, tant s’en faut! Le livre a d’ailleurs été écrit intégralement dans Python avec Quatro. Quiconque peut ainsi télécharger gratuitement le code source du livre sur GitHub et le modifier à sa guise (voir l’encadré intitulé Suggestions d’adaptation du manuel).\nRemixer, c’est-à-dire « combiner la ressource avec d’autres ressources dont la licence le permet aussi pour créer une nouvelle ressource intégrée » (fabriqueREL).\nRedistribuer, c’est-à-dire distribuer, en totalité ou en partie le manuel ou une version révisée sur d’autres canaux que le site Web du livre (par exemple, sur le site Moodle de votre université ou en faire une version imprimée).\n\nLa licence de ce livre, CC BY-SA (figure 1), oblige donc à :\n\nAttribuer la paternité de l’auteur dans vos versions dérivées, ainsi qu’une mention concernant les grandes modifications apportées, en utilisant la formulation suivante :\n\nSamuel Foucher, Apparicio Philippe, Mickaël Germain, Yacine Bouroubi et Étienne Clabaut (2025). Traitement d’images satellites avec Python. Université de Sherbrooke, Département de géomatique appliquée. fabriqueREL. Licence CC BY-SA.\n\nUtiliser la même licence ou une licence similaire à toutes versions dérivées.\n\n\n\n\n\n\n\nFigure 1: Licence Creative Commons du livre\n\n\n\n\n\n\n\n\nSuggestions d’adaptation du manuel\n\n\nPour chaque méthode de traitement d’image abordée dans le livre, une description détaillée et une mise en œuvre dans Python sont disponibles. Par conséquent, plusieurs adaptations du manuel sont possibles :\n\nConserver uniquement les chapitres sur les méthodes ciblées dans votre cours.\nEn faire une version imprimée et la distribuer aux personnes étudiantes.\nModifier la description d’une ou de plusieurs méthodes en effectuant les mises à jour directement dans les chapitres.\nInsérer ses propres jeux de données dans les sections intitulées Mise en œuvre dans Python.\nModifier les tableaux et figures.\nAjouter une série d’exercices.\nModifier les quiz de révision.\nRédiger un nouveau chapitre.\nModifier des syntaxes en Python. Plusieurs librairies Python peuvent être utilisées pour mettre en œuvre telle ou telle méthode. Ces derniers évoluent aussi très vite et de nouvelles librairies sont proposées fréquemment! Par conséquent, il peut être judicieux de modifier une syntaxe Python du livre en fonction de ses habitudes de programmation en Python (utilisation d’autres librairies que ceux utilisés dans le manuel par exemple) ou de bien mettre à jour une syntaxe à la suite de la parution d’une nouvelle librairie plus performante ou intéressante.\nToute autre adaptation qui permet de répondre au mieux à un besoin pédagogique.",
    "crumbs": [
      "Préface"
    ]
  },
  {
    "objectID": "index.html#sect002",
    "href": "index.html#sect002",
    "title": "Traitement d’images satellites avec Python",
    "section": "Comment lire ce manuel?",
    "text": "Comment lire ce manuel?\nLe livre comprend plusieurs types de blocs de texte qui en facilitent la lecture.\n\n\n\n\n\nBloc packages\n\n\nHabituellement localisé au début d’un chapitre, il comprend la liste des packages Python utilisés pour un chapitre.\n\n\n\n\n\n\n\nBloc objectif\n\n\nIl comprend une description des objectifs d’un chapitre ou d’une section.\n\n\n\n\n\n\n\nBloc notes\n\n\nIl comprend une information secondaire sur une notion, une idée abordée dans une section.\n\n\n\n\n\n\n\nBloc pour aller plus loin\n\n\nIl comprend des références ou des extensions d’une méthode abordée dans une section.\n\n\n\n\n\n\n\nBloc astuce\n\n\nIl décrit un élément qui vous facilitera la vie : une propriété statistique, un package, une fonction, une syntaxe Python.\n\n\n\n\n\n\n\nBloc attention\n\n\nIl comprend une notion ou un élément important à bien maîtriser.\n\n\n\n\n\n\n\nBloc exercice\n\n\nIl comprend un court exercice de révision à la fin de chaque chapitre.",
    "crumbs": [
      "Préface"
    ]
  },
  {
    "objectID": "index.html#sect003",
    "href": "index.html#sect003",
    "title": "Traitement d’images satellites avec Python",
    "section": "Comment utiliser les données du livre pour reproduire les exemples?",
    "text": "Comment utiliser les données du livre pour reproduire les exemples?\nCe livre comprend des exemples détaillés et appliqués en Python pour chacune des méthodes abordées. Ces exemples se basent sur des jeux de données ouverts et mis à disposition avec le livre. Ils sont disponibles sur le repo GitHub dans le sous-dossier data, à l’adresse https://github.com/serie-tele-pyton/TraitementImagesVol1/tree/main/data.\nUne autre option est de télécharger le repo complet du livre directement sur GitHub (https://github.com/serie-tele-pyton/TraitementImagesVol1) en cliquant sur le bouton Code, puis le bouton Download ZIP (figure 2). Les données se trouvent alors dans le sous-dossier nommé data.\n\n\n\n\n\n\nFigure 2: Téléchargement de l’intégralité du livre",
    "crumbs": [
      "Préface"
    ]
  },
  {
    "objectID": "index.html#sect004",
    "href": "index.html#sect004",
    "title": "Traitement d’images satellites avec Python",
    "section": "Structure du livre",
    "text": "Structure du livre\nLe livre est organisé autour de quatre grandes parties.\nPartie 1. Importation et manipulation de données spatiales. Dans cette première partie, nous voyons comment importer, manipuler, visualiser et exporter des données spatiales de type image (ou de type matriciel) avec Python, principalement avec les packages rasterio, xarray et numpy (chapitre 2  Importation et manipulation de données spatiales). Ce chapitre vous permettra de maîtriser la manipulation à bas niveau de différents types d’imagerie. Différents exemples et exercises sont disponibles avec différents capteurs satellites (multi-spectral, RGB-NIR, SAR, etc.)\nPartie 2. Transformations des données spatiales. Cette deuxième partie comprend deux chapitres : les transformations spectrales (chapitre 4  Transformations spectrales) et les transformations spatiales (chapitre 5  Transformations spatiales).\nPartie 3. Classifications d’images. Cette troisième partie comprend deux chapitres : les classifications supervisées (chapitre 6  Classifications d’images supervisées) et non supervisées (à venir dans une prochaine édition).\nPartie 4. Données massives. (à venir dans une édition future). Cette quatrième et dernière partie comprend un seul chapitre qui est dédié aux plateformes de mégadonnées, notamment Google Earth Engine.",
    "crumbs": [
      "Préface"
    ]
  },
  {
    "objectID": "index.html#sect005",
    "href": "index.html#sect005",
    "title": "Traitement d’images satellites avec Python",
    "section": "Remerciements",
    "text": "Remerciements\nDe nombreuses personnes ont contribué à l’élaboration de ce manuel.\nCe projet a bénéficié du soutien pédagogique et financier de la fabriqueREL (ressources éducatives libres). Les différentes rencontres avec le comité de suivi nous ont permis de comprendre l’univers des ressources éducatives libres (REL) et notamment leurs fameux 5R (Retenir — Réutiliser — Réviser — Remixer — Redistribuer), de mieux définir le besoin pédagogique visé par ce manuel, d’identifier des ressources pédagogiques et des outils pertinents pour son élaboration. Ainsi, nous remercions chaleureusement les membres de la fabriqueREL pour leur soutien inconditionnel :\n\nJosé-Miguel Escobar-Zuniga, bibliothécaire à l’Université de Sherbrooke.\nMarianne Dubé, coordonnatrice de la fabriqueREL, Université de Sherbrooke.\nClaude Potvin, conseiller en formation, Service de soutien à l’enseignement, Université Laval.\n\nNous remercions chaleureusement les personnes étudiantes du Baccalauréat en géomatique appliquée à l’environnement et du Microprogramme de 1er cycle en géomatique appliquée du Département de géomatique appliquée de l’Université de Sherbrooke qui utilisent le manuel dans le cadre de cours.",
    "crumbs": [
      "Préface"
    ]
  },
  {
    "objectID": "index.html#sect006",
    "href": "index.html#sect006",
    "title": "Traitement d’images satellites avec Python",
    "section": "Introduction aux images de télédétection",
    "text": "Introduction aux images de télédétection\nL’imagerie numérique a pris une place importante dans notre vie de tous les jours depuis une quinzaine d’années. Ces images sont prises généralement au niveau du sol (imagerie proximale) avec seulement trois couleurs dans le domaine de la vision humaine (rouge, vert et bleu). Dans la suite du manuel, on parlera d’images du domaine de la vision par ordinateur ou images en vision pour faire plus court.\nLes images de télédétection ont des particularités et des propriétés qui les différencient des images “classiques”, dont les cinq principales sont:\n\nLes images sont géoréférencées. Cela signifie que pour chaque pixel nous pouvons y associer une position géographique ou cartographique.\nLe point de vue est très différent. Ces images sont prises avec une vue d’en haut (Nadir) ou oblique avec une distance qui peut être très grande; on parle d’images distales.\nElles possèdent plus que 3 bandes. Contrairement aux images en vision, les images de télédétection possèdent bien souvent plus que trois bandes. Il n’est pas rare de trouver quatre bandes (Pléiade), 13 bandes (Sentinel-2, Landsat) et même 200 bandes pour des capteurs hyperspectraux.\nElles peuvent être calibrées. Les valeurs numérique de l’image peuvent être converties en quantités physiques (luminance, réflectance, section efficace, etc.) via une fonction de calibration.\nElles sont de grande taille. Il n’est pas rare de manipuler des images qui font plusieurs dizaines de milliers de pixels en dimension.\n\n\nRessources en ligne\n\n\nListes des librairies utilisés\nDans ce livre, nous utilisons de nombreux packages Python que vous pouvez installer en une seule fois (voir section 1.3.1 Création d’un environnement virtuel) ou chapitre par chapitre.",
    "crumbs": [
      "Préface"
    ]
  },
  {
    "objectID": "00-auteurs.html",
    "href": "00-auteurs.html",
    "title": "À propos des auteurs",
    "section": "",
    "text": "Samuel Foucher est professeur au Département de géomatique appliquée de l’Université de Sherbrooke. Il y enseigne aux programmes de 1er et 2e cycles de géomatique les cours Traitement numérique des images de télédétection, Base de données géospatiales et Apprentissage profond appliqué à l’observation de la Terre. Ses intérêts de recherche portent sur le traitement d’images et l’application de l’IA aux données géospatiales.\nPhilippe Apparicio est professeur titulaire au Département de géomatique appliquée de l’Université de Sherbrooke. Il y enseigne aux programmes de 1er et 2e cycles de géomatique les cours Transport et mobilité durable, Modélisation et analyse spatiale et Géomatique appliquée à la gestion urbaine. Durant les dernières années, il a offert plusieurs formations aux Écoles d’été du Centre interuniversitaire québécois de statistiques sociales (CIQSS). Géographe de formation, ses intérêts de recherche incluent la justice et l’équité environnementale, la mobilité durable, les pollutions atmosphérique et sonore, et le vélo en ville. Il a publié une centaine d’articles scientifiques dans différents domaines des études urbaines et de la géographie mobilisant la géomatique et l’analyse spatiale.\nMickaël Germain est professeur agrégé au Département de géomatique appliquée de l’Université de Sherbrooke. Ses expertises comprennent les systèmes d’information géographique (SIG) et leur utilisation sur Internet (SIG web), les systèmes décisionnels, les bases de données à référence spatiale et le forage de données massives. Il s’intéresse également à la cartographie web, à la mobilité, à l’Internet des objets (IoT), à l’apprentissage machine et à la fusion des données multisources pour l’aide à la décision. Ses systèmes sont utilisés dans divers domaines de la géomatique tels que les ressources naturelles, la géologie, les transports et les écosystèmes. De plus, il se passionne pour le traitement, la gestion et l’analyse des données de télédétection pour extraire des paramètres descriptifs facilitant l’identification, la segmentation ou la classification de l’information.\nYacine Bouroubi est professeur agrégé au Département de géomatique appliquée de l’Université de Sherbrooke. Il possède plus de vingt ans d’expérience dans le domaine du traitement des images numériques appliqué à diverses thématiques des sciences6. Ses travaux de recherche se concentrent principalement sur l’utilisation de techniques de traitement d’images pour résoudre des problèmes géographiques. En tant que professeur, il partage son expertise avec les étudiants et contribue à l’avancement des connaissances dans son domaine. Bien que les détails spécifiques de sa carrière ne soient pas fournis, son expérience étendue et son poste actuel témoignent de sa contribution significative à la géographie et au traitement d’images numériques.\nÉtienne Clabaut est professeur associé au Département de géomatique appliquée de l’Université de Sherbrooke. Étienne détient un doctorat en télédétection de l’Université de Sherbrooke. Il se spécialise dans l’utilisation de l’apprentissage automatique pour l’exploitation de l’imagerie satellite.s",
    "crumbs": [
      "À propos des auteurs"
    ]
  },
  {
    "objectID": "00-PriseEnMainPython.html",
    "href": "00-PriseEnMainPython.html",
    "title": "1  Introduction au langage Python",
    "section": "",
    "text": "1.1 Les distributions\nDans ce chapitre, nous présentons quelques éléments essentiels du langage Python qui nous seront utiles dans ce manuel. Python est un langage très riche et peut aboutir à des projets logiciels très sophistiqués. Il est important de comprendre que la programmation Python n’est pas ici une fin en soi, mais plutôt un outil de scriptage et de manipulation des données satellitaires.\nPython, créé par Guido van Rossum en 1991, est un langage de programmation polyvalent et facile à apprendre, souvent comparé à un couteau suisse numérique pour sa simplicité et sa polyvalence. Comme un outil multifonction, Python peut être utilisé pour une variété de tâches, du développement web à l’analyse de données, en passant par l’intelligence artificielle.\nIl existe plusieurs distributions du langage Python, ces distributions sont comme différentes saveurs de votre glace préférée - chacune a ses propres caractéristiques uniques, mais elles sont toutes fondamentalement Python. Voici un aperçu des principales distributions :\nChaque distribution a ses forces, que ce soit la simplicité, la vitesse ou des fonctionnalités spécifiques. Le choix dépend donc de vos besoins, comme choisir entre une glace simple ou une glace royal (banana split) élaboré.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction au langage Python</span>"
    ]
  },
  {
    "objectID": "00-PriseEnMainPython.html#les-distributions",
    "href": "00-PriseEnMainPython.html#les-distributions",
    "title": "1  Introduction au langage Python",
    "section": "",
    "text": "CPython est la distribution “vanille” officielle, comme la recette originale de Python. Elle est ainsi le choix idéal pour la compatibilité et la conformité aux standards.\nAnaconda. Pensez-y comme à un sundae tout garni. Elle vient avec de nombreuses bibliothèques scientifiques préinstallées, ce qui est idéal pour l’analyse de données et l’apprentissage automatique (machine learning).\nMiniconda est une distribution légère de Python qui vous permet d’ajouter au besoin d’autres bibliothèques.\nPyPy : est une version turbo de Python, optimisée pour la vitesse.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction au langage Python</span>"
    ]
  },
  {
    "objectID": "00-PriseEnMainPython.html#les-styles-de-programmation-en-python",
    "href": "00-PriseEnMainPython.html#les-styles-de-programmation-en-python",
    "title": "1  Introduction au langage Python",
    "section": "1.2 Les styles de programmation en Python",
    "text": "1.2 Les styles de programmation en Python\nIl existe plusieurs approches pour programmer en Python. La plus directe est en version interactive en tapant python et de rentrer des commandes ligne par ligne.\n\n1.2.1 Les outils de programmation\nUn code python prend la forme d’un simple fichier texte avec l’extension .py et peut être modifié avec un simple éditeur de texte. Cependant, il n’y aura pas de rétroactions immédiates de l’interpréteur Python, ce qui rend la correction d’erreurs (débogage) beaucoup plus laborieux.\nUn IDE (Integrated Developement Environnement) est comme une boîte à outils complète pour les programmeurs, vous trouverez :\n\nUn éditeur de texte amélioré pour écrire votre code, avec des fonctionnalités comme la coloration syntaxique qui rend le code plus lisible.\nUn compilateur qui transforme votre code en instructions que l’ordinateur peut comprendre.\nUn débogueur pour trouver et corriger les erreurs, tel un détective numérique.\nDes outils d’automatisation qui effectuent des tâches répétitives, comme un assistant virtuel pour le codage.\nL’accès à la documentation des différentes librairies.\n\nCes outils intégrés permettent aux développeurs de travailler plus efficacement, en passant moins de temps à jongler entre différentes applications et plus de temps à produire du code.\nVoici quelques options populaires :\n\nPyCharm: est un des outils les plus utilisés dans l’industrie. Il offre une multitude de fonctionnalités comme l’autocomplétion intelligente et le débogage intégré, idéal pour les grands projets. Cependant, cet outil peut être assez gourmand en mémoire et en CPU. Notez qu’il existe une version gratuite et une version professionnelle.\nVisual Studio Code : gratuit, léger mais puissant, il est personnalisable avec des extensions pour Python.\nSpyder : logiciel libre et gratuit, orienté vers les applications scientifiques.\nJupyter Notebooks : imaginez un cahier interactif pour le code. Idéal pour l’analyse de données et l’apprentissage, il permet de mélanger code, texte et visualisations. Des services gratuits dans le cloud sont disponibles comme Google Colab et Kaggle. Ces environnements sont néanmoins moins appropriés pour des grands projets et le débogage. Jupyter peut souffrir de problèmes de reproductibilité dus à l’exécution arbitraire des cellules.\nMarimo se veut une version plus moderne des notebooks jupyter. Contrairement à Jupyter, Marimo garantit la cohérence entre le code, les sorties et l’état du programme. Il analyse intelligemment les relations entre les cellules et réexécute automatiquement celles qui sont affectées par des changements, éliminant ainsi les problèmes d’état caché.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction au langage Python</span>"
    ]
  },
  {
    "objectID": "00-PriseEnMainPython.html#bonnes-pratiques",
    "href": "00-PriseEnMainPython.html#bonnes-pratiques",
    "title": "1  Introduction au langage Python",
    "section": "1.3 Bonnes pratiques",
    "text": "1.3 Bonnes pratiques\nPython est un langage très dynamique, qui évolue constamment. Cela pose certains défis pour la gestion du code à long terme. Il est fortement conseillé d’utiliser des environnements virtuels pour gérer vos différentes bibliothèques (libraries). Voici quelques bonnes pratiques à suivre :\n\nN’installez par la toute dernière version de Python : Il est recommandé d’installer 1 ou 2 version antérieure, par exemple si 3.13 est la version plus récente, installer plutôt la version 3.11. Les versions trop récentes peuvent être instables. La version de python désirée peut être spécifiée au moment de la création d’un environnement virtuel (voir plus bas). Vous pouvez afficher la liste des versions de python avec la commande conda search --full-name python.\nN’utilisez pas de version obsolète de Python. Cela peut sembler contradictoire avec le point précédent mais c’est l’excès inverse. Si vous utilisez une version trop ancienne alors toutes vos librairies cesseront d’évoluer et peuvent devenir obsolètes.\nUtilisez des environnements virtuels. Pensez-y comme à des compartiments séparées pour chaque projet. Cela évite les conflits entre les différentes versions de bibliothèques (libraries)et garde votre système propre. Par exemple, si vous souhaitez vérifier une nouvelle version de Python, utilisez un environnement : conda create --name test python=3.11\nVérifiez l’installation. Après l’installation, ouvrez un terminal et tapez python --version pour vous assurer que tout fonctionne correctement.\n\n\n1.3.1 Création d’un environnement virtuel\nIl y a deux façons d’installer un environnement virtuel selon votre distribution de Python:\n\nOption 1. Vous utilisez Anaconda ou Miniconda. La commande conda est utilisée pour créer un environnement test avec Python 3.10:\n\nconda env -n test python=3.10\nconda activate test\n\nOption 2. Vous utilisez CPython\n\nconda env -n test python=3.10\nconda activate test\n\n\n1.3.2 Création d’un environnement de travail local (avancé)\nNote: les notebooks peuvent fonctionner localement uniquement sous Linux ou avec WSL2.\nLes notebooks Python fonctionnent par défaut dans l’environnement Google Colab. Si vous souhaitez faire fonctionner ces notebook localement, vous pouvez installer un environnement local avec un serveur Jupyter. Il suffit de suivre les étapes suivantes:\n1. Installer WSL2 sous Windows\n2. Installer vscode\n3. Installer Miniconda\n4. Faire une installation du contenu du livre soit en utilisant une commande git clone ou en récupérant le .zip du livre\n5. Ouvrir WSL2 et placer vous dans le répertoire du livre TraitementImagesPythonVol1. Assurez vous que vous avez accès à conda en tapant conda --version\n6. Lancer la commande conda env create -f jupyter_env.yaml\n7. Activer le nouvel environnement: conda activate jupyter_env\n8. Le serveur jupyter peut ensuite être lancé avec la commande suivante: jupyter lab --ip='*' --NotebookApp.token='' --NotebookApp.password=''\nUne fenêtre devrait alors apparaître dans votre fureteur. Dans le menu de gauche vous pouvez accéder aux notebooks dans le répertoire notebooks:\n\n\n\n\n\n\nFigure 1.1: Fenêtre principale du serveur Jupyter Lab.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction au langage Python</span>"
    ]
  },
  {
    "objectID": "00-PriseEnMainPython.html#les-structures-de-base-en-python",
    "href": "00-PriseEnMainPython.html#les-structures-de-base-en-python",
    "title": "1  Introduction au langage Python",
    "section": "1.4 Les structures de base en Python",
    "text": "1.4 Les structures de base en Python\nIl y a essentiellement deux structures de données que Python manipule : les listes et les dictionnaires.\n\n1.4.1 Les listes\nLes listes sont comme des boites extensibles où vous pouvez ranger différents types d’objets:\n\nReprésentées par des crochets : [1, 2, 3, \"python\"].\nOrdonnées et modifiables (mutable), vous pouvez récupérer une valeur par sa position avec [].\nPermettent les doublons (deux fois la même valeur).\nIdéales pour stocker des collections d’éléments que vous voulez modifier\n\n\n\n1.4.2 Les tuples\nLes tuples sont similaires aux listes, mais les boîtes sont scellées:\n\nReprésentés par des parenthèses : (1, 2, 3, \"python\").\nOrdonnés mais non modifiables (immutable).\nPermettent les doublons.\nSouvent utilisés pour stocker des données qui ne doivent pas changer (comme des paramètres).\n\n\n\n1.4.3 Les ensembles (Sets)\nLes ensembles sont comme des boites magiques qui ne gardent qu’un exemplaire de chaque objet:\n\nReprésentés par des accolades : {1, 2, 3}.\nNon ordonnés et modifiables.\nN’autorisent pas les doublons.\nUtiles pour éliminer les doublons et effectuer des opérations mathématiques sur des ensembles.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction au langage Python</span>"
    ]
  },
  {
    "objectID": "00-PriseEnMainPython.html#dictionnaires",
    "href": "00-PriseEnMainPython.html#dictionnaires",
    "title": "1  Introduction au langage Python",
    "section": "1.5 Dictionnaires",
    "text": "1.5 Dictionnaires\nLes dictionnaires sont comme des boites avec des étiquettes sur chcune d’elle :\n\nReprésentés par des accolades avec des paires clé-valeur : {\"nom\": \"Python\", \"année\": 1991}.\nNon ordonnés et modifiables.\nLes clés doivent être uniques, mais les valeurs peuvent être dupliquées\nUtiles pour stocker des données associatives ou pour créer des tables de recherche rapide",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction au langage Python</span>"
    ]
  },
  {
    "objectID": "00-PriseEnMainPython.html#programmation-objet",
    "href": "00-PriseEnMainPython.html#programmation-objet",
    "title": "1  Introduction au langage Python",
    "section": "1.6 Programmation objet",
    "text": "1.6 Programmation objet\nLa programmation orientée objet (POO) en Python est comme construire avec des blocs LEGO. Chaque objet est un bloc LEGO avec ses propres caractéristiques (attributs) et capacités (méthodes). Les classes sont les plans pour créer ces blocs. Par exemple, une classe “Voiture” pourrait avoir des attributs comme “couleur” et “vitesse”, et des méthodes comme “démarrer” et “accélérer”.\nPython rend la POO accessible avec des fonctionnalités conviviales:\n\nEncapsulation: comme emballer un cadeau, elle cache les détails internes d’un objet.\nHéritage: permet de créer de nouvelles classes basées sur des classes existantes, comme un enfant héritant des traits de ses parents.\nPolymorphisme: permet à différents objets de répondre au même message de manière unique, comme si différents animaux répondaient différemment à “fais du bruit”.\n\nCes caractéristiques font de Python un excellent choix pour apprendre et appliquer les concepts de la POO, rendant le code plus organisé et réutilisable\n\n\n\n\n\nListe des packages utilisés dans ce chapitre\n\n\n\nPour importer et manipuler des fichiers géographiques :\n\nnumpy pour manipuler des données matricielles.\nrasterio pour importer et manipuler des données matricielles.\n\nPour construire des cartes et des graphiques :\n\nmatplotlib est certainement le package le plus complet pour l’affichage général.\nseaborn pour construire des graphiques plus détaillés en particulier pour les statistiques.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction au langage Python</span>"
    ]
  },
  {
    "objectID": "01-ImportationManipulationImages.html",
    "href": "01-ImportationManipulationImages.html",
    "title": "2  Importation et manipulation de données spatiales",
    "section": "",
    "text": "2.1 Préambule\nAssurez-vous de lire ce préambule avant d’exécutez le reste du notebook.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Importation et manipulation de données spatiales</span>"
    ]
  },
  {
    "objectID": "01-ImportationManipulationImages.html#préambule",
    "href": "01-ImportationManipulationImages.html#préambule",
    "title": "2  Importation et manipulation de données spatiales",
    "section": "",
    "text": "2.1.1 Objectifs\nDans ce chapitre, nous abordons quelques formats d’images ainsi que leur lecture. Ce chapitre est aussi disponible sous la forme d’un notebook Python:\n\n\n\n\n\n\nObjectifs d’apprentissage visés dans ce chapitre\n\nÀ la fin de ce chapitre, vous devriez être en mesure de :\n\nconnaître les principales bibliothèques Python pour lire une image;\naccéder à l’information d’une image avant de la lire;\ncomprendre les principaux formats pour une image\nmanipuler la matrice de la donnée d’une image avec numpy\n\n\n\n\n2.1.2 Bibliothèques\nLes bibliothèques qui vont être explorées dans ce chapitre sont les suivantes:\n\nSciPy\nNumPy\nopencv-python · PyPI\nscikit-image\nRasterio\nXarray\nrioxarray\n\nDans l’environnement Google Colab, seul rioxarray et gdal doivent être installés:\n\n!apt-get update\n!apt-get install gdal-bin libgdal-dev\n!pip install -q rioxarray\n\nVérifier les importations:\n\nimport numpy as np\nimport rioxarray as rxr\nfrom scipy import signal\nimport xarray as xr\nimport xrscipy\nimport matplotlib.pyplot as plt\n\n\n\n2.1.3 Données\nNous utilisons ces images dans ce chapitre:\n\nimport gdown\n\ngdown.download('https://drive.google.com/uc?export=download&confirm=pbef&id=1a6Ypg0g1Oy4AJt9XWKWfnR12NW1XhNg_', output= 'RGBNIR_of_S2A.tif')\ngdown.download('https://drive.google.com/uc?export=download&confirm=pbef&id=1a4PQ68Ru8zBphbQ22j0sgJ4D2quw-Wo6', output= 'landsat7.tif')\ngdown.download('https://drive.google.com/uc?export=download&confirm=pbef&id=1_zwCLN-x7XJcNHJCH6Z8upEdUXtVtvs1', output= 'berkeley.jpg')\n!wget https://raw.githubusercontent.com/sfoucher/TraitementImagesPythonVol1/refs/heads/main/images/modis-aqua.PNG -O modis-aqua.PNG\n\nVérifiez que vous êtes capable de les lire:\n\nwith rxr.open_rasterio('berkeley.jpg', mask_and_scale= True) as img_rgb:\n    print(img_rgb)\nwith rxr.open_rasterio('RGBNIR_of_S2A.tif', mask_and_scale= True) as img_rgbnir:\n    print(img_rgbnir)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Importation et manipulation de données spatiales</span>"
    ]
  },
  {
    "objectID": "01-ImportationManipulationImages.html#importation-dimages",
    "href": "01-ImportationManipulationImages.html#importation-dimages",
    "title": "2  Importation et manipulation de données spatiales",
    "section": "2.2 Importation d’images",
    "text": "2.2 Importation d’images\nLa première étape avant tout traitement est d’accéder à la donnée image pour qu’elle soit manipulée par le langage Python. L’imagerie satellite présente certains défis notamment en raison de la taille parfois très importante des images. Il existe maintenant certaines bibliothèques, comme Xarray, qui visent à optimiser la lecture et l’écriture de grandes images. Il est donc conseillé de toujours garder un oeil sur l’espace mémoire occupé par les variables Python représentant les images. La librairie principale en géomatique qui permettre d’importer (et d’exporter) de l’imagerie est la librairie GDAL qui rassemble la plupart des formats sous forme de driver (ou pilote en français).\nDans le domaine de la géomatique, il faut prêter attention à trois caractéristiques principales des images:\n1. La matrice des données elle-même qui contient les valeurs brutes des pixels. Cette matrice sera souvent un cube à trois dimensions. En Python, ce cube sera le plus souvent un objet de la librairie NumPy (voir section).\n2. La dynamique des images c.-à.-d le format de stockage des valeurs individuelles (octet, entier, double, etc.). Ce format décide principalement de la résolution radiométrique et des valeurs minimales et maximales supportées.\n3. Le nombre de bandes spectrales de l’image qui est souvent supérieur à trois et peut atteindre plusieurs centaines de bandes pour certains capteurs (notamment hyperspectraux).\n4. La métadonnée qui va transporter l’information auxiliaire de l’image comme les dimensions et la position de l’image, la date, etc. Cette donnée auxiliaire prendra souvent la forme d’un dictionnaire Python. Elle contiendra aussi l’information de géoréférence.\nLes différents formats se distinguent principalement sur la manière dont ces trois caractéristiques sont gérées.\n\n2.2.1 Formats des images\nIl existe de nombreux formats numériques pour la donnée de type image parfois appelé donnée matricielle ou donnée raster. La librairie GDAL rassemble la plupart des formats matriciels rencontrés en géomatique (voir Raster drivers — GDAL documentation pour une liste complète).\nOn peut distinguer deux grandes familles de format:\n1. Les formats de type RVB issus de l’imagerie numérique grand public comme JPEG, png, etc. Ces formats ne supportent généralement que trois bandes au maximum (rouge, vert et bleu) et des valeurs de niveaux de gris entre 0 et 255 (format dit 8 bits ou uint8).\n2. Les géo-formats issus des domaines scientifiques ou techniques comme GeoTIFF, HDF5, NetCDF, etc. qui peuvent inclure plus que trois bandes et des dynamiques plus élevées (16 bits ou même float).\nLes formats RVB restent très utilisés en Python notamment par les bibliothèques dites de vision par ordinateur (Computer Vision) comme OpenCV et sickit-image ainsi que les grandes bibliothèques en apprentissage profond (PyTorch, Tensorflow).\n\n\n\n\n\nInstallation de gdal dans un système Linux\n\n\n\nPour installer GDAL :\n\n!apt-get update\n!apt-get install gdal-bin libgdal-dev\n\n\n\n2.2.1.1 Formats de type RVB\nLes premiers formats pour de l’imagerie à une bande (monochrome) et à trois bandes (image couleur rouge-vert-bleu) sont issus du domaine des sciences de l’ordinateur. On trouvera, entre autres, les formats pbm, png et jpeg. Ces formats supportent peu de métadonnées et sont placées dans un entête (header) très limité. Cependant, ils restent très populaires dans le domaine de la vision par ordinateur et sont très utilisés en apprentissage profond en particulier. Pour la lecture des images RVB, on peut utiliser les bibliothèques Rasterio, PIL ou OpenCV.\n\n2.2.1.1.1 Lecture avec la librairie PIL\nLa librairie PIL retourne un objet de type PngImageFile, l’affichage de l’image se fait directement dans la cellule de sortie.\n\nfrom PIL import Image\nimg = Image.open('modis-aqua.PNG')\nimg\n\n\n\n\n\n\n\n\n\n\n2.2.1.1.2 Lecture avec la librairie OpenCV\nLa librairie OpenCV est aussi très populaire en vision par ordinateur. La fonction imread donne directement un objet de type NumPy en sortie.\n\nimport cv2\nimg = cv2.imread('modis-aqua.PNG')\nimg\n\narray([[[17, 50, 33],\n        [15, 49, 31],\n        [14, 48, 30],\n        ...,\n        [23, 56, 36],\n        [23, 55, 36],\n        [22, 55, 36]],\n\n       [[18, 51, 34],\n        [16, 50, 32],\n        [15, 49, 32],\n        ...,\n        [27, 59, 40],\n        [28, 60, 41],\n        [27, 60, 41]],\n\n       [[18, 53, 35],\n        [18, 52, 34],\n        [18, 51, 34],\n        ...,\n        [31, 64, 44],\n        [34, 66, 47],\n        [33, 65, 46]],\n\n       ...,\n\n       [[34, 74, 48],\n        [35, 73, 48],\n        [34, 70, 46],\n        ...,\n        [41, 74, 54],\n        [41, 73, 54],\n        [41, 73, 54]],\n\n       [[36, 76, 50],\n        [36, 74, 49],\n        [35, 71, 47],\n        ...,\n        [37, 70, 51],\n        [38, 71, 51],\n        [38, 71, 51]],\n\n       [[36, 76, 50],\n        [35, 73, 48],\n        [33, 69, 45],\n        ...,\n        [31, 63, 44],\n        [33, 65, 46],\n        [33, 66, 46]]], dtype=uint8)\n\n\n\n\n2.2.1.1.3 Lecture avec la librairie RasterIO\nRien ne nous empêche de lire une image de format RVB avec RasterIO comme décrit dans ci-dessous. Vous noterez cependant les avertissements concernant l’absence de géoréférence pour ce type d’image.\n\nimport rasterio\nimg= rasterio.open('modis-aqua.PNG')\nimg\n\n&lt;open DatasetReader name='modis-aqua.PNG' mode='r'&gt;\n\n\n\n\n\n2.2.1.2 Le format GeoTiff\nLe format GeoTIFF est une extension du format TIFF (Tagged Image File Format) qui permet d’incorporer des métadonnées géospatiales directement dans un fichier image. Développé initialement par Dr. Niles Ritter au Jet Propulsion Laboratory de la NASA dans les années 1990, GeoTIFF est devenu un standard de facto pour le stockage et l’échange d’images géoréférencées dans les domaines de la télédétection et des systèmes d’information géographique (SIG). Ce format supporte plus que trois bandes aussi longtemps que ces bandes sont de même dimension.\nLe format GeoTIFF est très utilisé et est largement supporté par les bibliothèques et logiciels géospatiaux, notamment GDAL (Geospatial Data Abstraction Library), qui offre des capacités de lecture et d’écriture pour ce format. Cette compatibilité étendue a contribué à son adoption généralisée dans la communauté géospatiale.\n\n2.2.1.2.1 Standardisation par l’OGC\nLe standard GeoTIFF proposé par l’Open Geospatial Consortium (OGC) en 2019 formalise et étend les spécifications originales du format GeoTIFF, offrant une norme robuste pour l’échange d’images géoréférencées. Cette standardisation, connue sous le nom d’OGC GeoTIFF 1.1 (2019), apporte plusieurs améliorations et clarifications importantes.\n\n\n\n2.2.1.3 Le format COG\nUne innovation récente dans l’écosystème GeoTIFF est le format Cloud Optimized GeoTIFF (COG), conçu pour faciliter l’utilisation de fichiers GeoTIFF hébergés sur des serveurs web HTTP. Le COG permet aux utilisateurs et aux logiciels d’accéder à des parties spécifiques du fichier sans avoir à le télécharger entièrement, ce qui est particulièrement utile pour les applications basées sur l’infonuagique.\n\n\n\n2.2.2 Métadonnées des images\nLa manière la plus directe d’accéder à la métadonnée d’une image est d’utiliser les commandes rio info de la librairie Rasterio ou gdalinfo de la librairie gdal. Le résultat est imprimé dans la sortie standard ou sous forme d’un dictionnaire Python.\n\n!gdalinfo RGBNIR_of_S2A.tif\n\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nDriver: GTiff/GeoTIFF\nFiles: RGBNIR_of_S2A.tif\n       RGBNIR_of_S2A.tif.aux.xml\nSize is 2074, 1926\nCoordinate System is:\nPROJCS[\"WGS 84 / UTM zone 18N\",\n    GEOGCS[\"WGS 84\",\n        DATUM[\"WGS_1984\",\n            SPHEROID[\"WGS 84\",6378137,298.257223563,\n                AUTHORITY[\"EPSG\",\"7030\"]],\n            AUTHORITY[\"EPSG\",\"6326\"]],\n        PRIMEM[\"Greenwich\",0,\n            AUTHORITY[\"EPSG\",\"8901\"]],\n        UNIT[\"degree\",0.0174532925199433,\n            AUTHORITY[\"EPSG\",\"9122\"]],\n        AUTHORITY[\"EPSG\",\"4326\"]],\n    PROJECTION[\"Transverse_Mercator\"],\n    PARAMETER[\"latitude_of_origin\",0],\n    PARAMETER[\"central_meridian\",-75],\n    PARAMETER[\"scale_factor\",0.9996],\n    PARAMETER[\"false_easting\",500000],\n    PARAMETER[\"false_northing\",0],\n    UNIT[\"metre\",1,\n        AUTHORITY[\"EPSG\",\"9001\"]],\n    AXIS[\"Easting\",EAST],\n    AXIS[\"Northing\",NORTH],\n    AUTHORITY[\"EPSG\",\"32618\"]]\nOrigin = (731780.000000000000000,5040800.000000000000000)\nPixel Size = (10.000000000000000,-10.000000000000000)\nMetadata:\n  AREA_OR_POINT=Area\n  TIFFTAG_IMAGEDESCRIPTION=subset_RGBNIR_of_S2A_MSIL2A_20240625T153941_N0510_R011_T18TYR_20240625T221903\n  TIFFTAG_RESOLUTIONUNIT=1 (unitless)\n  TIFFTAG_XRESOLUTION=1\n  TIFFTAG_YRESOLUTION=1\nImage Structure Metadata:\n  INTERLEAVE=BAND\nCorner Coordinates:\nUpper Left  (  731780.000, 5040800.000) ( 72d 2' 3.11\"W, 45d28'55.98\"N)\nLower Left  (  731780.000, 5021540.000) ( 72d 2'35.69\"W, 45d18'32.70\"N)\nUpper Right (  752520.000, 5040800.000) ( 71d46' 9.19\"W, 45d28'30.08\"N)\nLower Right (  752520.000, 5021540.000) ( 71d46'44.67\"W, 45d18' 6.95\"N)\nCenter      (  742150.000, 5031170.000) ( 71d54'23.16\"W, 45d23'31.71\"N)\nBand 1 Block=2074x1926 Type=UInt16, ColorInterp=Gray\n  Min=86.000 Max=15104.000 \n  Minimum=86.000, Maximum=15104.000, Mean=1426.625, StdDev=306.564\n  Metadata:\n    STATISTICS_MAXIMUM=15104\n    STATISTICS_MEAN=1426.6252674912\n    STATISTICS_MINIMUM=86\n    STATISTICS_STDDEV=306.56427126942\n    STATISTICS_VALID_PERCENT=100\nBand 2 Block=2074x1926 Type=UInt16, ColorInterp=Undefined\n  Min=1139.000 Max=14352.000 \n  Minimum=1139.000, Maximum=14352.000, Mean=1669.605, StdDev=310.919\n  Metadata:\n    STATISTICS_MAXIMUM=14352\n    STATISTICS_MEAN=1669.6050060032\n    STATISTICS_MINIMUM=1139\n    STATISTICS_STDDEV=310.91935787639\n    STATISTICS_VALID_PERCENT=100\nBand 3 Block=2074x1926 Type=UInt16, ColorInterp=Undefined\n  Min=706.000 Max=15280.000 \n  Minimum=706.000, Maximum=15280.000, Mean=1471.392, StdDev=385.447\n  Metadata:\n    STATISTICS_MAXIMUM=15280\n    STATISTICS_MEAN=1471.3923473736\n    STATISTICS_MINIMUM=706\n    STATISTICS_STDDEV=385.44654593014\n    STATISTICS_VALID_PERCENT=100\nBand 4 Block=2074x1926 Type=UInt16, ColorInterp=Undefined\n  Min=1067.000 Max=15642.000 \n  Minimum=1067.000, Maximum=15642.000, Mean=4393.945, StdDev=1037.934\n  Metadata:\n    STATISTICS_MAXIMUM=15642\n    STATISTICS_MEAN=4393.94485025\n    STATISTICS_MINIMUM=1067\n    STATISTICS_STDDEV=1037.933939728\n    STATISTICS_VALID_PERCENT=100\n\n\nLe plus simple est d’utiliser la fonction rio info:\n\n!rio info RGBNIR_of_S2A.tif --indent 2 --verbose",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Importation et manipulation de données spatiales</span>"
    ]
  },
  {
    "objectID": "01-ImportationManipulationImages.html#manipulation-des-images",
    "href": "01-ImportationManipulationImages.html#manipulation-des-images",
    "title": "2  Importation et manipulation de données spatiales",
    "section": "2.3 Manipulation des images",
    "text": "2.3 Manipulation des images\n\n2.3.1 Manipulation de la matrice de pixels\nLa donnée brute de l’image est généralement contenue dans un cube matricielle à trois dimensions (deux dimensions spatiales et une dimension spectrale). Comme exposé précédemment, la librairie dite “fondationnelle” pour la manipulation de matrices en Python est NumPy. Cette librairie contient un nombre très important de fonctionnalités couvrant l’algèbre linéaire, les statistiques, etc.; elle constitue la fondation de nombreuses bibliothèques en traitement numérique (voir (figure 2.1))\n\n\n\n\n\n\nFigure 2.1: La librairie NumPy est le fondement de nombreuses bibliothèques scientifiques (d’après (Harris 2020)).\n\n\n\n\n\n2.3.2 Information de base\nLes deux informations de base à afficher sur une matrice sont 1) les dimensions de la matrice et 2) le format de stockage (le type). Pour cela, on peut utiliser le code ci-dessous, dont le résultat nous informe que la matrice a trois dimensions et une taille de (442, 553, 3) et un type uint8 qui représente 1 octet (8 bit). Par conséquent, la matrice a 442 lignes, 553 colonnes et 3 canaux ou bandes. Il faut prêter une attention particulière aux valeurs minimales et maximales tolérées par le type de la donnée comme indiqué dans le (tableau 2.1) (voir aussi Data types — NumPy v2.1 Manual).\n\nimport cv2\nimg = cv2.imread('modis-aqua.PNG')\nprint('Nombre de dimensions: ',img.ndim)\nprint('Dimensions de la matrice: ',img.shape)\nprint('Type de la donnée: ',img.dtype)\n\nNombre de dimensions:  3\nDimensions de la matrice:  (442, 553, 3)\nType de la donnée:  uint8\n\n\n\n\n\n\nTableau 2.1: Type de données de NumPy\n\n\n\n\n\n\ndtype\nNom\nTaille (bits)\nMin\nMax\n\n\n\n\nuint8\nchar\n8\n0\n255\n\n\nint8\nsigned char\n8\n-127\n128\n\n\nuint16\nunsigned short\n16\n0\n-32768\n\n\nint16\nshort\n16\n0\n655355\n\n\n\n\n\n\n\n\n\n\n2.3.3 Découpage et indexation de la matrice\nL’indexation et le découpage (slicing) des matrices dans NumPy sont des techniques essentielles pour manipuler efficacement les données multidimensionnelles en Python, offrant une syntaxe puissante et flexible pour accéder et modifier des sous-ensembles spécifiques d’éléments dans les tableaux (voir figure 2.2). Indexer une matrice consiste à accéder à une valeur dans la matrice pour une position particulière, la syntaxe générale est matrice[ligne, colonne, bande] et est similaire à la manipulation des listes en Python. Les indices commencent à 0 et se termine à la taille-1 de l’axe considéré.\n\n\n\n\n\n\nFigure 2.2: Vue d’ensemble des opérations de base des matrices avec NumPy\n\n\n\nLe découpage (ou slicing en anglais) consiste à produire une nouvelle matrice qui est un sous-ensemble de la matrice d’origine. Un découpage se fait avec le symbole ‘:’, la syntaxe générale pour définir un découpage est [début:fin:pas]. Si on ne spécifie pas début ou fin alors les valeurs 0 ou dimension-1 sont considérées implicitement. Quelques exemples: * choisir un pixel en particulier avec toutes les bandes: matrice[1,1,:] * choisir la colonne 2: matrice[:,2,:]\nLa syntaxe de base pour le découpage (slicing) des tableaux NumPy repose sur l’utilisation des deux-points (:) à l’intérieur des crochets d’indexation. Cette notation permet de sélectionner des plages d’éléments de manière concise et intuitive. La structure générale du découpage est matrice[start:stop:step], où : 1. start représente l’index de départ (inclus) 2. stop indique l’index de fin (exclu) 3. step définit l’intervalle entre chaque élément sélectionné\nSi l’un de ces paramètres est omis, NumPy utilise des valeurs par défaut : 0 pour start, la taille du tableau pour stop, et 1 pour step. Par exemple, pour un tableau unidimensionnel array, on peut extraire les éléments du deuxième au quatrième avec array[1:4]. Pour sélectionner tous les éléments à partir du troisième, on utiliserait array[2:]. Cette syntaxe s’applique également aux tableaux multidimensionnels, où chaque dimension est séparée par une virgule. Ainsi, pour une matrice 2D m, m[0:2, 1:3] sélectionnerait une sous-matrice 2x2 composée des deux premières lignes et des deuxième et troisième colonnes. L’indexation négative est également supportée, permettant de compter à partir de la fin du tableau. Par exemple, a[-3:] sélectionnerait les trois derniers éléments d’un tableau.\n\nimport cv2\nimg = cv2.imread('modis-aqua.PNG')\nimg_col = img[:,1,:]\nprint('Nombre de dimensions: ',img_col.ndim)\nprint('Dimensions de la matrice: ',img_col.shape)\n\nNombre de dimensions:  2\nDimensions de la matrice:  (442, 3)\n\n\n\n\n\n\n\nUne vue versus une copie\n\n\nAvec NumPy, les manipulations peuvent créer des vues ou des copies. Une vue est une simple représentation de la même donnée originale alors qu’une copie est un nouvel espace mémoire.\nPar défaut, un découpage créé une vue.\nOn peut vérifier si l’espace mémoire est partagé avec np.shares_memory(arr, slice_arr).\nOn peut toujours forcer une copie avec la méthode copy()\n\n\n\n\n2.3.3.1 Masquage\nL’utilisation d’un masque est un outil important en traitement d’image car la plupart des images de télédétection contiennent des pixels non valides qu’il faut exclure des traitements (ce que l’on appelle le no data en Anglais). Il y a plusieurs raison possibles pour la présence de pixels non valides:\n\nL’image est projetée dans une grille cartographique et certaines zones, généralement situées en dehors de l’empreinte au sol du capteur, sont à exclure.\nLa présence de nuages que l’on veut exclure.\nLa présence de pixels erronés dus à des problèmes de capteurs.\nLa présence de valeurs non numériques (not a number ou nan)\n\nLa librairie NumPy fournit des mécanismes pour exclure automatiquement certaines valeurs.\n\n\n\n2.3.4 Changement de projection cartographique (à venir)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Importation et manipulation de données spatiales</span>"
    ]
  },
  {
    "objectID": "01-ImportationManipulationImages.html#données-en-géoscience",
    "href": "01-ImportationManipulationImages.html#données-en-géoscience",
    "title": "2  Importation et manipulation de données spatiales",
    "section": "2.4 Données en géoscience",
    "text": "2.4 Données en géoscience\nLes données en géoscience contiennent beaucoup de métadonnées et peuvent être composées de différentes variables avec différentes unités, résolution, etc. Ces données sont aussi souvent étiquetées avec des dates sur certains axes, des coordonnées géographiques, des identifiants d’expériences, etc. Par conséquent, utiliser seulement des matrices est souvent incomplet (Hoyer et Hamman 2017).\nCalibration, unités, données manquantes, données éparses.\n\n2.4.1 xarray\nXarray est une puissante bibliothèque Python qui améliore les matrices multidimensionnelles de type numpy en y ajoutant des étiquettes, des dimensions, des coordonnées et des attributs. Elle fournit deux structures de données principales : DataArray (un tableau étiqueté à n dimensions) et Dataset (une base de données de tableaux multidimensionnels en mémoire).\nLes caractéristiques principales sont les suivantes:\n\nOpérations sur les dimensions nommées au lieu des numéros d’axe\nSélection et opérations basées sur les étiquettes\nDiffusion automatique de tableaux basée sur les noms de dimensions\nAlignement de type base de données avec des étiquettes de coordonnées\nSuivi des métadonnées grâce à des dictionnaires Python\n\n\n2.4.1.1 Avantages\nLa bibliothèque réduit considérablement la complexité du code et améliore la lisibilité du code pour les applications de calcul scientifique dans divers domaines, notamment la physique, l’astronomie, les géosciences, la bio-informatique, l’ingénierie, la finance et l’apprentissage profond. Elle s’intègre de manière transparente avec NumPy et pandas tout en restant compatible avec l’écosystème Python au sens large.\n\n\n2.4.1.2 DataArray\nUn tableau multidimensionnel étiqueté avec des propriétés clées :\n\nvaleurs : Les données réelles du tableau\ndims : Dimensions nommées (par exemple, « x », « y », « z »)\ncoords : Dictionnaire de tableaux étiquetant chaque point\nattrs : Stockage de métadonnées arbitraires\nname : Identifiant facultatif\n\n\n\n2.4.1.3 Dataset\nUn conteneur de type dictionnaire de DataArrays avec des dimensions alignées, contenant :\n\ndims : Dictionnaire de correspondance entre les noms des dimensions et les longueurs\ndata_vars : Dictionnaire des variables du DataArray\ncoords : Dictionnaire des variables de coordonnées\nattrs : Stockage des métadonnées\n\nLes principales différences sont les suivantes :\n\nDataArray contient un seul tableau avec des étiquettes\nLe Dataset contient plusieurs DataArrays alignés.\n\nCes trois structures prennent en charge les opérations de type dictionnaire et les calculs de coordination tout en conservant les métadonnées.\n\n\n\n\n\n\nFigure 2.3: Organisation d’un Dataset dans xarray\n\n\n\n\n\n\n\n\n\nHarris, Millman, C. R. 2020. « Array programming with NumPy. » Nature: 357‑362. https://doi.org/10.1038/s41586-020-2649-2.\n\n\nHoyer, S. et J. Hamman. 2017. « xarray: N-D labeled Arrays and Datasets in Python. » Journal of Open Research Software 5 (1): 10. https://doi.org/10.5334/jors.148.\n\n\nOGC. 2019. « OGC GeoTIFF Standard. » https://docs.ogc.org/is/19-008r4/19-008r4.html/.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Importation et manipulation de données spatiales</span>"
    ]
  },
  {
    "objectID": "02-RehaussementVisualisationImages.html",
    "href": "02-RehaussementVisualisationImages.html",
    "title": "3  Réhaussement et visualisation d’images",
    "section": "",
    "text": "3.1 Préambule\nAssurez-vous de lire ce préambule avant d’exécutez le reste du notebook.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Réhaussement et visualisation d'images</span>"
    ]
  },
  {
    "objectID": "02-RehaussementVisualisationImages.html#préambule",
    "href": "02-RehaussementVisualisationImages.html#préambule",
    "title": "3  Réhaussement et visualisation d’images",
    "section": "",
    "text": "3.1.1 Objectifs\nDans ce chapitre, nous abordons quelques techniques de réhaussement et de visualisation d’images. Ce chapitre est aussi disponible sous la forme d’un notebook Python:\n\n\n\n\n\n\nObjectifs d’apprentissage visés dans ce chapitre\n\nÀ la fin de ce chapitre, vous devriez être en mesure de :\n\nexploiter les statistiques d’une image pour améliorer la visualisation;\ncalculer les histogrammes de valeurs;\nappliquer une transformation linéaire ou non linéaire pour améliorer une visualisation;\ncomprendre le principe des composés colorés;\n\n\n\n\n3.1.2 \n\n\n3.1.3 Bibliothèques\nLes bibliothèques qui vont être explorées dans ce chapitre sont les suivantes:\n\nSciPy\nNumPy\nopencv-python · PyPI\nscikit-image\nRasterio\nXarray\nrioxarray\n\nDans l’environnement Google Colab, seul rioxarray et GDAL doivent être installés:\n\n%%capture\n!apt-get update\n!apt-get install gdal-bin libgdal-dev\n\nDans l’environnement Google Colab, il convient de s’assurer que les librairies sont installées:\n\n%%capture\n!pip install -qU matplotlib rioxarray xrscipy scikit-image\n\nVérifier les importations:\n\nimport numpy as np\nimport rioxarray as rxr\nfrom scipy import signal\nimport xarray as xr\nimport xrscipy\nimport matplotlib.pyplot as plt\n\n\n\n3.1.4 Données\nNous utiliserons les images suivantes dans ce chapitre:\n\n%%capture\nimport gdown\n\ngdown.download('https://drive.google.com/uc?export=download&confirm=pbef&id=1a6Ypg0g1Oy4AJt9XWKWfnR12NW1XhNg_', output= 'RGBNIR_of_S2A.tif')\ngdown.download('https://drive.google.com/uc?export=download&confirm=pbef&id=1a6O3L_abOfU7h94K22At8qtBuLMGErwo', output= 'sentinel2.tif')\ngdown.download('https://drive.google.com/uc?export=download&confirm=pbef&id=1_zwCLN-x7XJcNHJCH6Z8upEdUXtVtvs1', output= 'berkeley.jpg')\ngdown.download('https://drive.google.com/uc?export=download&confirm=pbef&id=1dM6IVqjba6GHwTLmI7CpX8GP2z5txUq6', output= 'SAR.tif')\ngdown.download('https://drive.google.com/uc?export=download&confirm=pbef&id=1a4PQ68Ru8zBphbQ22j0sgJ4D2quw-Wo6', output= 'landsat7.tif')\n\nVérifiez que vous êtes capable de les lire :\n\nwith rxr.open_rasterio('berkeley.jpg', mask_and_scale= True) as img_rgb:\n    print(img_rgb)\nwith rxr.open_rasterio('sentinel2.tif', mask_and_scale= True) as img_s2:\n    print(img_s2)\nwith rxr.open_rasterio('RGBNIR_of_S2A.tif', mask_and_scale= True) as img_rgbnir:\n    print(img_rgbnir)\nwith rxr.open_rasterio('SAR.tif', mask_and_scale= True) as img_SAR:\n    print(img_SAR)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Réhaussement et visualisation d'images</span>"
    ]
  },
  {
    "objectID": "02-RehaussementVisualisationImages.html#visualisation-en-python",
    "href": "02-RehaussementVisualisationImages.html#visualisation-en-python",
    "title": "3  Réhaussement et visualisation d’images",
    "section": "3.2 Visualisation en Python",
    "text": "3.2 Visualisation en Python\nID’emblée, il faut mentionner que Python n’est pas vraiment fait pour visualiser de la donnée de grande taille, le niveau d’interactivité est aussi assez limité. Pour une visualisation interactives, il est plutôt conseillé d’utiliser un outil comme QGIS. Néanmoins, il est possible de visualiser de petites images avec la librairie matplotlib qui est la librairie principale de visualisation en Python. Cette librairie est extrêmement riche et versatile, nous ne présenterons que les bases nécessaires pour démarrer. Le lecteur désirant aller plus loin pourra consulter les nombreux tutoriels disponibles comme celui-ci.\nLa fonction de base pour créer une figure est subplots, la largeur et la hauteur en pouces de la figure peuvent être contrôlées via le paramètre figsize:\n\nimport matplotlib.pyplot as plt\nfig, ax= plt.subplots(figsize=(5, 4))\nplt.show()\n\n\n\n\n\n\n\n\nPour l’affichage des images, la fonction imshow permet d’afficher une matrice 2D à une dimension en format float ou une matrice RVB avec 3 bandes. Il est important que les dimensions de la matrice soient dans l’ordre hauteur, largeur et bande.\n\nimport matplotlib.pyplot as plt\nfig, ax= plt.subplots(figsize=(6, 5))\nplt.imshow(img_rgbnir[0].data)\nplt.show()\n\n\n\n\n\n\n\n\nPour un affichage à trois bandes, les valeurs seront ramenées sur une échelle de 0 à 1, il est donc nécessaire de normaliser les valeurs avant l’affichage:\n\nimport matplotlib.pyplot as plt\nfig, ax= plt.subplots(figsize=(6, 5))\nplt.imshow(img_rgbnir.data.transpose(1,2,0)/2500.0)\nplt.show()\n\n\n\n\n\n\n\n\nOn remarquera les valeurs des axes x et y avec une origine en haut à gauche. Ceci est un référentiel purement matriciel (lignes et colonnes); autrement dit, il n’y a pas ici de géoréférence. Pour pallier à cette limitation, les librairies rasterio et xarray proposent une extension de la fonction imshow permettant d’afficher les coordonnées cartographiques ainsi qu’un contrôle la dynamique de l’image:\n\nimport rioxarray as rxr\nfig, ax= plt.subplots(figsize=(6, 5))\nimg_rgbnir.sel(band=[1,2,3]).plot.imshow(vmin=86, vmax=5000)\nax.set_title('Imshow avec rioxarray')\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Réhaussement et visualisation d'images</span>"
    ]
  },
  {
    "objectID": "02-RehaussementVisualisationImages.html#réhaussements-visuels",
    "href": "02-RehaussementVisualisationImages.html#réhaussements-visuels",
    "title": "3  Réhaussement et visualisation d’images",
    "section": "3.3 Réhaussements visuels",
    "text": "3.3 Réhaussements visuels\nLe réhaussement visuel d’une image vise principalement à améliorer la qualité visuelle d’une image en améliorant le contraste, la dynamique ou la texture d’une image. De manière générale, ce réhaussement ne modifie pas la donnée d’origine mais il est appliquée dynamiquement à l’affichage pour des fins d’inspection visuelle. Le réhaussement nécessite généralement une connaissance des caractéristiques statistiques d’une image. Ces statistiques sont ensuite exploitées pour appliquer diverses transformations linéaires ou non linéaires.\n\n3.3.1 Statistiques d’une image\nOn peut considérer un ensemble de statistique pour chacune des bandes d’une image:\n\nvaleurs minimales et maximales\nvaleurs moyennes,\nQuartiles (1er quartile, médiane et 3ième quartile), quantiles et percentiles.\nécart-type, et coefficients d’asymétrie (skewness) et d’applatissement (kurtosis)\n\nCes statistiques doivent être calculées pour chaque bande d’une image multispectrale.\nEn ligne de commande, gdalinfo permet d’interroger rapidement un fichier image pour connaitre ces statistiques univariées de base:\n\n!gdalinfo -stats landsat7.tif\n\nDriver: GTiff/GeoTIFF\nFiles: landsat7.tif\n       landsat7.tif.aux.xml\nSize is 2181, 1917\nCoordinate System is:\nPROJCS[\"WGS 84 / Pseudo-Mercator\",\n    GEOGCS[\"WGS 84\",\n        DATUM[\"WGS_1984\",\n            SPHEROID[\"WGS 84\",6378137,298.257223563,\n                AUTHORITY[\"EPSG\",\"7030\"]],\n            AUTHORITY[\"EPSG\",\"6326\"]],\n        PRIMEM[\"Greenwich\",0,\n            AUTHORITY[\"EPSG\",\"8901\"]],\n        UNIT[\"degree\",0.0174532925199433,\n            AUTHORITY[\"EPSG\",\"9122\"]],\n        AUTHORITY[\"EPSG\",\"4326\"]],\n    PROJECTION[\"Mercator_1SP\"],\n    PARAMETER[\"central_meridian\",0],\n    PARAMETER[\"scale_factor\",1],\n    PARAMETER[\"false_easting\",0],\n    PARAMETER[\"false_northing\",0],\n    UNIT[\"metre\",1,\n        AUTHORITY[\"EPSG\",\"9001\"]],\n    AXIS[\"X\",EAST],\n    AXIS[\"Y\",NORTH],\n    EXTENSION[\"PROJ4\",\"+proj=merc +a=6378137 +b=6378137 +lat_ts=0.0 +lon_0=0.0 +x_0=0.0 +y_0=0 +k=1.0 +units=m +nadgrids=@null +wktext +no_defs\"],\n    AUTHORITY[\"EPSG\",\"3857\"]]\nOrigin = (-13651650.000000000000000,4576290.000000000000000)\nPixel Size = (30.000000000000000,-30.000000000000000)\nMetadata:\n  AREA_OR_POINT=Area\n  OVR_RESAMPLING_ALG=NEAREST\n  TIFFTAG_RESOLUTIONUNIT=1 (unitless)\n  TIFFTAG_XRESOLUTION=1\n  TIFFTAG_YRESOLUTION=1\nImage Structure Metadata:\n  COMPRESSION=DEFLATE\n  INTERLEAVE=PIXEL\nCorner Coordinates:\nUpper Left  (-13651650.000, 4576290.000) (122d38' 5.49\"W, 37d58'40.08\"N)\nLower Left  (-13651650.000, 4518780.000) (122d38' 5.49\"W, 37d34'10.00\"N)\nUpper Right (-13586220.000, 4576290.000) (122d 2'49.53\"W, 37d58'40.08\"N)\nLower Right (-13586220.000, 4518780.000) (122d 2'49.53\"W, 37d34'10.00\"N)\nCenter      (-13618935.000, 4547535.000) (122d20'27.51\"W, 37d46'26.05\"N)\nBand 1 Block=512x512 Type=Byte, ColorInterp=Red\n  Min=19.000 Max=233.000 \n  Minimum=19.000, Maximum=233.000, Mean=98.433, StdDev=21.164\n  NoData Value=0\n  Overviews: 1091x959, 546x480\n  Metadata:\n    STATISTICS_MAXIMUM=233\n    STATISTICS_MEAN=98.433096940153\n    STATISTICS_MINIMUM=19\n    STATISTICS_STDDEV=21.164021026458\nBand 2 Block=512x512 Type=Byte, ColorInterp=Green\n  Min=19.000 Max=178.000 \n  Minimum=19.000, Maximum=178.000, Mean=55.068, StdDev=22.204\n  NoData Value=0\n  Overviews: 1091x959, 546x480\n  Metadata:\n    STATISTICS_MAXIMUM=178\n    STATISTICS_MEAN=55.067787534804\n    STATISTICS_MINIMUM=19\n    STATISTICS_STDDEV=22.203571974581\nBand 3 Block=512x512 Type=Byte, ColorInterp=Blue\n  Min=19.000 Max=187.000 \n  Minimum=19.000, Maximum=187.000, Mean=43.341, StdDev=20.330\n  NoData Value=0\n  Overviews: 1091x959, 546x480\n  Metadata:\n    STATISTICS_MAXIMUM=187\n    STATISTICS_MEAN=43.340507443056\n    STATISTICS_MINIMUM=19\n    STATISTICS_STDDEV=20.32987736339\n\n\nLes librairies de base comme rasterio et xarray produisent facilement un sommaire des statistiques de base avec la fonction stats:\n\nimport rasterio as rio\nimport numpy as np\nwith rio.open('landsat7.tif') as src:\n    stats= src.stats()\n    print(stats)\n\nLa librairie xarray donne accès à des fonctionnalités plus sophistiquées comme le calcul des quantiles:\n\nimport rioxarray as riox\nwith riox.open_rasterio('landsat7.tif', masked= True) as src:\n    print(src)\nquantiles = src.quantile(dim=['x','y'], q=[.025,.25,.5,.75,.975])\nquantiles\n\n&lt;xarray.DataArray (band: 3, y: 1917, x: 2181)&gt; Size: 50MB\n[12542931 values with dtype=float32]\nCoordinates:\n  * band         (band) int64 24B 1 2 3\n  * x            (x) float64 17kB -1.365e+07 -1.365e+07 ... -1.359e+07\n  * y            (y) float64 15kB 4.576e+06 4.576e+06 ... 4.519e+06 4.519e+06\n    spatial_ref  int64 8B 0\nAttributes:\n    AREA_OR_POINT:           Area\n    OVR_RESAMPLING_ALG:      NEAREST\n    TIFFTAG_RESOLUTIONUNIT:  1 (unitless)\n    TIFFTAG_XRESOLUTION:     1\n    TIFFTAG_YRESOLUTION:     1\n    STATISTICS_MAXIMUM:      233\n    STATISTICS_MEAN:         98.433096940153\n    STATISTICS_MINIMUM:      19\n    STATISTICS_STDDEV:       21.164021026458\n    scale_factor:            1.0\n    add_offset:              0.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray (quantile: 5, band: 3)&gt; Size: 120B\narray([[ 54.,  19.,  19.],\n       [ 85.,  38.,  27.],\n       [ 99.,  54.,  38.],\n       [111.,  69.,  57.],\n       [140., 102.,  89.]])\nCoordinates:\n  * band      (band) int64 24B 1 2 3\n  * quantile  (quantile) float64 40B 0.025 0.25 0.5 0.75 0.975xarray.DataArrayquantile: 5band: 354.0 19.0 19.0 85.0 38.0 27.0 ... 111.0 69.0 57.0 140.0 102.0 89.0array([[ 54.,  19.,  19.],\n       [ 85.,  38.,  27.],\n       [ 99.,  54.,  38.],\n       [111.,  69.,  57.],\n       [140., 102.,  89.]])Coordinates: (2)band(band)int641 2 3array([1, 2, 3])quantile(quantile)float640.025 0.25 0.5 0.75 0.975array([0.025, 0.25 , 0.5  , 0.75 , 0.975])Indexes: (2)bandPandasIndexPandasIndex(Index([1, 2, 3], dtype='int64', name='band'))quantilePandasIndexPandasIndex(Index([0.025, 0.25, 0.5, 0.75, 0.975], dtype='float64', name='quantile'))Attributes: (0)\n\n\n\n3.3.1.1 Calcul de l’histogramme\nLe calcul d’un histogramme pour une image (une bande) permet d’avoir une vue plus détaillée de la répartition des valeurs radiométriques. Le calcul d’un histogramme nécessite minimalement de faire le choix du nombre de barre ( bins ou de la largeur ). Un bin est un intervalle de valeurs pour lequel on peut calculer le nombre de valeurs observées dans l’image. La fonction de base pour ce type de calcul est la fonction numpy.histogram():\n\nimport numpy as np\narray = np.random.randint(0,10,100) # 100 valeurs aléatoires entre 0 et 10\nhist, bin_limites = np.histogram(array, density=True)\nprint('valeurs :',hist)\nprint(';imites :',bin_limites)\n\nvaleurs : [0.1        0.07777778 0.07777778 0.08888889 0.16666667 0.12222222\n 0.05555556 0.13333333 0.14444444 0.14444444]\n;imites : [0.  0.9 1.8 2.7 3.6 4.5 5.4 6.3 7.2 8.1 9. ]\n\n\nLe calcul se fait avec 10 intervalles par défaut.\n\nfig, ax= plt.subplots(figsize=(5, 4))\nplt.bar(bin_limites[:-1],hist)\nplt.show()\n\n\n\n\n\n\n\n\nPour des besoins de visualisation, le calcul des valeurs extrêmes de l’histogramme peut aussi se faire via les quantiles comme discutés auparavant.\n\n3.3.1.1.1 Visualisation des histogrammes\nLa librarie rasterio est probablement l’outil le plus simples pour visualiser rapidement des histogrammes sur une image multi-spectrale:\n\nimport rasterio as rio\nfrom rasterio.plot import show_hist\nwith rio.open('RGBNIR_of_S2A.tif') as src:\n  show_hist(src, bins=50, lw=0.0, stacked=False, alpha=0.3,histtype='stepfilled', title=\"Histogram\")\n\n\n\n\n\n\n\n\n\n\n\n\n3.3.2 Réhaussements linéaires\nLe réhaussement linéaire (linear stretch) d’une image est la forme la plus simple de réhaussement, elle consiste à 1) optimiser les valeurs des pixels d’une image afin de maximiser la dynamique disponibles à l’affichage, ou 2) à changer le format de stockage des valeurs (de 8 bits à 16 bits):\n\\[ \\text{nouvelle valeur d'un pixel} = \\frac{\\text{valeur d'un pixel} - min_0}{max_0 - min_0}\\times (max_1 - min_1)+min_1 \\tag{3.1}\\]\nPar cette opération, on passe de la dynamique de départ (\\(max_0 - min_0\\)) vers la dynamique cible (\\(max_1 - min_1\\)). Bien que cette opération semble triviale, il est important d’être conscient des trois contraintes suivantes:\n\nFaire attention à la dynamique cible, ainsi, pour sauvegarder une image en format 8 bit, on utilisera alors \\(max_1=255\\) et \\(min_1=0\\).\n\n2. Préservation de la valeur de no data : il faut faire attention à la valeur \\(min_1\\) dans le cas d’une valeur présente pour no_data. Par exemple, si no_data=0 alors il faut s’assurer que \\(min_1&gt;0\\).\n3. Précision du calcul : si possible réaliser la division ci-dessus en format float\n\n3.3.2.1 Cas des histogrammes asymétriques\nDans certains cas, la distribution de valeurs est très asymétrique et présente une longue queue avec des valeurs extrêmes élevées (à droite ou à gauche de l’histogramme). Le cas des images SAR est particulièrement représentatif de ce type de données. En effet, celles-ci peuvent présenter une distribution de valeurs de type exponentiel. Il est alors préférable d’utiliser des percentiles au préalable afin d’explorer la forme de l’histogramme et la distribution des valeurs:\n\nNO_DATA_FLOAT= -999.0\n# on prend tous les pixels de la première bande\nvalues = img_SAR[0].values.flatten().astype(float)\n# on exclut les valeurs invalides\nvalues = values[~np.isnan(values)]\n# on exclut le no data\nvalues = values[values!=NO_DATA_FLOAT]\n# calcul des percentiles\npercentiles_position= (0,0.1,1,2,50,98,99,99.9,100)\npercentiles= dict(zip(percentiles_position, np.percentile(values, percentiles_position)))\nprint(percentiles)\n\n{0: np.float64(8.172580237442162e-06), 0.1: np.float64(1.588739885482937e-05), 1: np.float64(8.657756850880105e-05), 2: np.float64(0.00018846066552214325), 50: np.float64(0.012372820172458887), 98: np.float64(0.1719470709562302), 99: np.float64(0.27963151514529694), 99.9: np.float64(1.5235805057287233), 100: np.float64(483.223876953125)}\n\n\nOn constate que la valeur médiane (0.012) est très faible, ce qui signifie que 50% des valeurs sont inférieures à cette valeur alors que la valeur maximale (483) est 10 000 fois plus élevée! Une manière de visualiser cette distribution de valeurs est d’utiliser boxplot et violinplot de la librairie matplotlib:\n\nfig, ax = plt.subplots(nrows=2, ncols=1, figsize=(6, 4), sharex=True)\nax[0].set_title('Distribution de la bande 0 de img_SAR', fontsize='small')\nax[0].grid(True)\nax[0].violinplot(values, orientation  ='horizontal', \n                 quantiles =(0.01,0.02,0.50,0.98,0.99),\n                  showmeans=False,\n                  showmedians=True)\nax[1].set_xlabel('Valeur des pixels')\nax[1].grid(True)\nbplot = ax[1].boxplot(values, notch = True, orientation  ='horizontal')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nAfin de visualiser correctement l’histogramme, il faut se limiter à un intervalle de valeurs plus réduit. Dans le code ci-dessous, on impose à la fonction np.histogramme de compter les valeurs de pixels dans des intervalles de valeurs fixés par la fonction np.linspace(percentiles[0.1],percentiles[99.9], 50) où percentiles[0.1] et percentiles[99.9] sont les \\(0.1\\%\\) et \\(99.9\\%\\) percentiles respectivement:\n\nhist, bin_edges = np.histogram(values, \n                                bins=np.linspace(percentiles[0.1], \n                                percentiles[99.9], 50), \n                                density=True)\n\nfig, ax = plt.subplots(nrows=2,ncols=1,figsize=(6, 5), sharex=True)\nax[0].bar(bin_edges[:-1], \n                hist*(bin_edges[1]-bin_edges[0]), \n                width= (bin_edges[1]-bin_edges[0]), \n                edgecolor= 'w')\nax[0].set_title(\"Distribution de probabilité (PDF)\")\nax[0].set_ylabel(\"Densité de probabilité\")\nax[0].grid(True)\n\nax[1].plot(bin_edges[:-1], \n            hist.cumsum()*(bin_edges[1]-bin_edges[0]))\nax[1].set_title(\"Distribution de probabilité cumulée (CDF)\")\nax[1].set_xlabel(\"Valeur du pixel\")\nax[1].set_ylabel(\"Probabilité cumulée\")\nax[1].grid(True)\nplt.tight_layout()\nplt.show()                              \n\n\n\n\n\n\n\n\nAu niveau de l’affichage avec matplotlib, la dynamique peut être contrôlée directement avec les paramètres vmin et vmax comme ceci:\n\nfig, ax = plt.subplots(nrows=2, ncols=2, figsize=(6, 5), sharex=True, sharey=True)\n[a.axis('off') for a in ax.flatten()]\nax[0,0].imshow(img_SAR[0].values, vmin=percentiles[0], vmax=percentiles[100])\nax[0,0].set_title(f\"0% - 100%={percentiles[0]:2.1f} - {percentiles[100]:2.1f}\")\nax[0,1].imshow(img_SAR[0].values, vmin=percentiles[0.1], vmax=percentiles[99.9]) \nax[0,1].set_title(f\"0.1% - 99.9%={percentiles[0.1]:2.1f} - {percentiles[99.9]:2.1f}\")\nax[1,0].imshow(img_SAR[0].values, vmin=percentiles[1], vmax=percentiles[99]) \nax[1,0].set_title(f\"1% - 99%={percentiles[1]:2.1f} - {percentiles[99]:2.1f}\")\nax[1,1].imshow(img_SAR[0].values, vmin=percentiles[2], vmax=percentiles[98]) \nax[1,1].set_title(f\"2% - 98%={percentiles[2]:2.1f} - {percentiles[98]:2.1f}\")\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\n3.3.3 Réhaussements non linéaires\n\n3.3.3.1 Réhaussement par fonctions\nLe réhaussenent par fonction consiste à appliquer une fonction non linéaire afin de modifier la dynamique de l’image. Par exemple, pour une image radar, une transformation populaire est d’afficher les valeurs de rétrodiffusion en décibel (dB) avec la fonction log10().\n\npercentiles_position= (0,0.1,1,2,50,98,99,99.9,100)\nvalues= xr.apply_ufunc(lambda x: 10 * np.log10(x), img_SAR[0]).data\npercentiles_db= dict(zip(percentiles_position, np.percentile(values, percentiles_position)))\nprint(percentiles_db)\n\n{0: np.float64(-50.87641143798828), 0.1: np.float64(-47.98947440338135), 1: np.float64(-40.625951309204105), 2: np.float64(-37.24779266357422), 50: np.float64(-19.075313568115234), 98: np.float64(-7.6460519409179675), 99: np.float64(-5.534139237403945), 99.9: np.float64(1.8286541925668764), 100: np.float64(26.84148406982422)}\n\n\nLes boites à moustache (boxplots) ont une bien meilleure distribution qui est en effet très proche d’une distribution normale gaussienne:\n\n\n\n\n\n\n\n\n\nOn obtient ainsi les images suivantes:\n\n\n\n\n\n\n\n\n\n\n\n\n3.3.3.2 Égalisation d’histogramme\nL’égalisation d’histogramme consiste à modifier les valeurs des pixels d’une image source afin que la distribution cumulée des valeurs (CDF) devienne similaire à celle d’une image cible. La CDF (Cumulative Distribution Function) est simplement la somme cumulée des valeurs de l’histogramme:\n\\[\nCDF_{source}(i)= \\frac{1}{K}\\sum_{j=0}^{j \\leq i} hist_{source}(j)\n\\] avec \\(K\\) choisit de façon à ce que la dernière valeur soit égale à 1 (\\(CDF_{source}(i_{max})=1\\)). De la même manière, \\(CDF_{cible}\\) est la CDF d’une image cible. La formule générale pour l’égalisation d’histogramme est la suivante: \\[\nj = CDF_{cible}^{-1}(CDF_{source}(i))\n\\]\nOn peut choisir \\(CDF_{cible}\\) comme correspondant à une image où chaque valeur de pixel est équiprobable (d’où le terme égalisation), ce qui veut dire \\(hist_{cible}(j)=1/L\\) avec \\(L\\) égale au nombre de valeurs possibles dans l’image (par exemple \\(L=256\\)). \\[\nj = L \\times CDF_{source}(i)\n\\] On peut appliquer cette procédure sur l’image SAR en dB de la façon suivante:\n\nvalues= np.sort(10*np.log10(img_SAR[0].data.flatten()))\ncdf_x= np.linspace(values[0], values[-1], 1000)\ncdf_source= np.interp(cdf_x, values, np.arange(len(values))/len(values)*255)\nvalues_eq=np.interp(np.log10(img_SAR[0].data), cdf_x, cdf_source).astype('uint8')\nplt.imshow(values_eq)\nplt.axis('off')\n\n\n\n\n\n\n\n\n\n\n\n3.3.3.3 Palettes de couleur\nLes palettes de couleurs sont appliquées dynamiquement à l’affichage sur une image à une seule bande. La librairie matplotlib contient un nombre considérable de palettes.\n\nfrom matplotlib import colormaps\nlist(colormaps)\n\nVoici quelques exemples ci-dessous, les valeurs de l’image doivent être normalisées entre 0 et 1 ou entre 0 et 255 sinon les paramètres vmin et vmax doivent être spécifiés. On peut observer comment ces palettes révèlent les détails de l’image malgré une image originalement très sombre.\n\nfig, ax = plt.subplots(nrows=2, ncols=2, figsize=(6, 5), sharex=True, sharey=True)\n[a.axis('off') for a in ax.flatten()]\nax[0,0].imshow(img_SAR[0].data, vmin=percentiles[2], vmax=percentiles[98], cmap='jet')\nax[0,0].set_title(f\"jet\")\nax[0,1].imshow(img_SAR[0].data, vmin=percentiles[2], vmax=percentiles[98], cmap='hot')\nax[0,1].set_title(f\"hot\")\nax[1,0].imshow(img_SAR[0].data, vmin=percentiles[2], vmax=percentiles[98], cmap='hsv')\nax[1,0].set_title(f\"hsv\")\nax[1,1].imshow(img_SAR[0].data, vmin=percentiles[2], vmax=percentiles[98], cmap='terrain')\nax[1,1].set_title(f\"terrain\")\nplt.tight_layout()\n\n\n\n\n\n\n\n\nIl peut être utile d’ajouter une barre de couleurs afin d’indiquer la correspondance entre les couleurs et les valeurs numériques:\n\nimport matplotlib as mpl\nfig, ax = plt.subplots(figsize=(6, 6))\ncmap= mpl.colormaps.get_cmap('jet').with_extremes(under='white', over='magenta')\nh=plt.imshow(img_SAR[0].data, norm=mpl.colors.LogNorm(vmin=percentiles[2], vmax=percentiles[98]),\n                   cmap=cmap)\nfig.colorbar(h, ax=ax,  orientation='horizontal', label=\"Intensité\", extend='both')\nax.axis('off') \n\n\n\n\n\n\n\n\n\n\n\n3.3.4 Composés colorés\nLe système visuel humain est sensible seulement à la partie visible du spectre électromagnétique qui compose les couleurs de l’arc-en-ciel du bleu au rouge. L’ensemble des couleurs du spectre visible peut être obtenu à partir du mélange de trois couleurs primaires (rouge, vert et bleu). Ce système de décomposition à trois couleurs est à la base de la plupart des systèmes de visualisation ou de représentation de l’information de couleur. Si on prend le cas des images Sentinel-2, 12 bandes sont disponibles, plusieurs composés couleurs sont donc possibles (voir le site de Copernicus). Voici quelques exemples possibles, chaque composé mettant en valeur des propriétés différentes de la surface.\n\nimport rioxarray as rxr\nfig, ax= plt.subplots(nrows=2, ncols= 2, figsize=(8, 6), sharex=True, sharey=True)\nimg_s2.sel(band=[4,3,2]).plot.imshow(vmin=86, vmax=4000, ax=ax[0,0])\nax[0,0].set_title('RVB')\nimg_s2.sel(band=[8,3,2]).plot.imshow(vmin=86, vmax=4000, ax=ax[0,1])\nax[0,1].set_title('NIR,V,B')\nimg_s2.sel(band=[12,8,4]).plot.imshow(vmin=86, vmax=4000, ax=ax[1,0])\nax[1,0].set_title('SWIR2,NIR,R')\nimg_s2.sel(band=[12,11,4]).plot.imshow(vmin=86, vmax=4000, ax=ax[1,1])\nax[1,1].set_title('SWIR2,SWIR1,NIR')\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Réhaussement et visualisation d'images</span>"
    ]
  },
  {
    "objectID": "03-TransformationSpectrales.html",
    "href": "03-TransformationSpectrales.html",
    "title": "4  Transformations spectrales",
    "section": "",
    "text": "4.1 Préambule\nAssurez-vous de lire ce préambule avant d’exécuter le reste du notebook.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Transformations spectrales</span>"
    ]
  },
  {
    "objectID": "03-TransformationSpectrales.html#préambule",
    "href": "03-TransformationSpectrales.html#préambule",
    "title": "4  Transformations spectrales",
    "section": "",
    "text": "4.1.1 Objectifs\nDans ce chapitre, nous abordons l’exploitation de la dimension spectrale des images satellites. Ce chapitre est aussi disponible sous la forme d’un notebook Python:\n\n\n\n\n\n\nObjectifs d’apprentissage visés dans ce chapitre\n\nÀ la fin de ce chapitre, vous devriez être en mesure de :\n\ncomprendre le principe des indices spectraux;\ncalculer différents indices avec spyndex;\nanalyser le gain en information des indices;\n\n\n\n\n4.1.2 Librairies\nLes librairies qui vont être explorées dans ce chapitre sont les suivantes:\n\nSciPy\nNumPy\nspyindex\nRasterio\nXarray\nrioxarray\n\nDans l’environnement Google Colab, seul rioxarray doit être installés\n\n%%capture\n!pip install -qU matplotlib rioxarray xrscipy scikit-image pyarrow spyndex\n\nVérifiez les importations:\n\nimport numpy as np\nimport rioxarray as rxr\nfrom scipy import signal\nimport xarray as xr\nimport xrscipy\nimport matplotlib.pyplot as plt\nimport spyndex\nimport rasterio as rio\n\n\n\n4.1.3 Images utilisées\nNous utilisons les images suivantes dans ce chapitre:\n\n%%capture\nimport gdown\n\ngdown.download('https://drive.google.com/uc?export=download&confirm=pbef&id=1a6Ypg0g1Oy4AJt9XWKWfnR12NW1XhNg_', output= 'RGBNIR_of_S2A.tif')\ngdown.download('https://drive.google.com/uc?export=download&confirm=pbef&id=1a6O3L_abOfU7h94K22At8qtBuLMGErwo', output= 'sentinel2.tif')\ngdown.download('https://drive.google.com/uc?export=download&confirm=pbef&id=1_zwCLN-x7XJcNHJCH6Z8upEdUXtVtvs1', output= 'berkeley.jpg')\ngdown.download('https://drive.google.com/uc?export=download&confirm=pbef&id=1dM6IVqjba6GHwTLmI7CpX8GP2z5txUq6', output= 'SAR.tif')\ngdown.download('https://drive.google.com/uc?export=download&confirm=pbef&id=1aAq7crc_LoaLC3kG3HkQ6Fv5JfG0mswg', output= 'carte.tif')\ngdown.download('https://drive.google.com/uc?export=download&confirm=pbef&id=1iCZNYTv0qEZRzPhe22nPdpV4Ks7NsY3b', output= 'ASCIIdata_splib07b_rsSentinel2.zip')\n!unzip -q ASCIIdata_splib07b_rsSentinel2.zip\n\nVérifiez que vous êtes capable de les lire :\n\nwith rxr.open_rasterio('berkeley.jpg', mask_and_scale= True) as img_rgb:\n    print(img_rgb)\nwith rxr.open_rasterio('RGBNIR_of_S2A.tif', mask_and_scale= True) as img_rgbnir:\n    print(img_rgbnir)\nwith rxr.open_rasterio('sentinel2.tif', mask_and_scale= True) as img_s2:\n    print(img_s2)\nwith rxr.open_rasterio('carte.tif', mask_and_scale= True) as img_carte:\n    print(img_carte)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Transformations spectrales</span>"
    ]
  },
  {
    "objectID": "03-TransformationSpectrales.html#quest-ce-que-linformation-spectrale",
    "href": "03-TransformationSpectrales.html#quest-ce-que-linformation-spectrale",
    "title": "4  Transformations spectrales",
    "section": "4.2 Qu’est ce que l’information spectrale?",
    "text": "4.2 Qu’est ce que l’information spectrale?\nL’information spectrale touche à l’exploitation de la dimension spectrale des images (c.à.d le long des bandes spectrales de l’image). La taille de cette dimension spectrale dépend du type de capteurs considéré. Un capteur à très haute résolution spectrale par exemple aura très peu de bandes (4 ou 5). Un capteur multispectral pourra contenir une quinzaine de bande. À l’autre extrême, on trouvera les capteurs hyperspectraux qui peuvent contenir des centaines de bandes spectrales.\n\n\n\nPositions des bandes spectrales pour quelques capteurs (source)\n\n\nPour une surface donnée, la forme des valeurs le long de l’axe spectrale caractérise le type de matériau observé ainsi que son état. On parle souvent alors de signature spectrale. On peut voir celle-ci comme une généralisation de la couleur d’un matériau au delà des bandes visibles du spectre. L’exploitation de ces signatures spectrales est probablement un des principes les plus importants en télédétection qui le distingue de la vison par ordinateur. L’USGS maintient une base de données spectrales acquises en laboratoire (Kokaly et Klein 2017). On peut observer sur la figure ci-dessous comment la forme et l’amplitude de trois signatures différentes peut changer en fonction du type de surface.\n\n\nText(0, 0.5, 'Réflectance')\nExemples de signatures spectrales pour trois surfaces différentes pour les bandes spectrales de Sentinel-2",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Transformations spectrales</span>"
    ]
  },
  {
    "objectID": "03-TransformationSpectrales.html#indices-spectraux",
    "href": "03-TransformationSpectrales.html#indices-spectraux",
    "title": "4  Transformations spectrales",
    "section": "4.3 Indices spectraux",
    "text": "4.3 Indices spectraux\nIl existe une vaste littérature sur les indices spectraux, le choix d’un indice plutôt qu’un autre dépend fortement de l’application visée, nous allons simplement couvrir les principes de base ici. Le principe d’un indice spectral consiste à mettre en valeur certaines caractéristiques saillantes du spectre comme des pentes, des gradients, etc.\nLa librairie Python Awesome Spectral Indices maintient une liste de plus de 200 indices spectraux (radar et optiques). La liste complète est affichable avec la commande suivante:\n\nspyndex.indices\n\nSpectralIndices(['AFRI1600', 'AFRI2100', 'ANDWI', 'ARI', 'ARI2', 'ARVI', 'ATSAVI', 'AVI', 'AWEInsh', 'AWEIsh', 'BAI', 'BAIM', 'BAIS2', 'BCC', 'BI', 'BITM', 'BIXS', 'BLFEI', 'BNDVI', 'BRBA', 'BWDRVI', 'BaI', 'CCI', 'CIG', 'CIRE', 'CRI550', 'CRI700', 'CSI', 'CSIT', 'CVI', 'DBI', 'DBSI', 'DPDD', 'DSI', 'DSWI1', 'DSWI2', 'DSWI3', 'DSWI4', 'DSWI5', 'DVI', 'DVIplus', 'DpRVIHH', 'DpRVIVV', 'EBBI', 'EBI', 'EMBI', 'ENDVI', 'EVI', 'EVI2', 'EVIv', 'ExG', 'ExGR', 'ExR', 'FAI', 'FCVI', 'GARI', 'GBNDVI', 'GCC', 'GDVI', 'GEMI', 'GLI', 'GM1', 'GM2', 'GNDVI', 'GOSAVI', 'GRNDVI', 'GRVI', 'GSAVI', 'GVMI', 'IAVI', 'IBI', 'IKAW', 'IPVI', 'IRECI', 'LSWI', 'MBI', 'MBWI', 'MCARI', 'MCARI1', 'MCARI2', 'MCARI705', 'MCARIOSAVI', 'MCARIOSAVI705', 'MGRVI', 'MIRBI', 'MLSWI26', 'MLSWI27', 'MNDVI', 'MNDWI', 'MNLI', 'MRBVI', 'MSAVI', 'MSI', 'MSR', 'MSR705', 'MTCI', 'MTVI1', 'MTVI2', 'MuWIR', 'NBAI', 'NBLI', 'NBLIOLI', 'NBR', 'NBR2', 'NBRSWIR', 'NBRT1', 'NBRT2', 'NBRT3', 'NBRplus', 'NBSIMS', 'NBUI', 'ND705', 'NDBI', 'NDBaI', 'NDCI', 'NDDI', 'NDGI', 'NDGlaI', 'NDII', 'NDISIb', 'NDISIg', 'NDISImndwi', 'NDISIndwi', 'NDISIr', 'NDMI', 'NDPI', 'NDPolI', 'NDPonI', 'NDREI', 'NDSI', 'NDSII', 'NDSIWV', 'NDSInw', 'NDSWIR', 'NDSaII', 'NDSoI', 'NDTI', 'NDVI', 'NDVI705', 'NDVIMNDWI', 'NDVIT', 'NDWI', 'NDWIns', 'NDYI', 'NGRDI', 'NHFD', 'NIRv', 'NIRvH2', 'NIRvP', 'NLI', 'NMDI', 'NRFIg', 'NRFIr', 'NSDS', 'NSDSI1', 'NSDSI2', 'NSDSI3', 'NSTv1', 'NSTv2', 'NWI', 'NormG', 'NormNIR', 'NormR', 'OCVI', 'OSAVI', 'OSI', 'PI', 'PISI', 'PSRI', 'QpRVI', 'RCC', 'RDVI', 'REDSI', 'RENDVI', 'RFDI', 'RGBVI', 'RGRI', 'RI', 'RI4XS', 'RNDVI', 'RVI', 'S2REP', 'S2WI', 'S3', 'SARVI', 'SAVI', 'SAVI2', 'SAVIT', 'SEVI', 'SI', 'SIPI', 'SLAVI', 'SR', 'SR2', 'SR3', 'SR555', 'SR705', 'SWI', 'SWM', 'SeLI', 'TCARI', 'TCARIOSAVI', 'TCARIOSAVI705', 'TCI', 'TDVI', 'TGI', 'TRRVI', 'TSAVI', 'TTVI', 'TVI', 'TWI', 'TriVI', 'UI', 'VARI', 'VARI700', 'VDDPI', 'VHVVD', 'VHVVP', 'VHVVR', 'VI6T', 'VI700', 'VIBI', 'VIG', 'VVVHD', 'VVVHR', 'VVVHS', 'VgNIRBI', 'VrNIRBI', 'WDRVI', 'WDVI', 'WI1', 'WI2', 'WI2015', 'WRI', 'bNIRv', 'kEVI', 'kIPVI', 'kNDVI', 'kRVI', 'kVARI', 'mND705', 'mSR705', 'sNIRvLSWI', 'sNIRvNDPI', 'sNIRvNDVILSWIP', 'sNIRvNDVILSWIS', 'sNIRvSWIR'])\n\n\nLe détail d’un indice particulier, par exemple le `NDVI`, est aussi affichable:\n\nspyndex.indices[\"NDVI\"]\n\nSpectralIndex(NDVI: Normalized Difference Vegetation Index)\n        * Application Domain: vegetation\n        * Bands/Parameters: ['N', 'R']\n        * Formula: (N-R)/(N+R)\n        * Reference: https://ntrs.nasa.gov/citations/19740022614\n        \n\n\nspyndex pré-suppose une nomenclature prédéfinie des bandes, on peut voir la correspondance sur le tableau ci-dessous:\n\nspyndex.bands\n\nBands(['A', 'B', 'G', 'G1', 'N', 'N2', 'R', 'RE1', 'RE2', 'RE3', 'S1', 'S2', 'T', 'T1', 'T2', 'WV', 'Y'])\n\n\n\nNoms des bandes Sentinel-2\n\n\nIndex\nNoms\nSpyndex\nNoms\n\n\n\n\n1\nB01\nA\nAérosol\n\n\n2\nB02\nB\nBleu\n\n\n3\nB03\nG\nVert\n\n\n4\nB04\nR\nRouge\n\n\n5\nB05\nRE1\nRed edge 1\n\n\n6\nB06\nRE1\nRed edge 2\n\n\n7\nB07\nRE2\nRed edge 3\n\n\n8\nB08\nN\nProche-infrarouge 1\n\n\n9\nB08A\nN2\nProche-infrarouge 2\n\n\n10\nB09\n-\nVapeur d’eau\n\n\n11\nB11\nS1\nInfra-rouge onde courte 1\n\n\n12\nB12\nS2\nInfra-rouge onde courte 1\n\n\n\nDeux options sont possibles, on peut soit renommer les noms des bandes avec xarray ou “mapper” les noms vers les noms appropriés. Regardons les dimensions de notre jeux de données:\n\nimg_s2.dims\n\n('band', 'y', 'x')\n\n\nOn peut simplement changer les index (coords) de la dimension band:\n\nimg_s2 = img_s2.sel(band = list(range(1,13))).assign_coords({'band':[\"A\", \"B\", \"G\", \"R\", \"RE1\", \"RE2\", \"RE3\", \"N\", \"N2\", \"WV\", \"S1\", \"S2\"]})\nimg_s2=img_s2/10000 # normalisation en réflectance\n\n\nfrom rasterio import plot\nidx = spyndex.computeIndex(\n    index = [\"NDVI\",\"GNDVI\",\"SAVI\"],\n    params = {\n        \"N\": img_s2.sel(band = \"N\"),\n        \"R\": img_s2.sel(band = \"R\"),\n        \"G\": img_s2.sel(band = \"G\"),\n        \"L\": 0.5\n    }\n)\n\n# Plot the indices (and the RGB image for comparison)\nfig, ax = plt.subplots(2,2,figsize = (9,9))\n[a.axis('off') for a in ax.flatten()]\nplot.show(img_s2.sel(band = [\"R\",\"G\",\"B\"]).data / 0.3,ax = ax[0,0],title = \"RGB\")\nplot.show(idx.sel(index = \"NDVI\"),ax = ax[0,1],title = \"NDVI\")\nplot.show(idx.sel(index = \"GNDVI\"),ax = ax[1,0],title = \"GNDVI\")\nplot.show(idx.sel(index = \"SAVI\"),ax = ax[1,1],title = \"SAVI\")\n\n\n\n\n\n\n\n\nOn peut vérifier l’utilité des indices en vérifiant leur séparabilité pour certaines classes d’intérêts. Nous reprenons ici l’exemple de la section Section 6.2.3 pour vérifier l’utilité des indices NDVI, NDWI et NDBI:\n\nimport pandas as pd\nimport seaborn as sns\n\n# On sélectionne trois classes\nclass_selected= [1,3,9]\ndf= pd.concat([gdf[gdf['class'] ==c] for c in class_selected], ignore_index=True)\nidx[\"Land Cover\"] = [nom_classes[l] for l in df[\"class\"].tolist()]\n# Compute the desired spectral indices\nidx = spyndex.computeIndex(\n    index = [\"NDVI\",\"NDWI\",\"NDBI\"],\n    params = {\n        \"N\": df[\"SR_B8\"],\n        \"R\": df[\"SR_B4\"],\n        \"G\": df[\"SR_B3\"],\n        \"S1\": df[\"SR_B11\"]\n    }\n)\n\ncolors= [couleurs_classes[c] for c in class_selected]\n# Plot a pairplot to check the indices behaviour\nplt.figure(figsize = (15,15))\ng = sns.PairGrid(idx,hue = \"Land Cover\",palette = sns.color_palette(colors))\ng.map_lower(sns.scatterplot)\ng.map_upper(sns.kdeplot,fill = True,alpha = .5)\ng.map_diag(sns.kdeplot,fill = True)\ng.add_legend()\nplt.show()\n\n\n\n\nVisualisation des points d’une image Sentinel-2 pour trois classes",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Transformations spectrales</span>"
    ]
  },
  {
    "objectID": "03-TransformationSpectrales.html#quiz",
    "href": "03-TransformationSpectrales.html#quiz",
    "title": "4  Transformations spectrales",
    "section": "4.4 Quiz",
    "text": "4.4 Quiz\n\n\n\n\nKokaly, Clark, R. F. et A. J. Klein. 2017. « USGS Spectral Library Version 7 Data: U.S. Geological Survey data release. » Applied Sciences. 10.5066/F7RR1WDJ.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Transformations spectrales</span>"
    ]
  },
  {
    "objectID": "04-TransformationSpatiales.html",
    "href": "04-TransformationSpatiales.html",
    "title": "5  Transformations spatiales",
    "section": "",
    "text": "5.1 Préambule\nAssurez-vous de lire ce préambule avant d’exécuter le reste du notebook.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Transformations spatiales</span>"
    ]
  },
  {
    "objectID": "04-TransformationSpatiales.html#préambule",
    "href": "04-TransformationSpatiales.html#préambule",
    "title": "5  Transformations spatiales",
    "section": "",
    "text": "5.1.1 Objectifs\nDans ce chapitre, nous abordons quelques techniques de traitement d’images dans le domaine spatial uniquement. Ce chapitre est aussi disponible sous la forme d’un notebook Python sur Google Colab:\n\n\n\n\n\n\nObjectifs d’apprentissage visés dans ce chapitre\n\nÀ la fin de ce chapitre, vous devriez être en mesure de :\n\ncomprendre le principe de la décomposition de Fourier;\ncomprendre le principe de la convolution;\nappliquer un filtrage local à l’aide d’une fenêtre;\nsegmenter une image en super-pixels et calculer leurs propriétés\n\n\n\n\n5.1.2 Librairies\nLes librairies utilisées dans ce chapitre sont les suivantes:\n\nSciPy\nNumPy\nopencv-python · PyPI\nscikit-image\nRasterio\nXarray\nrioxarray\n\nDans l’environnement Google Colab, seul rioxarray doit être installé:\n\n%%capture\n!pip install -qU matplotlib rioxarray xrscipy scikit-image\n\nVérifiez les importations:\n\nimport numpy as np\nimport numpy.fft\nimport rioxarray as rxr\nfrom scipy import signal, ndimage\nimport xarray as xr\nimport xrscipy\nimport matplotlib.pyplot as plt\nfrom skimage import data, measure, graph, segmentation, color\nfrom skimage.color import rgb2gray\nfrom skimage.segmentation import slic, mark_boundaries\nimport pandas as pd\n\n\n\n5.1.3 Images utilisées\nNous utilisons les images suivantes dans ce chapitre:\n\n%%capture\nimport gdown\n\ngdown.download('https://drive.google.com/uc?export=download&confirm=pbef&id=1a6Ypg0g1Oy4AJt9XWKWfnR12NW1XhNg_', output= 'RGBNIR_of_S2A.tif')\ngdown.download('https://drive.google.com/uc?export=download&confirm=pbef&id=1a4PQ68Ru8zBphbQ22j0sgJ4D2quw-Wo6', output= 'landsat7.tif')\ngdown.download('https://drive.google.com/uc?export=download&confirm=pbef&id=1_zwCLN-x7XJcNHJCH6Z8upEdUXtVtvs1', output= 'berkeley.jpg')\ngdown.download('https://drive.google.com/uc?export=download&confirm=pbef&id=1dM6IVqjba6GHwTLmI7CpX8GP2z5txUq6', output= 'SAR.tif')\n\nVérifiez que vous êtes capable de les lire :\n\nwith rxr.open_rasterio('berkeley.jpg', mask_and_scale= True) as img_rgb:\n    print(img_rgb)\nwith rxr.open_rasterio('RGBNIR_of_S2A.tif', mask_and_scale= True) as img_rgbnir:\n    print(img_rgbnir)\nwith rxr.open_rasterio('SAR.tif', mask_and_scale= True) as img_SAR:\n    print(img_SAR)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Transformations spatiales</span>"
    ]
  },
  {
    "objectID": "04-TransformationSpatiales.html#analyse-fréquentielle",
    "href": "04-TransformationSpatiales.html#analyse-fréquentielle",
    "title": "5  Transformations spatiales",
    "section": "5.2 Analyse fréquentielle",
    "text": "5.2 Analyse fréquentielle\nL’analyse fréquentielle, issue du traitement du signal, permet d’avoir un autre point de vue sur les données à partir de ses composantes harmoniques. La modification de ces composantes de Fourier modifie l’ensemble de l’image et permet de corriger des problèmes systématiques comme des artefacts ou du bruit de capteur. Bien que ce domaine soit un peu éloigné de la télédétection, les images issues des capteurs sont toutes sujettes à des étapes de traitement du signal et il faut donc en connaître les grands principes afin de pouvoir comprendre certains enjeux lors des traitements.\n\n5.2.1 La transformée de Fourier\nLa transformée de Fourier permet de transformer une image dans un espace fréquentielle. Cette transformée est complètement réversible. Dans le cas des images numériques, on parle de 2D-DFT (2D-Discrete Fourier Transform) qui est un algorithme optimisé pour le calcul fréquentiel (Cooley et Tukey 1965). La 1D-DFT peu s’écrire simplement comme une projection sur une série d’exponentielles complexes:\n\\[X[k] = \\sum_{n=0 \\ldots N-1} x[n] \\times \\exp(-j \\times 2\\pi \\times k \\times n/N)) \\tag{5.1}\\]\nLa transformée inverse prend une forme similaire:\n\\[x[k] = \\frac{1}{N}\\sum_{n=0 \\ldots N-1} X[n] \\times \\exp(j \\times 2\\pi \\times k \\times n/N)) \\tag{5.2}\\]\nLe signal d’origine est donc reconstruit à partir d’une somme de sinusoïdes complexes \\(\\exp(j2\\pi \\frac{k}{N}n))\\) de fréquence \\(k/N\\). Noter qu’à partir de \\(k=N/2\\), les sinusoïdes se répètent à un signe près et forme un miroir des composantes, la convention est alors de mettre ces composantes dans une espace négatif \\([-N/2,\\ldots,-1]\\).\nDans le cas d’un simple signal périodique à une dimension avec une fréquence de 4/16 (donc 4 périodes sur 16) on obtient deux pics de fréquence à la position de 4 cycles observés sur \\(N=16\\) observations. Les puissances de Fourier sont affichées dans un espace fréquentiel en cycles par unité d’espacement de l’échantillon (avec zéro au début) variant entre -1 et +1. Par exemple, si l’espacement des échantillons est en secondes, l’unité de fréquence est cycles/seconde (ou Hz). Dans le cas de N échantillons, le pic sera observé à la fréquence \\(+/- 4/16=0.25\\) cycles/secondes. La fréquence d’échantillonnage \\(F_s\\) du signal a aussi beaucoup d’importance aussi et doit être au moins a deux fois la plus haute fréquence observée (ici \\(F_s &gt; 0.5\\)) sinon un phénomène de repliement appelé aliasing sera observé.\n\nimport math\nFs= 2.0\nTs= 1/Fs\nN= 16\narr = xr.DataArray(np.sin(2*math.pi*np.arange(0,N,Ts)*4/16),\n                   dims=('x'), coords={'x': np.arange(0,N,Ts)})\nfourier = np.fft.fft(arr)\nfreq = np.fft.fftfreq(fourier.size, d=Ts)\nfourier = xr.DataArray(fourier,\n                   dims=('f'), coords={'f': freq})\n\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\nplt.subplot(1, 2, 1)\narr.plot.line(color='red', linestyle='dashed', marker='o', markerfacecolor='blue')\naxes[0].set_title(\"Signal périodique\")\nplt.subplot(1, 2, 2)\nnp.abs(fourier).plot.line(color='red', linestyle='dashed', marker='o', markerfacecolor='blue')\naxes[1].set_title(\"Composantes de Fourier (amplitude)\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n5.2.2 Filtrage fréquentiel\nUn filtrage fréquentiel consiste à modifier le spectre de Fourier afin d’éliminer ou de réduire certaines composantes fréquentielles. On distingue habituellement trois catégories de filtres fréquentiels:\n\nLes filtres passe-bas qui ne préservent que les basses fréquences pour, par exemple, lisser une image.\nLes filtres passe-hauts qui ne préservent que les hautes fréquences pour ne préserver que les détails.\nLes filtres passe-bandes qui vont préserver les fréquences dans une bande de fréquence particulière.\n\nLa librairie Scipy contient différents filtres fréquentiels. Notez, qu’un filtrage fréquentielle est une simple multiplication de la réponse du filtre \\(F[k]\\) par les composantes fréquentielles du signal à filtrer \\(X[k]\\):\n\\[\nX_f[k] = F[k] \\times X[k]\n\\tag{5.3}\\]\nÀ noter que cette multiplication dans l’espace de Fourier est équivalente à une opération de convolution dans l’espace originale du signal \\(x\\):\n\\[\nx_f = IDFT^{-1}[F]*x\n\\tag{5.4}\\]\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\ninput_ = numpy.fft.fft2(img_rgb.to_numpy()) \nresult = [ndimage.fourier_gaussian(input_[b], sigma=4) for b in range(3)] # on filtre chaque bande avec un filtre Gaussien\nresult = numpy.fft.ifft2(result)\nax1.imshow(img_rgb.to_numpy().transpose(1, 2, 0).astype('uint8'))\nax1.set_title('Originale')\nax2.imshow(result.real.transpose(1, 2, 0).astype('uint8'))  # La partie imaginaire n'est pas utile ici\nax2.set_title('Filtrage Gaussien')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n5.2.3 L’aliasing\nL’aliasing est un problème fréquent en traitement du signal. Il résulte d’une fréquence d’échantillonnage trop faible par rapport au contenu fréquentielle du signal. Cela peut se produire lorsque vous sous-échantillonner fortement une image avec un facteur de décimation (par exemple un pixel sur deux). En prenant un pixel sur deux, on réduit la fréquence d’échantillonnage d’un facteur 2 ce qui réduit le contenu fréquentiel de l’image et donc les fréquences maximales de l’image. L’image présente alors un aspect faussement texturée avec beaucoup de hautes fréquences:\n\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\nplt.subplot(1, 2, 1)\nimg_rgb.astype('int').plot.imshow(rgb=\"band\")\naxes[0].set_title(\"Originale\")\nplt.subplot(1, 2, 2)\nimg_rgb[:,::4,::4].astype('int').plot.imshow(rgb=\"band\")\naxes[1].set_title(\"Décimée par un facteur 4\")\nplt.show()\n\n\n\n\n\n\n\n\nUne façon de réduire le contenu fréquentiel est de filtrer par un filtre passe-bas pour réduire les hautes fréquences par exemple avec un filtre Gaussien:\n\nfrom scipy.ndimage import gaussian_filter\nq= 4\nsigma= q*1.1774/math.pi\narr = xr.DataArray(gaussian_filter(img_rgb.to_numpy(), sigma= (0,sigma,sigma)), dims=('band',\"y\", \"x\"), coords= {'x': img_rgb.coords['x'], 'y': img_rgb.coords['y'], 'spatial_ref': 0})\n\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\nplt.subplot(1, 2, 1)\nimg_rgb.astype('int').plot.imshow(rgb=\"band\")\naxes[0].set_title(\"Originale\")\nplt.subplot(1, 2, 2)\narr[:,::q,::q].astype('int').plot.imshow(rgb=\"band\")\naxes[1].set_title(\"Décimée par un facteur 4\")\nplt.show()\n\n\n\n\n\n\n\n\nLa fonction decimate dans scipy.signal réalise l’opération de décimation (downsampling) en une seule étape:\n\nimport xrscipy.signal as dsp\n\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\nplt.subplot(1, 2, 1)\nimg_rgb.astype('int').plot.imshow(rgb=\"band\")\naxes[0].set_title(\"Originale\")\nplt.subplot(1, 2, 2)\ndsp.decimate(img_rgb, q=4, dim='x').astype('int').plot.imshow(rgb=\"band\")\naxes[1].set_title(\"Décimée par un facteur 4\")\n\nText(0.5, 1.0, 'Décimée par un facteur 4')",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Transformations spatiales</span>"
    ]
  },
  {
    "objectID": "04-TransformationSpatiales.html#filtrage-dimage",
    "href": "04-TransformationSpatiales.html#filtrage-dimage",
    "title": "5  Transformations spatiales",
    "section": "5.3 Filtrage d’image",
    "text": "5.3 Filtrage d’image\nLe filtrage d’image a plusieurs objectifs en télédétection:\n\nLa réduction du bruit afin d’améliorer la résolution radiométrique et améliorer la lisibilité de l’image.\nLe réhaussement de l’image afin d’améliorer le contraste ou faire ressortir les contours.\nLa production de nouvelles caractéristiques, c.-à.-d dérivées de nouvelles images mettant en valeur certaines informations dans l’image comme la texture, les contours, etc.\n\nIl existe de nombreuses méthodes de filtrage dans la littérature qui sont habituellement regroupées en quatre catégories:\n\nLe filtrage peut-être global ou local, c.-à.-d qu’il prend en compte soit toute l’image pour filtrer (ex: filtrage par Fourier), soit uniquement avec une fenêtre ou un voisinage local.\nLa fonction de filtrage peut-être linéaire ou non linéaire.\nLa fonction de filtrage peut être stationnaire ou adaptative.\nLe filtrage peut-être mono-échelle ou multi-échelle.\n\nLa librairie Scipy (Multidimensional image processing (scipy.ndimage)) contient une panoplie complète de filtres.\n\n5.3.1 Filtrage linéaire stationnaire\nUn filtrage linéaire stationnaire consiste à appliquer une même pondération locale des valeurs des pixels dans une fenêtre glissante. La taille de cette fenêtre est généralement un chiffre impair (3,5, etc.) afin de définir une position centrale et une fenêtre symétrique. La valeur calculée à partir de tous les pixels dans la fenêtre est alors attribuée au pixel central.\n\nLe filtre le plus simple est certainement le filtre moyen qui consiste à appliquer le même poids uniforme dans la fenêtre glissante. Par exemple pour un filtre 5x5:\n\\[\nF= \\frac{1}{25}\\left[\n\\begin{array}{c|c|c|c|c}\n1 & 1 & 1 & 1 & 1 \\\\\n\\hline\n1 & 1 & 1 & 1 & 1 \\\\\n\\hline\n1 & 1 & 1 & 1 & 1 \\\\\n\\hline\n1 & 1 & 1 & 1 & 1 \\\\\n\\hline\n1 & 1 & 1 & 1 & 1\n\\end{array}\n\\right]\n\\tag{5.5}\\]\nEn python, on dispose des fonctions rolling et sliding_window définis dans la librairie numpy. Par exemple pour le cas du filtre moyen, on construit une nouvelle vue de l’image avec deux nouvelles dimensions x_win et y_win:\n\nrolling_win = img_rgb.rolling(x=5, y=5,  min_periods= 3, center= True).construct(x=\"x_win\", y=\"y_win\", keep_attrs= True)\nprint(rolling_win[0,0,1,...])\nprint(rolling_win.shape)\n\n&lt;xarray.DataArray (x_win: 5, y_win: 5)&gt; Size: 100B\narray([[ nan,  nan,  nan,  nan,  nan],\n       [ nan,  nan, 209., 210., 209.],\n       [ nan,  nan, 213., 214., 212.],\n       [ nan,  nan, 213., 212., 210.],\n       [ nan,  nan, 210., 209., 206.]], dtype=float32)\nCoordinates:\n    band         int64 8B 1\n    x            float64 8B 1.5\n    y            float64 8B 0.5\n    spatial_ref  int64 8B 0\nDimensions without coordinates: x_win, y_win\n(3, 771, 1311, 5, 5)\n\n\nL’avantage de cette approche est qu’il n’y a pas d’utilisation inutile de la mémoire. Noter les nan sur les bords de l’image car la fenêtre déborde sur les bordures de l’image. Par la suite un opérateur de moyenne peut être appliqué sur les axes x_win et y_win correspondant aux fenêtres glissantes.\n\nfiltre_moyen= rolling_win.mean(dim= ['x_win', 'y_win'], skipna= True)\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8, 4))\nfiltre_moyen.astype('int').plot.imshow(rgb=\"band\")\nax.set_title(\"Filtre moyen 5x5\")\n\nText(0.5, 1.0, 'Filtre moyen 5x5')\n\n\n\n\n\n\n\n\n\nLorsque la taille \\(W\\) de la fenêtre devient trop grande, il est préférable d’utiliser une convolution dans le domaine fréquentielle. La fonction fftconvolve de la librairie scipy.signal offre cette possibilité:\n\nkernel = np.outer(signal.windows.gaussian(70, 8),\n                  signal.windows.gaussian(70, 8))\nblurred = signal.fftconvolve(img_rgb, kernel, mode='same')\n\n\n5.3.1.1 Filtrage par convolution\nLa façon la plus efficace d’appliquer un filtre linéaire est d’appliquer une convolution. La convolution est généralement très efficace car elle est peut être calculée dans le domaine fréquentiel. Prenons l’exemple du filtre de Scharr (Jahne et S. 1999), qui permet de détecter les contours horizontaux et verticaux:\n\\[\nF= \\left[\n\\begin{array}{ccc}\n-3-3j & 0-10j & +3-3j \\\\\n-10+0j & 0+0j & +10+0j \\\\\n-3+3j & 0+10j & +3+3j\n\\end{array}\n\\right]\n\\tag{5.6}\\]\nRemarquez l’utilisation de chiffres complexes afin de passer deux filtres différents sur la partie réelle et imaginaire.\n\nscharr = np.array([[ -3-3j, 0-10j,  +3 -3j],\n                   [-10+0j, 0+ 0j, +10 +0j],\n                   [ -3+3j, 0+10j,  +3 +3j]]) # Gx + j*Gy\nprint(img_rgb.isel(band=0).shape)\ngrad = signal.convolve2d(img_rgb.isel(band=0), scharr, boundary='symm', mode='same')\n# on reconstruit un xarray à partir du résultat:\narr = xr.DataArray(np.abs(grad), dims=(\"y\", \"x\"), coords= {'x': img_rgb.coords['x'], 'y': img_rgb.coords['y'], 'spatial_ref': 0})\nprint(arr)\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8, 4))\narr.plot.imshow()\nax.set_title(\"Amplitude du filtre de Scharr\")\n\n(771, 1311)\n&lt;xarray.DataArray (y: 771, x: 1311)&gt; Size: 8MB\narray([[  65.96969001,   58.85575588,   54.91812087, ..., 1474.        ,\n        1037.01205393,  389.99487176],\n       [  61.07372594,   39.8246155 ,   89.18520057, ..., 1763.79647352,\n         864.92543031,  270.20362692],\n       [  98.48857802,  112.44554237,  168.10710871, ..., 2110.61365484,\n         870.36658943,  204.40156555],\n       ...,\n       [ 143.17821063,  597.00753764, 2479.42977315, ...,  216.00925906,\n         248.33847869,  200.89798406],\n       [ 106.07544485,  393.67245268, 2188.78824924, ...,  124.96399481,\n         159.90622252,  346.34087255],\n       [  41.59326869,  229.05894438, 1845.1216762 , ...,  175.16278143,\n          33.37663854,  414.3911196 ]])\nCoordinates:\n  * x            (x) float64 10kB 0.5 1.5 2.5 ... 1.308e+03 1.31e+03 1.31e+03\n  * y            (y) float64 6kB 0.5 1.5 2.5 3.5 4.5 ... 767.5 768.5 769.5 770.5\n    spatial_ref  int64 8B 0\n\n\nText(0.5, 1.0, 'Amplitude du filtre de Scharr')",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Transformations spatiales</span>"
    ]
  },
  {
    "objectID": "04-TransformationSpatiales.html#gestion-des-bordures",
    "href": "04-TransformationSpatiales.html#gestion-des-bordures",
    "title": "5  Transformations spatiales",
    "section": "5.4 Gestion des bordures",
    "text": "5.4 Gestion des bordures\nL’application de filtres à l’intérieur de fenêtres glissantes implique de gérer les bords de l’image, car la fenêtre de traitement va nécessairement déborder de quelques pixels en dehors de l’image (généralement la moitié de la fenêtre déborde). On peut soit décider d’ignorer les valeurs en dehors de l’image en imposant une valeur nan, soit prolonger l’image de quelques lignes et colonnes avec des valeurs miroirs ou constantes.\n\n5.4.0.1 Filtrage par une couche convolutionnelle\nInstallation de Pytorch\nCette section nécessite la librairie Pytorch avec un GPU et ne fonctionnera que sur Colab. On peut quand même installer une version locale CPU de pytorch: pip install -qU torch==2.4.0+cpu\nUne couche convolutionnelle est simplement un ensemble de filtres appliqués sur la donnée d’entrée. Ce type de filtrage est à la base des réseaux dits convolutionnels qui seront abordés dans le tome 2. On peut ici imposer les mêmes filtres de gradient dans la couche convolutionnelle :\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport matplotlib.pyplot as plt\nnormalized_img= torch.tensor(img_rgb.to_numpy())\nnchannels= normalized_img.size()[0] # nombre de canaux de l'image\n\n# On forme une couche convolutionnelle\nconv_layer = nn.Conv2d(in_channels= nchannels, out_channels=2, kernel_size=3, padding=1, stride=1, dilation= 1)\n\n# Filtre de Sobel\nsobel_x = np.array([[-3, 0, 3], [-10, 0, 10], [-3, 0, 3]])\nsobel_y = np.array([[-3, -10, -3], [0, 0, 0], [3, 10, 3]])\n# Le filtre (kernel) est formé de deux filtres\nkernel = np.stack([sobel_x, sobel_y])\nkernel = kernel.reshape(2, 1, 3, 3)\n# On répète le filtre pour chaque bande\nkernel = np.tile(kernel,(1,nchannels,1,1))\nprint(kernel.shape)\nkernel = torch.as_tensor(kernel,dtype=torch.float32)\nconv_layer.weight = nn.Parameter(kernel)\nconv_layer.bias = nn.Parameter(torch.zeros(2,))\n\ninput= normalized_img.unsqueeze(0) # il faut ajouter une dimension pour le nombre d'échantillons\nprint(input.shape)\n# Visualize the filters\nfig, axs = plt.subplots(1, 2, figsize=(8, 5))\nfor i in range(2):\n    axs[i].imshow(conv_layer.weight.data.numpy()[i, 0])\n    axs[i].set_title(f'Filtre {i+1}')\nplt.show()\n\n(2, 3, 3, 3)\ntorch.Size([1, 3, 771, 1311])\n\n\n\n\n\n\n\n\n\nLe résultat est alors calculé sur GPU (si disponible):\n\nimport torch\nimport matplotlib.pyplot as plt\n\noutput = conv_layer(input)\nprint(f'Image (BxCxHxW): {input.shape}')\nprint(f'Sortie (BxFxHxW): {output.shape}')\n\nfig, axs = plt.subplots(1, 2, figsize=(20, 5))\nfor i in range(2):\n    axs[i].imshow(output.detach().data.numpy()[0,i], vmin=-5000, vmax=5000, cmap= 'gray')\n    axs[i].set_title(f'Filtrage {i+1}')\nplt.show()\n\nImage (BxCxHxW): torch.Size([1, 3, 771, 1311])\nSortie (BxFxHxW): torch.Size([1, 2, 771, 1311])\n\n\n\n\n\n\n\n\n\n\n\n5.4.1 Filtrage adaptatif\nLes filtrages adaptatifs consistent à appliquer un traitement en fonction du contenu local d’une image. Le filtre n’est alors plus stationnaire et sa réponse peut varier en fonction du contenu local. Ce type de filtre est très utilisé pour filtrer les images SAR (Synthetic Aperture Radar) qui sont dégradées par un bruit multiplicatif que l’on appelle speckle. On peut voir un exemple d’une image Sentinel-1 (bande HH) sur la région de Montréal, remarquez que l’image est affichée en dB en appliquant la fonction log10.\n\nprint(img_SAR.rio.resolution())\nprint(img_SAR.rio.crs)\nfig, axs = plt.subplots(1, 1, figsize=(6, 4))\nxr.ufuncs.log10(img_SAR.sel(band=1).drop(\"band\")).plot()\naxs.set_title(\"Image SAR Sentinel-1 (dB)\")\n\n(0.00029254428869762705, -0.000287092818453516)\nEPSG:4326\n\n\nText(0.5, 1.0, 'Image SAR Sentinel-1 (dB)')\n\n\n\n\n\n\n\n\n\nUn des filtres les plus simples pour réduire le bruit est d’appliquer un filtre moyen, par exemple un \\(5 \\times 5\\) ci-dessous:\n\nrolling_win = img_SAR.sel(band=2).rolling(x=5, y=5,  min_periods= 3, center= True).construct(x=\"x_win\", y=\"y_win\", keep_attrs= True)\nfiltre_moyen= rolling_win.mean(dim= ['x_win', 'y_win'], skipna= True)\nfig, axs = plt.subplots(1, 1, figsize=(6, 4))\nxr.ufuncs.log10(filtre_moyen).plot.imshow()\naxs.set_title(\"Filtrage moyen 5x5 (dB)\")\n\nText(0.5, 1.0, 'Filtrage moyen 5x5 (dB)')\n\n\n\n\n\n\n\n\n\nAu lieu d’appliquer un filtre moyen de manière indiscriminée, le filtre de Lee (Lee 1986) applique une pondération en fonction du contenu local de l’image \\(I\\) dans sa forme la plus simple :\n\\[\n\\begin{aligned}\nI_F & = I_M + K \\times (I - I_M) \\\\\nK & = \\frac{\\sigma^2_I}{\\sigma^2_I + \\sigma^2_{bruit}}\n\\end{aligned}\n\\tag{5.7}\\]\nDe la sorte, si la variance locale est élevée \\(K\\) s’approche de \\(1\\) préservant ainsi les détails de l’image \\(I\\) sinon l’image moyenne \\(I_M\\) est appliquée.\n\nrolling_win = img_SAR.sel(band=2).rolling(x=5, y=5,  min_periods= 3, center= True).construct(x=\"x_win\", y=\"y_win\", keep_attrs= True)\nfiltre_moyen= rolling_win.mean(dim= ['x_win', 'y_win'], skipna= True)\necart_type= rolling_win.std(dim= ['x_win', 'y_win'], skipna= True)\ncv= ecart_type/filtre_moyen\nponderation = (cv - 0.25) / cv\n\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 4), sharex=True, sharey=True)\nplt.subplot(1, 2, 1)\ncv.plot.imshow( vmin=0, vmax=2)\naxes[0].set_title(\"CV\")\nplt.subplot(1, 2, 2)\nponderation.plot.imshow( vmin=0, vmax=1) \naxes[1].set_title(\"Pondération\")\nplt.tight_layout()\n\n\n\n\n\n\n\n\nOn zoomant sur l’image, on voit clairement que les détails de l’image sont mieux préservés :",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Transformations spatiales</span>"
    ]
  },
  {
    "objectID": "04-TransformationSpatiales.html#segmentation",
    "href": "04-TransformationSpatiales.html#segmentation",
    "title": "5  Transformations spatiales",
    "section": "5.5 Segmentation",
    "text": "5.5 Segmentation\nLa segmentation d’image consiste à séparer une image en régions homogènes spatialement connexes (segments) où les valeurs sont uniformes selon un certain critère (couleurs, texture, etc.). Une image présente généralement beaucoup de pixels redondants, l’intérêt de ce type de méthode est essentiellement de réduire la quantité de pixels nécessaire. En télédétection, on parle souvent d’approche objet. En vision par ordinateur, on parle parfois de super-pixel. Il existe de nombreuses méthodes de segmentation, la librairie sickit-image rend disponible plusieurs implémentations sur des images RVB (Comparison of segmentation and superpixel algorithms — skimage 0.25.0 documentation).\n\n5.5.1 Super-pixel\nCe type de méthode cherche à former des régions homogènes et compactes dans l’image (Achanta et Süsstrunk 2012). Une des méthodes les plus simples est la méthode SLIC (Simple Linear Iterative Clustering), elle combine un regroupement de type k-moyennes avec une distance hybride qui prend en compte les différences de couleur entre pixels mais aussi leur distance par rapport centre du super-pixel:\n\nDécomposer l’image en N régions régulières de taille \\(S \\times S\\)\nInitialiser les centres \\(C_k\\) de chaque segment \\(k\\)\nRechercher les pixels ayant la distance la plus petite dans une région \\(2S \\times 2S\\):\n\n\\[\nD_{SLIC}= d_{couleur} + \\frac{m}{S}d_{xy}\n\\]\n\nMettre à jour les centre \\(C_k\\) de chaque segment \\(k\\) et réitérer à l’étape 3.\n\nLes régions évoluent rapidement avec les itérations, plus le poids \\(m\\) est élevé, plus la forme du super-pixel est contrainte et ne suivra pas vraiment le contenu de l’image:\n\nimg = img_rgb.to_numpy().astype('uint8').transpose(1,2,0) \n\nsegments_slic1 = slic(img, n_segments=250, compactness=10, sigma=1, start_label=1, max_num_iter=1)\nsegments_slic2 = slic(img, n_segments=250, compactness=10, sigma=1, start_label=1, max_num_iter=2)\nsegments_slic100 = slic(img, n_segments=250, compactness=100, sigma=1, start_label=1, max_num_iter=10)\nsegments_slic100b = slic(img, n_segments=250, compactness=10, sigma=1, start_label=1, max_num_iter=10)\n\nprint(f'SLIC nombre de segments: {len(np.unique(segments_slic1))}')\n\nfig, ax = plt.subplots(2, 2, figsize=(10, 6), sharex=True, sharey=True)\n\nax[0, 0].imshow(mark_boundaries(img, segments_slic1))\nax[0, 0].set_title(\"Initialisation\")\nax[0, 1].imshow(mark_boundaries(img, segments_slic2))\nax[0, 1].set_title('2 itérations')\nax[1, 0].imshow(mark_boundaries(img, segments_slic100))\nax[1, 0].set_title('10 itérations avec m=100')\nax[1, 1].imshow(mark_boundaries(img, segments_slic100b))\nax[1, 1].set_title('10 itérations avec m=10')\n\nfor a in ax.ravel():\n    a.set_axis_off()\n\nplt.tight_layout()\nplt.show()\n\nSLIC nombre de segments: 240\n\n\n\n\n\n\n\n\n\nLe nombre de segments initial est probablement le paramètre le plus important. Une manière de l’estimer est d’évaluer l’échelle moyenne des segments homogènes dans l’image à analyser. On observe ci-dessous l’impact de passer d’une échelle 40 x 40 à 20 x 20. En prenant la moyenne de chaque segment, on constate que l’échelle 40 x 40 génère des segments trop grands mélangeant plusieurs classes.\n\nfrom skimage import color, segmentation\nn_regions = int((img.shape[0] * img.shape[1])/(40*40))\nprint('Nb segments: ',n_regions)\nsegments_slic_40 = slic(img, n_segments=n_regions, compactness=10, sigma=1, start_label=1, max_num_iter=10)\nprint(f'SLIC nombre de segments: {len(np.unique(segments_slic_40))}')\nout = color.label2rgb(segments_slic_40, img, kind='avg', bg_label=0)\nout_40 = segmentation.mark_boundaries(out, segments_slic_40, (0, 0, 0))\n\nn_regions = int((img.shape[0] * img.shape[1])/(20*20))\nprint('Nb segments: ',n_regions)\nsegments_slic_20 = slic(img, n_segments=n_regions, compactness=10, sigma=1, start_label=1, max_num_iter=10)\nprint(f'SLIC nombre de segments: {len(np.unique(segments_slic_20))}')\nout = color.label2rgb(segments_slic_20, img, kind='avg', bg_label=0)\nout_20 = segmentation.mark_boundaries(out, segments_slic_20, (0, 0, 0))\n\nfig, ax = plt.subplots(2, 1, figsize=(6, 8), sharex=True, sharey=True)\n\nax[0].imshow(out_40)\nax[0].set_title(\"Initialisation avec 631 segments\")\nax[1].imshow(out_20)\nax[1].set_title('Initialisation avec 2526 segments')\nfor a in ax.ravel():\n    a.set_axis_off()\nplt.tight_layout()\nplt.show()\n\nNb segments:  631\nSLIC nombre de segments: 459\nNb segments:  2526\nSLIC nombre de segments: 2201\n\n\n\n\n\n\n\n\n\n\n\n5.5.2 Fusion des segments par graphe de proximité\nUne segmentation peut produire beaucoup trop de segments. On parle alors de sur-segmentation. Ceci est recherché dans certains cas pour permettre de bien capturer les détails fins de l’image. Cependant, afin de réduire le nombre de segments, un post-traitement possible est de fusionner les segments similaires selon certaines règles ou distances. Un graphe d’adjacence de régions (figure 5.1) est formé à partir des segments connectés où chaque nœud représente un segment et un lien de proximité (Jaworek-Korjakowska (2018)). À partir de ce graphe, on peut fusionner les nœuds similaires à partir de leur distance radiométrique.\n\n\n\n\n\n\nFigure 5.1: Graphe d’adjacence de régions, d’après (Jaworek-Korjakowska (2018)). Chaque nœud est un segment, un lien est formé uniquement si les segments se touchent (par exemple le segment 6 ne touche que la région 5). La fonction graph.rag_mean_color produit un graphe à partir d’une segmentation et de l’image originale. Chaque nœud tient la couleur de chaque segment dans un attribut appelé 'mean color'.\n\n\n\n\ndef _weight_mean_color(graph, src, dst, n):\n    \"\"\"Fonction pour gérer la fusion des nœuds en recalculant la couleur moyenne.\n    La méthode suppose que la couleur moyenne de `dst` est déjà calculée.\n    \"\"\"\n    diff = graph.nodes[dst]['mean color'] - graph.nodes[n]['mean color']\n    diff = np.linalg.norm(diff)\n    #print(diff)\n    return {'weight': diff}\n\n\ndef merge_mean_color(graph, src, dst):\n    \"\"\"Fonction appelée avant la fusion de deux nœuds d'un graphe de distance de couleur moyenne.\n      Cette méthode calcule la couleur moyenne de `dst`.\n    \"\"\"\n    graph.nodes[dst]['total color'] += graph.nodes[src]['total color']\n    graph.nodes[dst]['pixel count'] += graph.nodes[src]['pixel count']\n    graph.nodes[dst]['mean color'] = (\n        graph.nodes[dst]['total color'] / graph.nodes[dst]['pixel count']\n    )\ng = graph.rag_mean_color(img, segments_slic_20)\nprint('Nombre de segments:',len(g))\nlabels2 = graph.merge_hierarchical(\n    segments_slic_20,\n    g,\n    thresh=20,\n    rag_copy=False,\n    in_place_merge=True,\n    merge_func=merge_mean_color,\n    weight_func=_weight_mean_color,\n)\nprint('Nombre de segments:',len(g))\n\nout1 = color.label2rgb(segments_slic_20, img, kind='avg', bg_label=0)\nout1 = segmentation.mark_boundaries(out1, segments_slic_20, (0, 0, 0))\nout2 = color.label2rgb(labels2, img, kind='avg', bg_label=0)\nout2 = segmentation.mark_boundaries(out2, labels2, (0, 0, 0))\n\nfig, ax = plt.subplots(nrows=2, sharex=True, sharey=True, figsize=(6, 8))\n\nax[0].imshow(out1)\nax[0].set_title(\"Avant fusion\")\nax[1].imshow(out2)\nax[1].set_title(\"Après fusion\")\nfor a in ax:\n    a.axis('off')\n\nplt.tight_layout()\n\nNombre de segments: 2201\nNombre de segments: 1187\n\n\n\n\n\n\n\n\n\n\n\n5.5.3 Approche objet\nL’approche objet consiste à traiter chaque segment comme un objet avec un ensemble de propriétés. La librairie skimage offre la possibilité d’enrichir chaque segment avec des propriétés et de former un tableau:\n\nproperties = ['label', 'area', 'centroid', 'num_pixels', 'intensity_mean', 'intensity_std']\n\ntable=   measure.regionprops_table(labels2, intensity_image= img_rgb.to_numpy().transpose(1,2,0), properties=properties)\n\ntable = pd.DataFrame(table)\ntable.head(10)\n\n\n\n\n\n\n\n\nlabel\narea\ncentroid-0\ncentroid-1\nnum_pixels\nintensity_mean-0\nintensity_mean-1\nintensity_mean-2\nintensity_std-0\nintensity_std-1\nintensity_std-2\n\n\n\n\n0\n1\n641.0\n15.466459\n69.489860\n641\n136.730103\n132.851791\n117.126366\n32.289547\n31.451048\n37.421638\n\n\n1\n2\n480.0\n10.997917\n92.614583\n480\n201.208328\n198.262497\n188.483337\n14.184592\n14.151334\n15.475913\n\n\n2\n3\n712.0\n16.683989\n114.776685\n712\n185.349716\n183.113770\n170.994385\n25.453747\n26.184948\n28.128426\n\n\n3\n4\n1803.0\n31.974487\n139.379368\n1803\n117.897392\n108.367722\n97.769829\n31.086676\n26.577900\n28.297256\n\n\n4\n5\n448.0\n5.004464\n166.542411\n448\n183.511154\n181.276779\n167.720978\n29.824030\n30.625013\n31.297607\n\n\n5\n6\n459.0\n9.934641\n191.668845\n459\n133.557739\n133.821350\n129.697174\n22.902142\n23.013086\n22.428919\n\n\n6\n7\n355.0\n5.160563\n222.895775\n355\n148.574646\n148.802811\n142.580276\n21.334805\n21.457472\n20.931572\n\n\n7\n8\n334.0\n4.904192\n255.904192\n334\n125.973053\n121.197601\n108.973053\n23.151978\n24.556480\n25.351229\n\n\n8\n9\n1481.0\n32.279541\n292.865631\n1481\n204.102631\n172.359894\n137.501007\n13.884891\n14.092896\n15.865581\n\n\n9\n10\n445.0\n8.013483\n308.053933\n445\n145.373032\n138.182022\n121.402245\n18.543356\n18.644655\n22.237881\n\n\n\n\n\n\n\nCe tableau pourra être exploiter pour une tâche de classification par la suite (on parle alors de classification objet).\n\n\n\n\nAchanta, Kevin Smith adhakrishna, Appu Shaji et Sabine Süsstrunk. 2012. « SLIC Superpixels Compared to State-of-the-art Superpixel Methods. » TPAMI: 636‑643. https://doi.org/10.1109/TPAMI.2012.120.\n\n\nCooley, James W. et John W. Tukey. 1965. « An algorithm for the machine calculation of complex Fourier series. » Math. Comput.: 297‑301. https://web.stanford.edu/class/cme324/classics/cooley-tukey.pdf.\n\n\nJahne, Scharr, B et Korkel S. 1999. Principles of filter design. Handbook of Computer Vision; Applications; Academic Press.\n\n\nJaworek-Korjakowska, P., J.; Kłeczek. 2018. « Region Adjacency Graph Approach for Acral Melanocytic Lesion Segmentation. » Applied Sciences 8: 1430. 10.3390/app8091430.\n\n\nLee, J. S. 1986. « Speckle suppression and analysis for synthetic aperture radar images. » Opt. Eng. 25 (5): 636‑643. https://doi.org/10.1117/12.7973877.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Transformations spatiales</span>"
    ]
  },
  {
    "objectID": "05-ClassificationsSupervisees.html",
    "href": "05-ClassificationsSupervisees.html",
    "title": "6  Classifications d’images supervisées",
    "section": "",
    "text": "6.1 Préambule\nAssurez-vous de lire ce préambule avant d’exécuter le reste du notebook.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Classifications d'images supervisées</span>"
    ]
  },
  {
    "objectID": "05-ClassificationsSupervisees.html#préambule",
    "href": "05-ClassificationsSupervisees.html#préambule",
    "title": "6  Classifications d’images supervisées",
    "section": "",
    "text": "6.1.1 Objectifs\nDans ce chapitre, nous ferons une introduction générale à l’apprentissage automatique et abordons quelques techniques fondamentales. La librairie centrale utilisée dans ce chapitre sera sickit-learn. Ce chapitre est aussi disponible sous la forme d’un notebook Python sur Google Colab:\n\n\n\n\n\n\nObjectifs d’apprentissage visés dans ce chapitre\n\nÀ la fin de ce chapitre, vous devriez être en mesure de :\n\ncomprendre les principes de l’apprentissage automatique supervisé;\nmettre en place un pipeline d’entraînement;\nsavoir comment évaluer les résultats d’un classificateur;\nvisualiser les frontières de décision;\nmettre en place des techniques de classifications comme K-NN et les arbres de décision;\n\n\n\n\n6.1.2 Librairies\nLes librairies utilisées dans ce chapitre sont les suivantes :\n\nSciPy\nNumPy\nopencv-python · PyPI\nscikit-image\nRasterio\nxarray\nrioxarray\ngeopandas\nscikit-learn\n\nDans l’environnement Google Colab, seul rioxarray et xrscipy sont installés:\n\n%%capture\n!pip install -qU matplotlib rioxarray xrscipy supertree\n\nVérifiez les importations nécessaires en premier:\n\nimport numpy as np\nimport rioxarray as rxr\nfrom scipy import signal\nimport xarray as xr\nimport rasterio\nimport xrscipy\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nimport geopandas\nfrom shapely.geometry import Point\nimport pandas as pd\nfrom numba import jit\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.inspection import DecisionBoundaryDisplay\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.datasets import make_blobs, make_classification, make_gaussian_quantiles\n\n\n\n6.1.3 Images utilisées\nNous utilisons les images suivantes dans ce chapitre:\n\n%%capture\nimport gdown\n\ngdown.download('https://drive.google.com/uc?export=download&confirm=pbef&id=1a6Ypg0g1Oy4AJt9XWKWfnR12NW1XhNg_', output= 'RGBNIR_of_S2A.tif')\ngdown.download('https://drive.google.com/uc?export=download&confirm=pbef&id=1a4PQ68Ru8zBphbQ22j0sgJ4D2quw-Wo6', output= 'landsat7.tif')\ngdown.download('https://drive.google.com/uc?export=download&confirm=pbef&id=1_zwCLN-x7XJcNHJCH6Z8upEdUXtVtvs1', output= 'berkeley.jpg')\ngdown.download('https://drive.google.com/uc?export=download&confirm=pbef&id=1dM6IVqjba6GHwTLmI7CpX8GP2z5txUq6', output= 'SAR.tif')\ngdown.download('https://drive.google.com/uc?export=download&confirm=pbef&id=1aAq7crc_LoaLC3kG3HkQ6Fv5JfG0mswg', output= 'carte.tif')\n\nVérifiez que vous êtes capable de les lire :\n\nwith rxr.open_rasterio('berkeley.jpg', mask_and_scale= True) as img_rgb:\n    print(img_rgb)\nwith rxr.open_rasterio('RGBNIR_of_S2A.tif', mask_and_scale= True) as img_rgbnir:\n    print(img_rgbnir)\nwith rxr.open_rasterio('SAR.tif', mask_and_scale= True) as img_SAR:\n    print(img_SAR)\nwith rxr.open_rasterio('carte.tif', mask_and_scale= True) as img_carte:\n    print(img_carte)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Classifications d'images supervisées</span>"
    ]
  },
  {
    "objectID": "05-ClassificationsSupervisees.html#principes-généraux",
    "href": "05-ClassificationsSupervisees.html#principes-généraux",
    "title": "6  Classifications d’images supervisées",
    "section": "6.2 Principes généraux",
    "text": "6.2 Principes généraux\nUne classification supervisée ou dirigée consiste à attribuer une étiquette (une classe) de manière automatique à chaque point d’un jeu de données. Cette classification peut se faire à l’aide d’une cascade de règles pré-établies (arbre de décision) ou à l’aide de techniques d’apprentissage automatique (machine learning). L’utilisation de règles pré-établies atteint vite une limite car ces règles doivent être fournies manuellement par un expert. Ainsi, l’avantage de l’apprentissage automatique est que les règles de décision sont dérivées automatiquement du jeu de données via une phase dite d’entraînement. On parle souvent de solutions générées par les données (Data Driven Solutions). Cet ensemble de règles est souvent appelé modèle. On visualise souvent ces règles sous la forme de frontières de décisions dans l’espace des données. Cependant, un des défis majeurs de ce type de technique est d’être capable de produire des règles qui soient généralisables au-delà du jeu d’entraînement.\nLes classifications supervisées ou dirigées présupposent donc que nous avons à disposition un jeu d’entraînement déjà étiqueté. Celui-ci va nous permettre de construire un modèle. Afin que ce modèle soit représentatif et robuste, il nous faut assez de données d’entraînement. Les algorithmes d’apprentissage automatique sont très nombreux et plus ou moins complexes pouvant produire des frontières de décision très complexes et non linéaires.\n\n6.2.1 Comportement d’un modèle\nCet exemple tiré de sickit-learn illustre les problèmes d’ajustement insuffisant ou sous-apprentissage (underfitting) et d’ajustement excessif ou sur-apprentissage (overfitting) et montre comment nous pouvons utiliser la régression linéaire avec un modèle polynomiale pour approximer des fonctions non linéaires. La figure 6.1 montre la fonction que nous voulons approximer, qui est une partie de la fonction cosinus (couleur orange). En outre, les échantillons de la fonction réelle et les approximations de différents modèles sont affichés en bleu. Les modèles ont des caractéristiques polynomiales de différents degrés. Nous pouvons constater qu’une fonction linéaire (polynôme de degré 1) n’est pas suffisante pour s’adapter aux échantillons d’apprentissage. C’est ce qu’on appelle un sous-ajustement (underfitting) qui produit un biais systématique quels que soient les points d’entraînement. Un polynôme de degré 4 se rapproche presque parfaitement de la fonction réelle. Cependant, pour des degrés plus élevés, le modèle s’adaptera trop aux données d’apprentissage, c’est-à-dire qu’il apprendra le bruit des données d’apprentissage. Nous évaluons quantitativement le sur-apprentissage et le sous-apprentissage à l’aide de la validation croisée. Nous calculons l’erreur quadratique moyenne (EQM) sur l’ensemble de validation. Plus elle est élevée, moins le modèle est susceptible de se généraliser correctement à partir des données d’apprentissage.\n\n\n\n\n\n\n\n\nFigure 6.1: Exemples de sur et sous-apprentissage.\n\n\n\n\n\nOn constate aussi que sans les échantillons de validation, nous serions incapables de déterminer la situation de sur-apprentissage, l’erreur sur les points d’entraînement seuls étant excellente pour un degré 15.\n\n\n6.2.2 Pipeline\nLa construction d’un modèle implique généralement toujours les mêmes étapes illustrées sur la figure 6.2:\n\nLa préparation des données implique parfois un pré-traitement afin de normaliser les données.\nPartage des données en trois groupes: entraînement, validation et test.\nL’apprentissage du modèle sur l’ensemble d’entraînement. Cet apprentissage nécessite de déterminer les valeurs des hyper-paramètres du modèle par l’usager.\nLa validation du modèle sur l’ensemble de validation. Cette étape vise à vérifier que les hyper-paramètres du modèle sont adéquats.\nEnfin le test du modèle sur un ensemble de données indépendant.\n\n\n\n\n\n\n\nflowchart TD\n    A[fa:fa-database Données] --&gt; B(fa:fa-gear Prétraitement)\n    B --&gt; C(fa:fa-folder-tree Partage des données) -.-&gt; D(fa:fa-gears Entraînement)\n    H[[Hyper-paramètres]] --&gt; D\n    D --&gt; |Modèle| E&gt;Validation]\n    E --&gt; |Modèle| G&gt;Test]\n    C -.-&gt; E\n    C -.-&gt; G\n\n\n\n\nFigure 6.2: Étapes standards dans un entraînement.\n\n\n\n\n\n\n\n6.2.3 Construction d’un ensemble d’entraînement\nLes données d’entraînement permettent de construire un modèle. Elles peuvent prendre des formes très variées mais on peut voir cela sous la forme d’un tableau \\(N \\times D\\):\n\nLa taille \\(N\\) du jeu de données.\nChaque entrée définit un échantillon ou un point dans un espace à plusieurs dimensions.\nChaque échantillon est décrit par \\(D\\) dimensions ou caractéristiques (features).\n\nUne façon simple de construire un ensemble d’entraînement est d’échantillonner un produit existant. Nous utilisons une carte d’occupation des sols qui contient 12 classes différentes.\n\ncouleurs_classes= {'NoData': 'black', 'Commercial': 'yellow', 'Nuages': 'lightgrey', \n                    'Foret': 'darkgreen', 'Faible_végétation': 'green', 'Sol_nu': 'saddlebrown',\n                  'Roche': 'dimgray', 'Route': 'red', 'Urbain': 'orange', 'Eau': 'blue', 'Tourbe': 'salmon', 'Végétation éparse': 'darkgoldenrod', 'Roche avec végétation': 'darkseagreen'}\nnom_classes= [*couleurs_classes.keys()]\ncouleurs_classes= [*couleurs_classes.values()]\n\nOn peut visualiser la carte de la façon suivante :\n\nimport matplotlib.pyplot as plt\nimport rioxarray as rxr\ncmap_classes = ListedColormap(couleurs_classes)\n\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8, 6))\nimg_carte.squeeze().plot.imshow(cmap=cmap_classes, vmin=0, vmax=12)\nax.set_title(\"Carte d'occupation des sols\", fontsize=\"small\")\n\nText(0.5, 1.0, \"Carte d'occupation des sols\")\n\n\n\n\n\n\n\n\n\nOn peut facilement calculer la fréquence d’occurrences des 12 classes dans l’image à l’aide de numpy:\n\nimg_carte= img_carte.squeeze() # nécessaire pour ignorer la dimension du canal\ncompte_classe = np.unique(img_carte.data, return_counts=True)\nprint(compte_classe)\n\n(array([ 1.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 11., 12., nan],\n      dtype=float32), array([ 193558, 2104777,  670158,   29523,   14624,   94751,  750046,\n        123671,    9079,    4327,      10]))\n\n\nLa fréquence d’apparition de chaque classe varie grandement, on parle alors d’un ensemble déséquilibré. Ceci est très commun dans la plupart des ensembles d’entraînement, puisque les classes ont très rarement la même fréquence. Par exemple, à la lecture du graphique en barres verticales, on constate que la classe forêt est très présentes contrairement à plusieurs autres classes (notamment, tourbe, végétation éparse, roche, sol nu et nuages).\n\nvaleurs, comptes = compte_classe\n\n# Create the histogram\nplt.figure(figsize=(5, 3))\nplt.bar(valeurs, comptes/comptes.sum()*100)\nplt.xlabel(\"Classes\")\nplt.ylabel(\"%\")\nplt.title(\"Fréquences des classes\", fontsize=\"small\")\nplt.xticks(range(len(nom_classes)), nom_classes, rotation=45, ha='right')\nplt.show()\n\n\n\n\n\n\n\n\nOn peut échantillonner aléatoirement 100 points pour chaque classe:\n\nimg_carte= img_carte.squeeze()\nclass_counts = np.unique(img_carte.data, return_counts=True)\n\n# Liste vide des points échantillonnées\nsampled_points = []\nclass_labels= [] # contient les étiquettes des classes\nfor class_label in range(1,13): # pour chacune des 12 classes\n  # On cherche tous les pixels pour cette étiquette\n  class_pixels = np.argwhere(img_carte.data == class_label)\n\n  # On se limite à 100 pixels par classe\n  n_samples = min(100, len(class_pixels))\n\n  # On les choisit les positions aléatoirement\n  np.random.seed(0) # ceci permet de répliquer le tirage aléatoire\n  sampled_indices = np.random.choice(len(class_pixels), n_samples, replace=False)\n\n  # On prends les positions en lignes, colonnes\n  sampled_pixels = class_pixels[sampled_indices]\n\n  # On ajoute les points à la liste\n  sampled_points.extend(sampled_pixels)\n  class_labels.extend(np.array([class_label]*n_samples)[:,np.newaxis])\n\n# Conversion en NumPy array\nsampled_points = np.array(sampled_points)\nclass_labels = np.array(class_labels)\n# On peut naviguer les points à l'aide de la géoréférence\ntransformer = rasterio.transform.AffineTransformer(img_carte.rio.transform())\ntransform_sampled_points= transformer.xy(sampled_points[:,0], sampled_points[:,1])\n\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8, 6))\nimg_carte.squeeze().plot.imshow(cmap=cmap_classes, vmin=0, vmax=12)\nax.scatter(transform_sampled_points[0], transform_sampled_points[1], c='w', s=1)  # Plot sampled points\nax.set_title(\"Carte d'occupation des sols avec les points échantillonnés\", fontsize=\"small\")\nplt.show()\n\n\n\n\n\n\n\n\nUne fois les points sélectionnés, on ajoute les valeurs des bandes provenant d’une image satellite. Pour cela, on utilise la méthode sample() de rasterio. Éventuellement, la librairie geopandas permet de gérer les données d’entraînement sous la forme d’un tableau transportant aussi l’information de géoréférence. Afin de pouvoir classifier ces points, on ajoute les valeurs radiométriques provenant de l’image Sentinel-2 à 4 bandes RGBNIR_of_S2A.tif. Ces valeurs seront stockées dans la colonne value sous la forme d’un vecteur en format string :\n\npoints = [Point(xy) for xy in zip(transform_sampled_points[0], transform_sampled_points[1])]\ngdf = geopandas.GeoDataFrame(range(1,len(points)+1), geometry=points, crs=img_carte.rio.crs)\ncoord_list = [(x, y) for x, y in zip(gdf[\"geometry\"].x, gdf[\"geometry\"].y)]\nwith rasterio.open('RGBNIR_of_S2A.tif') as src:\n  gdf[\"value\"] = [x for x in src.sample(coord_list)]\ngdf['class']= class_labels\ngdf.to_csv('sampling_points.csv') # sauvegarde sous forme d'un format csv\ngdf.head()\n\n\n\n\n\n\n\n\n0\ngeometry\nvalue\nclass\n\n\n\n\n0\n1\nPOINT (740369.77 5032078.683)\n[1894, 1994, 2112, 2318]\n1\n\n\n1\n2\nPOINT (737542.924 5031770.119)\n[1440, 1650, 1449, 5021]\n1\n\n\n2\n3\nPOINT (736726.722 5031411.786)\n[1666, 1972, 1819, 3437]\n1\n\n\n3\n4\nPOINT (736816.305 5027470.128)\n[1858, 2078, 2190, 2436]\n1\n\n\n4\n5\nPOINT (736746.629 5031362.018)\n[2194, 2304, 2268, 3075]\n1",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Classifications d'images supervisées</span>"
    ]
  },
  {
    "objectID": "05-ClassificationsSupervisees.html#analyse-préliminaire-des-données",
    "href": "05-ClassificationsSupervisees.html#analyse-préliminaire-des-données",
    "title": "6  Classifications d’images supervisées",
    "section": "6.3 Analyse préliminaire des données",
    "text": "6.3 Analyse préliminaire des données\nUne bonne pratique avant d’appliquer une technique d’apprentissage automatique est de regarder les caractéristiques de vos données:\n\nLe nombre de dimensions (features).\nCertaines dimensions sont informatives (discriminantes) et d’autres ne le sont pas.\nLe nombre classes.\nLe nombre de modes (clusters) par classes.\nLe nombre d’échantillons par classe.\nLa forme des groupes.\nLa séparabilité des classes ou des groupes.\n\nUne manière d’évaluer la séparabilité de vos classes est d’appliquer des modèles Gaussiens sur chacune des classes. Le modèle Gaussien multivarié suppose que les données sont distribuées comme un nuage de points symétrique et unimodale. La distribution d’un point \\(x\\) appartenant à la classe \\(i\\) est la suivante:\n\\[\nP(x | Classe=i) = \\frac{1}{(2\\pi)^{D/2} |\\Sigma_i|^{1/2}}\\exp\\left(-\\frac{1}{2} (x-m_i)^t \\Sigma_k^{-1} (x-m_i)\\right)\n\\]\nLa méthode QuadraticDiscriminantAnalysis permet de calculer les paramètres des Gaussiennes multivariées pour chacune des classes.\nOn peut calculer une distance entre deux nuages Gaussiens avec la distance dites de Jeffries-Matusita (JM) basée sur la distance de Bhattacharyya \\(B\\) (Jensen 2016):\n\\[\n\\begin{aligned}\nJM_{ij} &= 2(1 - e^{-B_{ij}}) \\\\\nB_{ij} &= \\frac{1}{8}(m_i - m_j)^t \\left( \\frac{\\Sigma_i + \\Sigma_j}{2} \\right)^{-1} (m_i - m_j) + \\frac{1}{2} \\ln \\left( \\frac{|(\\Sigma_i + \\Sigma_j)/2|}{|\\Sigma_i|^{1/2} |\\Sigma_j|^{1/2}} \\right)\n\\end{aligned}\n\\]\nCette distance présuppose que chaque classe \\(i\\) est décrite par son centre \\(m_i\\) et de sa dispersion dans l’espace à \\(D\\) dimensions mesurée par la matrice de covariance \\(\\Sigma_i\\). On peut en faire facilement une fonction Python à l’aide de numpy:\n\ndef bhattacharyya_distance(m1, s1, m2, s2):\n    # Calcul de la covariance moyenne\n    s = (s1 + s2) / 2\n    \n    # Calcul du premier terme (différence des moyennes)\n    m_diff = m1 - m2\n    term1 = np.dot(np.dot(m_diff.T, np.linalg.inv(s)), m_diff) / 8\n    \n    # Calcul du second terme (différence de covariances)\n    term2 = 0.5 * np.log(np.linalg.det(s) / np.sqrt(np.linalg.det(s1) * np.linalg.det(s2)))\n    \n    return term1 + term2\n\ndef jeffries_matusita_distance(m1, s1, m2, s2):\n    B = bhattacharyya_distance(m1, s1, m2, s2)\n    return 2 * (1 - np.exp(-B))\n\nLa figure ci-dessous illustre différentes situations avec des données simulées ainsi que les distances JM correspondantes :\n\n\n\n\n\n\n\n\n\nOn forme notre ensemble d’entrainement à partir du fichier csv de la section Section 6.2.3.\n\ndf= pd.read_csv('sampling_points.csv')\n# Extraire la colonne 'value'.\n# 'value' est une chaîne de caractères représentation d'une liste de nombres.\n# Nous devons la convertir en données numériques réelles.\nX = df['value'].apply(lambda x: np.fromstring(x[1:-1], dtype=float, sep=' ')).to_list()\n\n# on obtient une liste de numpy array  qu'il faut convertir en un numpy array 2D\nX= np.array([row.tolist() for row in X])\nidx= X.sum(axis=-1)&gt;0 # on exclut certains points sans valeurs\nX= X[idx,...]\ny = df['class'].to_numpy()\ny= y[idx]\nclass_labels = np.unique(y).tolist() # on cherche à savoir combien de classes uniques\nn_classes = len(class_labels)\nif max(class_labels) &gt; n_classes: # il se peut que certaines classes soit absentes\n  y_new= []\n  for i,l in enumerate(class_labels):\n    y_new.extend([i]*sum(y==l))\n  y_new = np.array(y_new)\n\ncouleurs_classes2= [couleurs_classes[c] for c in np.unique(y).tolist()] # couleurs des classes\nnom_classes2= [nom_classes[c] for c in np.unique(y).tolist()]\ncmap_classes2 = ListedColormap(couleurs_classes2)\n\nOn peut faire une analyse de séparabilité sur notre ensemble d’entrainement de 10 classes. On obtient un tableau symmétrique de 10x10 valeurs. On observe des valeurs inférieures à 1, indiquant des séparabilités faibles entre ces classes sous l’hypothèse du modèle Gaussien:\n\nqda= QuadraticDiscriminantAnalysis(store_covariance=True)\nqda.fit(X, y_new) # calcul des paramètres des distributions Gaussiennes\nJM= []\nclasses= np.unique(y_new).tolist() # étiquettes uniques des classes\nfor cl1 in classes:\n  for cl2 in classes:\n    JM.append(jeffries_matusita_distance(qda.means_[cl1], qda.covariance_[cl1], qda.means_[cl2], qda.covariance_[cl2]))\n\nJM= np.array(JM).reshape(len(classes),len(classes))\nJM= pd.DataFrame(JM, index=classes, columns=classes)\nJM.head(10)\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n\n0\n0.000000\n1.931891\n1.809590\n1.761760\n1.156486\n1.326107\n1.319344\n1.830671\n1.873676\n1.700417\n\n\n1\n1.931891\n0.000000\n1.082978\n0.918865\n1.788737\n1.527192\n1.331400\n1.901749\n0.854802\n1.133180\n\n\n2\n1.809590\n1.082978\n0.000000\n0.266647\n1.428062\n1.255001\n1.198888\n1.947302\n0.193032\n0.782982\n\n\n3\n1.761760\n0.918865\n0.266647\n0.000000\n1.413401\n1.219793\n1.127950\n1.929637\n0.377379\n0.840250\n\n\n4\n1.156486\n1.788737\n1.428062\n1.413401\n0.000000\n0.397103\n0.596618\n1.956182\n1.517926\n1.036828\n\n\n5\n1.326107\n1.527192\n1.255001\n1.219793\n0.397103\n0.000000\n0.167221\n1.976696\n1.248383\n0.660213\n\n\n6\n1.319344\n1.331400\n1.198888\n1.127950\n0.596618\n0.167221\n0.000000\n1.956804\n1.207618\n0.660589\n\n\n7\n1.830671\n1.901749\n1.947302\n1.929637\n1.956182\n1.976696\n1.956804\n0.000000\n1.966022\n1.886064\n\n\n8\n1.873676\n0.854802\n0.193032\n0.377379\n1.517926\n1.248383\n1.207618\n1.966022\n0.000000\n0.741273\n\n\n9\n1.700417\n1.133180\n0.782982\n0.840250\n1.036828\n0.660213\n0.660589\n1.886064\n0.741273\n0.000000\n\n\n\n\n\n\n\nAfin d’évaluer chaque classe, on peut calculer la séparabilité minimale, ce qui nous permet de constater que la classe eau a le maximum de séparabilité avec les autres classes.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Classifications d'images supervisées</span>"
    ]
  },
  {
    "objectID": "05-ClassificationsSupervisees.html#mesures-de-performance-dune-méthode-de-classification",
    "href": "05-ClassificationsSupervisees.html#mesures-de-performance-dune-méthode-de-classification",
    "title": "6  Classifications d’images supervisées",
    "section": "6.4 Mesures de performance d’une méthode de classification",
    "text": "6.4 Mesures de performance d’une méthode de classification\nLorsque que l’on cherche à établir la performance d’un modèle, il convient de mesurer la performance du classificateur utilisé. Il existe de nombreuses mesures de performance qui sont toutes dérivées de la matrice de confusion. Cette matrice compare les étiquettes provenant de l’annotation (la vérité terrain) et les étiquettes prédites par un modèle. On peut définir \\(C(i,j)\\) comme étant le nombre de prédictions dont la vérité terrain indique la classe \\(i\\) qui sont prédites dans la classe \\(j\\). La fonction confusion_matrix permet de faire ce calcul, voici un exemple très simple:\n\ny_true = [\"cat\", \"ant\", \"cat\", \"cat\", \"ant\", \"bird\", \"bird\"]\ny_pred = [\"ant\", \"ant\", \"cat\", \"cat\", \"ant\", \"cat\", \"bird\"]\nconfusion_matrix(y_true, y_pred, labels=[\"ant\", \"bird\", \"cat\"])\n\narray([[2, 0, 0],\n       [0, 1, 1],\n       [1, 0, 2]])\n\n\nLa fonction classification_report permet de générer quelques métriques:\n\ny_true = [\"cat\", \"ant\", \"cat\", \"cat\", \"ant\", \"bird\", \"bird\"]\ny_pred = [\"ant\", \"ant\", \"cat\", \"cat\", \"ant\", \"cat\", \"bird\"]\nprint(classification_report(y_true, y_pred, target_names=[\"ant\", \"bird\", \"cat\"]))\n\n              precision    recall  f1-score   support\n\n         ant       0.67      1.00      0.80         2\n        bird       1.00      0.50      0.67         2\n         cat       0.67      0.67      0.67         3\n\n    accuracy                           0.71         7\n   macro avg       0.78      0.72      0.71         7\nweighted avg       0.76      0.71      0.70         7\n\n\n\nLe rappel (recall) pour une classe donnée est la proportion de la vérité terrain qui a été correctement identifiée et est sensible aux confusions entre classes (erreurs d’omission). Les valeurs de rappels correspondent à une normalisation de la matrice de confusion par rapport aux lignes.\n\\[\nRecall_i= C_{ii} / \\sum_j C_{ij}\n\\] Une faible valeur de rappel signifie que le classificateur confond facilement la classe concernée avec d’autres classes.\nLa précision est la portion des prédictions qui ont été bien classifiées et est sensible aux fausses alarmes (erreurs de commission). Les valeurs de précision correspondent à une normalisation de la matrice de confusion par rapport aux colonnes. \\[\nPrecision_i= C_{ii} / \\sum_i C_{ij}\n\\] Une faible valeur de précision signifie que le classificateur trouve facilement la classe concernée dans d’autres classes.\nLe f1-score calcul une moyenne des deux métriques précédentes: \\[\n\\text{f1-score}_i=2\\frac{Recall_i \\times Precision_i}{Recall_i + Precision_i}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Classifications d'images supervisées</span>"
    ]
  },
  {
    "objectID": "05-ClassificationsSupervisees.html#méthodes-non-paramétriques",
    "href": "05-ClassificationsSupervisees.html#méthodes-non-paramétriques",
    "title": "6  Classifications d’images supervisées",
    "section": "6.5 Méthodes non paramétriques",
    "text": "6.5 Méthodes non paramétriques\nLes méthodes non paramétriques ne font pas d’hypothèses particulières sur les données. Un des inconvénients de ces modèles est que le nombre de paramètres du modèle augmente avec la taille des données.\n\n6.5.1 Méthode des parallélépipèdes\nLa méthode du parallélépipède est probablement la plus simple et consiste à délimiter directement le domaine des points d’une classe par une boite (un parallélépipède) à \\(D\\) dimensions. Les limites de ces parallélépipèdes forment alors des frontières de décision manuelles qui permettent d’attribuer une classe d’appartenance à un nouveau point. Un des avantages de cette technique est que si un point n’est dans aucun parallélépipède alors il est non classifié. Par contre, la construction de ces parallélépipèdes se complexifient grandement avec le nombre de bandes. À une dimension, deux paramètres, équivalents à un seuillage d’histogramme, sont suffisants. À deux dimensions, vous devez définir 4 segments par classe. Avec trois bandes, vous devez définir six plans par classes et à D dimensions, D hyperplans à D-1 dimensions par classe. Le modèle ici est donc une suite de valeurs min et max pour chacune des bandes et des classes:\n\ndef parrallepiped_train(X_train, y_train):\n  classes= np.unique(y_train).tolist()\n  clf= []\n  for cl in classes:\n      data_cl= X_train[y_train == cl,...] # on cherche les données pour la classe courante\n      \n      limits=[]\n      for b in range(data_cl.shape[1]):\n        limits.append([data_cl[:,b].min(), data_cl[:,b].max()]) # on calcul le min et max pour chaque bande\n      clf.append(np.array(limits))\n  return clf\nclf= parrallepiped_train(X, y_new)\n\nLa prédiction consiste à trouver pour chaque point la première limite qui est satisfaite. Notez qu’il n’y a aucun moyen de décider quelle est la meilleure classe si le point appartient à plusieurs classes.\n\n@jit(nopython=True)\ndef parrallepiped_predict(clf, X_test):\n  y_pred= []\n  for data in X_test:\n    y_pred.append(np.nan)\n    for cl, limits in enumerate(clf):\n      inside= True\n      for b,limit in enumerate(limits):\n        inside = inside and (data[b] &gt;= limit[0]) & (data[b] &lt;= limit[1])\n        if ~inside:\n          break\n      if inside:\n        y_pred[-1]=cl\n  return np.array(y_pred)\n\nOn peut appliquer ensuite le modèle sur l’image au complet. Les résultats sont assez mauvais, puisque seule la classe eau en bleu semble être bien classifiée.\n\ndata_image= img_rgbnir.to_numpy().transpose(1,2,0).reshape(img_rgbnir.shape[1]*img_rgbnir.shape[2],4)\ny_image= parrallepiped_predict(clf, data_image)\ny_image= y_image.reshape(img_rgbnir.shape[1],img_rgbnir.shape[2])\n\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8, 6))\nplt.imshow(y_image, cmap=cmap_classes2)\nax.set_title(\"Méthode des parrallélépipèdes\", fontsize=\"small\")\nplt.show()\n\n\n\n\n\n\n\n\nOn peut calculer quelques mesures de performance sur l’ensemble d’entrainement :\n\ny_pred= parrallepiped_predict(clf, X)\nnom_classes2= [nom_classes[c] for c in np.unique(y).tolist()]\nprint(classification_report(y_new, y_pred, target_names=nom_classes2, zero_division=np.nan))\n\n                       precision    recall  f1-score   support\n\n           Commercial       1.00      0.06      0.11       100\n                Foret       1.00      0.09      0.17       100\n    Faible_végétation       1.00      0.02      0.04       100\n               Sol_nu        nan      0.00      0.00       100\n                Roche       0.00      0.00      0.00       100\n                Route       0.00      0.00      0.00       100\n               Urbain       0.08      0.08      0.08       100\n                  Eau       0.83      0.88      0.85       100\n    Végétation éparse       1.00      0.01      0.02       100\nRoche avec végétation       0.13      1.00      0.23       100\n\n             accuracy                           0.21      1000\n            macro avg       0.56      0.21      0.15      1000\n         weighted avg       0.56      0.21      0.15      1000\n\n\n\n\n6.5.1.1 La malédiction de la haute dimension\nAugmenter le nombre de dimension ou de caractéristiques des données permet de résoudre des problèmes complexes comme la classification d’image. Cependant, cela amène beaucoup de contraintes sur le volume des données. Supposons que nous avons N points occupant un segment linéaire de taille d. La densité de points est \\(N/d\\). Si nous augmentons le nombre de dimension D, la densité de points va diminuer exponentiellement en \\(1/d^D\\). Par conséquent, pour garder une densité constante et donc une bonne estimation des parallélépipèdes, il nous faudrait augmenter le nombre de points en puissance de D. Ceci porte le nom de la malédiction de la dimensionnalité (dimensionality curse). En résumé, l’espace vide augmente plus rapidement que le nombre de données d’entraînement et l’espace des données devient de plus en plus parcimonieux (sparse). Pour contrecarrer ce problème, on peut sélectionner les meilleures caractéristiques ou appliquer une réduction de dimension comme une ACP (Analyse en composantes principales).\n\n\n\n6.5.2 Plus proches voisins\nLa méthode des plus proches voisins (K-Nearest-Neighbors) est certainement la plus simple des méthodes pour classifier des données. Elle consiste à comparer une nouvelle donnée avec ses voisins les plus proches en fonction d’une simple distance Euclidienne. Si une majorité de ces \\(K\\) voisins appartiennent à une classe majoritaire alors cette classe est sélectionnée. Afin de permettre un vote majoritaire, on choisira un nombre impair pour la valeur de \\(K\\). Mallgré sa simplicité, cette technique peut devenir assez demandante en temps de calcul pour un nombre important de points et un nombre élevé de dimensions.\nReprenons l’ensemble d’entraînement formé à partir de notre image RGBNIR précédente :\n\ndf= pd.read_csv('sampling_points.csv')\n# Extraire la colonne 'value'.\n# 'value' est une chaîne de caractères comme représentation d'une liste de valeurs.\n# Nous devons la convertir en données numériques réelles.\nX = df['value'].apply(lambda x: np.fromstring(x[1:-1], dtype=float, sep=' ')).to_list()\n\n# on obtient une liste de numpy array  qu'il faut convertir en un numpy array 2D\nX= np.array([row.tolist() for row in X])\nidx= X.sum(axis=-1)&gt;0 # il se peut qu'il y ait des valeurs erronées\nX= X[idx,...]\ny = df['class'].to_numpy()\ny= y[idx]\nclass_labels = np.unique(y).tolist() # on cherche à savoir combien de classes uniques\nn_classes = len(class_labels)\nif max(class_labels) &gt; n_classes: # il se peut que certaines classes soit absentes\n  y_new= []\n  for i,l in enumerate(class_labels):\n    y_new.extend([i]*sum(y==l))\n  y_new = np.array(y_new)\nnom_classes2= [nom_classes[c] for c in np.unique(y).tolist()]\n\nIl importe de préalablement centrer (moyenne = 0) et de réduire (variance = 1) les données avant d’appliquer la méthode K-NN; avec cette méthode de normalisation, on dit parfois que l’on blanchit les données. Puisque la variance de chaque dimension est égale à 1 (et donc l’inertie totale est égale au nombre de bandes), on s’assure qu’elle ait le même poids ait le même poids dans le calcul des distances entre points. Cette opération porte le nom de StandardScaler dans scikit-learn. On peut alors former un pipeline de traitement combinant les deux opérations :\n\nclf = Pipeline(\n    steps=[(\"scaler\", StandardScaler()), (\"knn\", KNeighborsClassifier(n_neighbors=1))]\n)\n\nAvant d’effectuer un entraînement, on met généralement une portion des données pour valider les performances :\n\nX_train, X_test, y_train, y_test = train_test_split(X, y_new, test_size=0.2, random_state=0)\n\nOn peut visualiser les frontières de décision du K-NN pour différentes valeurs de \\(K\\) lorsque seulement deux bandes sont utilisées (Rouge et proche infra-rouge ici) :\n\n\nNumber of mislabeled points out of a total 200 points : 143\nNumber of mislabeled points out of a total 200 points : 141\nNumber of mislabeled points out of a total 200 points : 136\nNumber of mislabeled points out of a total 200 points : 130\n\n\n\n\n\nFrontières de décision pour le classificateur K-NN\n\n\n\n\nOn peut voir comment les différentes frontières de décision se forment dans l’espace des bandes Rouge-NIR. L’augmentation de K rend ces frontières plus complexes et le calcul plus long.\n\nclf.set_params(knn__weights='distance', knn__n_neighbors = 7).fit(X_train, y_train)\ny_pred = clf.predict(X_test)\nprint(\"Nombre de points misclassifiés sur %d points : %d\"\n  % (X_test.shape[0], (y_test != y_pred).sum()))\n\nNombre de points misclassifiés sur 200 points : 117\n\n\nLe rapport de performance est le suivant :\n\nnom_classes2= [nom_classes[c] for c in np.unique(y).tolist()]\nprint(classification_report(y_test, y_pred, target_names=nom_classes2, zero_division=np.nan))\n\n                       precision    recall  f1-score   support\n\n           Commercial       0.38      0.40      0.39        15\n                Foret       0.45      0.82      0.58        11\n    Faible_végétation       0.29      0.15      0.20        27\n               Sol_nu       0.53      0.45      0.49        22\n                Roche       0.38      0.26      0.31        23\n                Route       0.16      0.17      0.16        18\n               Urbain       0.25      0.20      0.22        20\n                  Eau       0.96      0.96      0.96        24\n    Végétation éparse       0.26      0.53      0.35        15\nRoche avec végétation       0.40      0.40      0.40        25\n\n             accuracy                           0.41       200\n            macro avg       0.40      0.43      0.40       200\n         weighted avg       0.42      0.41      0.40       200\n\n\n\nLa matrice de confusion peut-être affichée de manière graphique :\n\ndisp= ConfusionMatrixDisplay.from_predictions(y_test, y_pred, display_labels=nom_classes2, xticks_rotation='vertical')\n\n\n\n\n\n\n\n\nL’application du modèle (la prédiction) peut se faire sur toute l’image en transposant l’image sous forme d’une matrice avec Largeur x Hauteur lignes et 4 colonnes :\n\ndata_image= img_rgbnir.to_numpy().transpose(1,2,0).reshape(img_rgbnir.shape[1]*img_rgbnir.shape[2],4)\ny_classe= clf.predict(data_image)\ny_classe= y_classe.reshape(img_rgbnir.shape[1],img_rgbnir.shape[2])\n\n\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8, 6))\nplt.imshow(y_classe, cmap=cmap_classes2)\nax.set_title(\"Carte d'occupation des sols avec K-NN\", fontsize=\"small\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n6.5.3 Méthodes par arbre de décision\nLa méthode par arbre de décision consiste à construire une cascade de règles de décision sur chaque caractéristique du jeu de donnée (Breiman et C. Stone 1984). On pourra trouver plus de détails dans la documentation de scikit-learn (Decision Trees). Les arbres de décision on tendance à surapprendre surtout si le nombre de dimensions est élevé. Il est donc conseillé d’avoir un bon ratio entre le nombre d’échantillons et le nombre de dimensions.\n\nX_train, X_test, y_train, y_test = train_test_split(X, y_new, test_size=0.2, random_state=0)\n\n\n\nNumber of mislabeled points out of a total 200 points : 167\nNumber of mislabeled points out of a total 200 points : 154\nNumber of mislabeled points out of a total 200 points : 143\nNumber of mislabeled points out of a total 200 points : 128\n\n\n\n\n\nFrontières de décision pour des arbres de décision de différente profondeur\n\n\n\n\nOn peut observer que les frontières de décision sont formées d’un ensemble de plans simple. Chaque plan étant issu d’une règle de décison formé d’un seuil sur chacune des dimensions. On entraine un arbre de décision avec une profondeur maximale de 5:\n\nclf = tree.DecisionTreeClassifier(max_depth=5)\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\nprint(\"Nombre de points misclassifiés sur %d points : %d\"\n  % (X_test.shape[0], (y_test != y_pred).sum()))\n\nNombre de points misclassifiés sur 200 points : 130\n\n\nLe rapport de performance et la matrice de confusion:\n\n\n                       precision    recall  f1-score   support\n\n           Commercial       0.37      0.47      0.41        15\n                Foret       0.57      0.73      0.64        11\n    Faible_végétation       0.19      0.19      0.19        27\n               Sol_nu       0.57      0.18      0.28        22\n                Roche       0.40      0.09      0.14        23\n                Route       0.32      0.44      0.37        18\n               Urbain        nan      0.00      0.00        20\n                  Eau       0.95      0.79      0.86        24\n    Végétation éparse        nan      0.00      0.00        15\nRoche avec végétation       0.20      0.68      0.31        25\n\n             accuracy                           0.35       200\n            macro avg       0.45      0.36      0.32       200\n         weighted avg       0.44      0.35      0.31       200\n\n\n\n\n\n\n\n\n\n\n\n\nL’application du modèle (la prédiction) peut se faire sur toute l’image en transposant l’image sous forme d’une matrice avec Largeur x Hauteur lignes et 4 colonnes:\n\n\n\n\n\n\n\n\n\nIl est possible de visualiser l’arbre avec l’outil SuperTree mais cela contient beaucoup d’information\n\nfrom supertree import SuperTree # &lt;- import supertree :)\nsuper_tree = SuperTree(clf, X_train, y_train, ['Bleu', 'Vert', 'Rouge', 'NIR'], nom_classes2)\n\nsuper_tree.show_tree()\n\n\n    \n    \n\n    \n        \n        \n        \n      \n            ×\n        \n    \n    \n    \n    \n\n\nOn peut voir que le nœud le plus haut utilise la bande proche-infrarouge pour séparer l’eau de la forêt comme première décision.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Classifications d'images supervisées</span>"
    ]
  },
  {
    "objectID": "05-ClassificationsSupervisees.html#méthodes-paramétriques",
    "href": "05-ClassificationsSupervisees.html#méthodes-paramétriques",
    "title": "6  Classifications d’images supervisées",
    "section": "6.6 Méthodes paramétriques",
    "text": "6.6 Méthodes paramétriques\nLes méthodes paramétriques se basent sur des modélisations statistiques des données pour permettre une classification. Contrairement au méthodes non paramétriques, elles ont un nombre fixe de paramètres qui ne dépend pas de la taille du jeu de données. Par contre, des hypothèses sont faites a priori sur le comportement statistique des données. La classification consiste alors à trouver la classe la plus vraisemblable dont le modèle statistique décrit le mieux les valeurs observées. L’ensemble d’entraînement permettra alors de calculer les paramètres de chaque Gaussienne pour chacune des classes d’intérêt.\n\n6.6.1 Méthode Bayésienne naïve\nLa méthode Bayésienne naïve Gaussienne consiste à poser des hypothèses simplificatrices sur les données, en particulier l’indépendance des données et des dimensions. Ceci permet un calcul plus simple.\n\nfrom sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\ny_pred = gnb.fit(X_train, y_train).predict(X_test)\nprint(\"Nombre de points erronés sur %d points : %d\"\n      % (X_test.shape[0], (y_test != y_pred).sum()))\n\nNombre de points erronés sur 200 points : 131\n\n\n\n\n\n\n\nFrontières de décision pour un classificateur Bayésien naif\n\n\n\n\nOn observe que les frontières de décision sont beaucoup plus régulières que pour K-NN.\n\ngnb.fit(X_train, y_train)\ny_pred = gnb.predict(X_test)\nprint(\"Nombre de points misclassifiés sur %d points : %d\"\n  % (X_test.shape[0], (y_test != y_pred).sum()))\n\nNombre de points misclassifiés sur 200 points : 131\n\n\nDe la même manière, la prédiction peut s’appliquer sur toute l’image:\n\n\n\n\n\n\n\n\n\n\n\n6.6.2 Analyse discriminante quadratique (ADQ)\nL’analyse discriminante quadratique peut-être vue comme une généralisation de l’approche Bayésienne naive qui suppose des modèles Gaussiens indépendants pour chaque dimension et chaque point. Ici, on va considérer un modèle Gaussien multivarié.\n\nqda = QuadraticDiscriminantAnalysis(store_covariance=True)\nqda.fit(X_train, y_train)\ny_pred = qda.predict(X_test)\nprint(\"Nombre de points misclassifiés sur %d points : %d\"\n  % (X_test.shape[0], (y_test != y_pred).sum()))\n\nNombre de points misclassifiés sur 200 points : 124\n\n\nLes Gaussiennes multivariées peuvent être visualiser sous forme d’éllipses décrivant le domaine des valeurs de chaque classe:\n\n\n\n\n\n\n\n\n\nDe la même manière, la prédiction peut s’appliquer sur toute l’image:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBreiman, Friedman, L. et C. C. Stone. 1984. Classification and Regression Trees. Wadsworth, Belmont, CA.\n\n\nJensen, J. R. 2016. Introductory digital image processing: A remote sensing perspective. Pearson Education, Inc.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Classifications d'images supervisées</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Bibliographie",
    "section": "",
    "text": "Achanta, Kevin Smith adhakrishna, Appu Shaji et Sabine Süsstrunk. 2012.\n« SLIC Superpixels Compared to State-of-the-art Superpixel\nMethods. » TPAMI: 636‑643. https://doi.org/10.1109/TPAMI.2012.120.\n\n\nBreiman, Friedman, L. et C. C. Stone. 1984. Classification and\nRegression Trees. Wadsworth, Belmont, CA.\n\n\nCooley, James W. et John W. Tukey. 1965. « An algorithm for the\nmachine calculation of complex Fourier series. » Math.\nComput.: 297‑301. https://web.stanford.edu/class/cme324/classics/cooley-tukey.pdf.\n\n\nHarris, Millman, C. R. 2020. « Array programming with\nNumPy. » Nature: 357‑362. https://doi.org/10.1038/s41586-020-2649-2.\n\n\nHoyer, S. et J. Hamman. 2017. « xarray: N-D labeled Arrays and\nDatasets in Python. » Journal of Open Research\nSoftware 5 (1): 10. https://doi.org/10.5334/jors.148.\n\n\nJahne, Scharr, B et Korkel S. 1999. Principles of filter\ndesign. Handbook of Computer Vision; Applications; Academic Press.\n\n\nJaworek-Korjakowska, P., J.; Kłeczek. 2018. « Region Adjacency\nGraph Approach for Acral Melanocytic Lesion Segmentation. »\nApplied Sciences 8: 1430. 10.3390/app8091430.\n\n\nJensen, J. R. 2016. Introductory digital image processing: A remote\nsensing perspective. Pearson Education, Inc.\n\n\nKokaly, Clark, R. F. et A. J. Klein. 2017. « USGS Spectral Library\nVersion 7 Data: U.S. Geological Survey data release. »\nApplied Sciences. 10.5066/F7RR1WDJ.\n\n\nLee, J. S. 1986. « Speckle suppression and analysis for synthetic\naperture radar images. » Opt. Eng. 25 (5):\n636‑643. https://doi.org/10.1117/12.7973877.\n\n\nOGC. 2019. « OGC GeoTIFF Standard. » https://docs.ogc.org/is/19-008r4/19-008r4.html/.",
    "crumbs": [
      "Bibliographie"
    ]
  }
]